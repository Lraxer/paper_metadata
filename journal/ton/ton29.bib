@article{DBLP:journals/ton/GiotsasNKSGMDKD21,
	author = {Vasileios Giotsas and
                  George Nomikos and
                  Vasileios Kotronis and
                  Pavlos Sermpezis and
                  Petros Gigis and
                  Lefteris Manassakis and
                  Christoph Dietzel and
                  Stavros Konstantaras and
                  Xenofontas A. Dimitropoulos},
	title = {O Peer, Where Art Thou? Uncovering Remote Peering Interconnections
                  at IXPs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {1--16},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3025945},
	doi = {10.1109/TNET.2020.3025945},
	timestamp = {Mon, 22 Feb 2021 15:13:15 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GiotsasNKSGMDKD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet eXchange Points (IXPs) are Internet hubs that mainly provide the switching infrastructure to interconnect networks and exchange traffic. While the initial goal of IXPs was to bring together networks residing in the same city or country, and thus keep local traffic local, this model is gradually shifting. Many networks connect to IXPs without having physical presence at their switching infrastructure. This practice, called Remote Peering, is changing the Internet topology and economy, and has become the subject of a contentious debate within the network operators' community. However, despite the increasing attention it attracts, the understanding of the characteristics and impact of remote peering is limited. In this work, we introduce and validate a heuristic methodology for discovering remote peers at IXPs. We (i) identify critical remote peering inference challenges, (ii) infer remote peers with high accuracy (>97%) and coverage (94%) per IXP, and (iii) characterize different aspects of the remote peering ecosystem by applying our methodology to 30 large IXPs. We observe that remote peering is a significantly common practice in all the studied IXPs; for the largest IXPs, remote peers account for 40% of their member base. We also show that today, IXP growth is mainly driven by remote peering, which contributes two times more than local peering.}
}


@article{DBLP:journals/ton/GuoXLLCZX21,
	author = {Zehua Guo and
                  Yang Xu and
                  Ya{-}Feng Liu and
                  Sen Liu and
                  H. Jonathan Chao and
                  Zhi{-}Li Zhang and
                  Yuanqing Xia},
	title = {AggreFlow: Achieving Power Efficiency, Load Balancing, and Quality
                  of Service in Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {17--33},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3026015},
	doi = {10.1109/TNET.2020.3026015},
	timestamp = {Thu, 17 Nov 2022 09:51:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoXLLCZX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Power-efficient Data Center Networks (DCNs) have been proposed to save power of DCNs using OpenFlow. In these DCNs, the OpenFlow controller adaptively turns on/off links and OpenFlow switches to form a minimum-power subnet that satisfies the traffic demand. As the subnet changes, flows are dynamically routed and rerouted to the routes composed of active switches and links. However, existing flow scheduling schemes could cause undesired results: (1) power inefficiency: due to unbalanced traffic allocation on active routes, extra switches and links may be activated to cater to bursty traffic surges on congested routes, and (2) Quality of Service (QoS) fluctuation: because of the limited flow entry processing ability, switches may not be able to timely install/delete/update flow entries to properly route/reroute flows. In this paper, we propose AggreFlow, a dynamic flow scheduling scheme that achieves power efficiency and QoS improvement using three techniques: Flow-set Routing, Lazy Rerouting, and Adaptive Rerouting. Flow-set Routing achieves load balancing with a small number of flow entry operations by routing flows in a coarse-grained flow-set fashion. Lazy Rerouting spreads rerouting operations over a relatively long period of time, reducing the burstiness of entry operation on switches. Adaptive Rerouting selectively reroutes flow-sets to maintain load balancing. We built an NS3 based fat-tree network simulation platform to evaluate AggreFlow's performance. The simulation results show that AggreFlow reduces power consumption by about 18%, yet achieving load balancing and improved QoS (low packet loss rate and reducing the number of processing entries for flow scheduling by 98%), compared with baseline schemes.}
}


@article{DBLP:journals/ton/WangJHLB21,
	author = {Lin Wang and
                  Lei Jiao and
                  Ting He and
                  Jun Li and
                  Henri E. Bal},
	title = {Service Placement for Collaborative Edge Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {34--47},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3025985},
	doi = {10.1109/TNET.2020.3025985},
	timestamp = {Tue, 09 Nov 2021 09:18:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangJHLB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing is emerging as a promising computing paradigm for supporting next-generation applications that rely on low-latency network connections in the Internet-of-Things (IoT) era. Many edge applications, such as multi-player augmented reality (AR) gaming and federated machine learning, require that distributed clients work collaboratively for a common goal through message exchanges. Given an edge network, it is an open problem how to deploy such collaborative edge applications to achieve the best overall system performance. This paper presents a formal study of this problem. We first provide a mix of cost models to capture the system. Based on a thorough formulation, we propose an iterative algorithm dubbed ITEM, where in each iteration, we construct a graph to encode all the costs and convert the cost optimization problem into a graph cut problem. By obtaining the minimum s-t cut via existing max-flow algorithms, we address the original problem via solving a series of graph cuts. We rigorously prove that ITEM has a parameterized constant approximation ratio. Inspired by the optimal stopping theory, we further design an online algorithm called OPTS, based on optimally alternating between partial and full placement updates. Our evaluations with real-world data traces demonstrate that ITEM performs close to the optimum (within 5%) and converges fast. OPTS achieves a bounded performance as expected while reducing full updates by more than 67% of the time.}
}


@article{DBLP:journals/ton/DaiLYWCHL21,
	author = {Haipeng Dai and
                  Yunhuai Liu and
                  Nan Yu and
                  Chaofeng Wu and
                  Guihai Chen and
                  Tian He and
                  Alex X. Liu},
	title = {Radiation Constrained Wireless Charger Placement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {48--64},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3028704},
	doi = {10.1109/TNET.2020.3028704},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DaiLYWCHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Power Transfer has become a commercially viable technology to charge devices because of the convenience of no power wiring and the reliability of continuous power supply. This paper concerns the fundamental issue of wireless charger placement with electromagnetic radiation (EMR) safety. Although there are a few wireless charging schemes consider EMR safety, none of them addresses the charger placement issue. In this paper, we propose PESA, a wireless charger Placement scheme that guarantees EMR SAfety for every location on the plane. First, we discretize the whole charging area and formulate the problem into the Multidimensional 0/1 Knapsack (MDK) problem. Second, we propose a fast approximation algorithm to the MDK problem. Third, we propose a near optimal scheme to improve speed by double partitioning the area. We prove that the output of our algorithm is better than\n(1−ϵ)\nof the optimal solution to PESA with a smaller EMR threshold\n(1−ϵ/2)\nR\nt\nand a larger EMR coverage radius\n(1+ϵ/2)D\n. We conducted both simulations and field experiments to evaluate the performance of our scheme. Our experimental results show that in terms of charging utility, our algorithm outperforms the comparison algorithms.}
}


@article{DBLP:journals/ton/JinHMFC21,
	author = {Meng Jin and
                  Yuan He and
                  Xin Meng and
                  Dingyi Fang and
                  Xiaojiang Chen},
	title = {Parallel Backscatter in the Wild: When Burstiness and Randomness Play
                  With You},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {65--77},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027735},
	doi = {10.1109/TNET.2020.3027735},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JinHMFC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parallel backscatter is a promising technique for high throughput, low power communications. The existing approaches of parallel backscatter are based on a common assumption, i.e. the states of the collided signals are distinguishable from each other in either the time domain or the IQ (the In-phase and Quadrature) domain. We in this paper disclose the superclustering phenomenon, which invalidates that assumption and seriously affects the decoding performance. Then we propose an interstellar travelling model to capture the bursty Gaussian process of a collided signal. Based on this model, we design Hubble, a reliable signal processing approach to support parallel backscatter in the wild. Hubble addresses several technical challenges: (i) a novel scheme based on Pearson's Chi-Square test to extract the collided signals' combined states, (ii) a Markov driven method to capture the law of signal state transitions, and (iii) error correction schemes to guarantee the reliability of parallel decoding. Theoretically, Hubble is able to decode all the backscattered data, as long as the signals are detectable by the receiver. The experiment results demonstrate that the median throughput of Hubble is 11.7× higher than that of the state-of-the-art approach.}
}


@article{DBLP:journals/ton/ZhangLZWY21,
	author = {Xiaoli Zhang and
                  Qi Li and
                  Zeyu Zhang and
                  Jianping Wu and
                  Jiahai Yang},
	title = {vSFC: Generic and Agile Verification of Service Function Chains in
                  the Cloud},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {78--91},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3028846},
	doi = {10.1109/TNET.2020.3028846},
	timestamp = {Tue, 18 Oct 2022 08:35:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLZWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of network function virtualization (NFV), outsourcing network functions (NFs) to the cloud is becoming increasingly popular for enterprises since it brings significant benefits for NF deployment and maintenance, such as improved scalability and reduced overhead. However, NF outsourcing limits the control of customer enterprises over NF deployment and management, consequently raising serious security concerns. Enterprises cannot ensure whether their outsourced NFs and associated service function chains (SFCs) are correctly enforced according to their specifications. In this paper, we propose vSFC, an SFC verification scheme that allows an enterprise to accurately verify the correctness of SFC enforcement in real time. Specifically, it can detect a wide range of SFC violations including forwarding path incompliance, packet dropping, and flow dropping attacks. Meanwhile, it is generic and agile, which can be applied to arbitrary cloud architectures without requiring any modification to NFs. To demonstrate the feasibility and performance of vSFC, we implement a vSFC prototype on top of Linux kernel-based virtual machines (KVM) and conduct extensive experiments with real traffic. The experimental results show that vSFC can accurately detect SFC violations with negligible overhead.}
}


@article{DBLP:journals/ton/WangQLSC21,
	author = {Minmei Wang and
                  Chen Qian and
                  Xin Li and
                  Shouqian Shi and
                  Shigang Chen},
	title = {Collaborative Validation of Public-Key Certificates for IoT by Distributed
                  Caching},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {92--105},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3029135},
	doi = {10.1109/TNET.2020.3029135},
	timestamp = {Tue, 02 Mar 2021 11:23:49 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangQLSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Public-key certificate validation is an important building block for various security protocols for IoT devices, such as secure channel establishment, handshaking, and verifying sensing data authenticity from cloud storage. However, certification validation incurs non-trivial overhead on resource-constrained IoT devices, because it either brings long latency or large cache space. This work proposes to utilize the power of distributed caching and explores the feasibility of using the cache spaces on all IoT devices as a large pool to store validated certificates. We design a Collaborative Certificate Validation (CCV) protocol including a memory-efficient and fast locator for certificate holders, a trust model to evaluate the trustworthiness of devices, and a protocol suite for dynamic update and certificate revocation. Evaluation results show that CCV only uses less than 25% validation time and reduces >90% decryption operations on each device, compared to a recent method. Malicious devices that conduct dishonest validations can be detected by the network using the proposed trust model.}
}


@article{DBLP:journals/ton/NaghshV21,
	author = {Zahra Naghsh and
                  Shahrokh Valaee},
	title = {Conflict-Free Scheduling in Cellular {V2X} Communications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {106--119},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3030850},
	doi = {10.1109/TNET.2020.3030850},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NaghshV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular V2X, the “Vehicle to Everything” standard, defines a framework for information exchange among vehicles and other network entities. In one of the main modes, LTE V2X relies on a central scheduler to minimize the consumed resources in a conflict-free manner. This NP-hard problem leads to a scheduling strategy that assigns separate resources to the conflicting links and can enjoy from opportunistic resource reuse. In this paper, a novel polynomial-time heuristic, MUCS, is introduced, which models this scheduling as a Vehicle Routing Problem. The existing conflict-free schedulers face major incompetence in satisfying the LTE and 5G V2X requirements mainly due to their reliance on simplifications that are unnatural to the V2X environment. On the contrary, MUCS can flexibly accommodate general Device-to-Device topologies as the basis of V2X networks without imposing any packet segmentation. This way, MUCS minimizes the control information overhead in the cellular V2X standard. Due to scalability and a high quality resource utilization (near-optimal in certain conditions), compared to the existing literature, MUCS is desirable for LTE V2X and the 5G networks.}
}


@article{DBLP:journals/ton/AoHP21,
	author = {Weng{-}Chon Ao and
                  Po{-}Han Huang and
                  Konstantinos Psounis},
	title = {Joint Workload Distribution and Capacity Augmentation in Hybrid Datacenter
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {120--133},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027607},
	doi = {10.1109/TNET.2020.3027607},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AoHP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In hybrid datacenter networks, wired connections are augmented with wireless links to facilitate data transfers between racks. The usage of mmWave/FSO wireless links enables dynamic bandwidth/capacity allocation with extremely small reconfiguration delay. Also, on-demand workload distribution, where the workload of a job is divided into multiple tasks that can be distributed/routed to different racks to be processed in parallel, allows better utilization of computational resources in data centers. In prior work, the dynamic wireless capacity augmentation and workload distribution decisions were mostly made independently and in a heuristic manner for serving distributed and parallel computing jobs. In this paper, we propose a novel analytical framework and algorithms to jointly optimize both the wireless capacity augmentation and the workload distribution, to minimize the job completion time. We consider workload that is not amenable to pipelining, fully amenable to pipelining, and partially amenable to pipelining. With extensive simulation studies, we show that the gain (in terms of the reduction in the job completion time) can be very substantial when allowing such joint optimization.}
}


@article{DBLP:journals/ton/ZouHWH21,
	author = {Shaojun Zou and
                  Jiawei Huang and
                  Jianxin Wang and
                  Tian He},
	title = {Flow-Aware Adaptive Pacing to Mitigate {TCP} Incast in Data Center
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {134--147},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027749},
	doi = {10.1109/TNET.2020.3027749},
	timestamp = {Thu, 11 Aug 2022 15:50:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZouHWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In data center networks, many network-intensive applications leverage large fan-in and many-to-one communication to achieve high performance. However, the special traffic patterns, such as micro-burst and high concurrency, easily cause TCP Incast problem and seriously degrade the application performance. To address the TCP Incast problem, we first reveal theoretically and empirically that alleviating packet burstiness is much more effective in reducing the Incast probability than controlling the congestion window. Inspired by the findings and insights from our experimental observations, we further propose a general supporting scheme Adaptive Pacing (AP), which dynamically adjusts burstiness according to the flow concurrency without any change on switch. Additionally, a sender-based approach is proposed to estimate the flow concurrency. Another feature of AP is its broad applicability. We integrate AP transparently into different TCP protocols (i.e., DCTCP, L 2 DCT and D 2 TCP). Through a series of large-scale NS2 simulations and testbed experiments, we show that AP significantly reduces the Incast probability across different TCP protocols and the network goodput can be increased consistently by on average 7× under severe congestion.}
}


@article{DBLP:journals/ton/LiCDW21,
	author = {Yunpeng Li and
                  Costas Courcoubetis and
                  Lingjie Duan and
                  Richard R. Weber},
	title = {Optimal Pricing for Peer-to-Peer Sharing With Network Externalities},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {148--161},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3029398},
	doi = {10.1109/TNET.2020.3029398},
	timestamp = {Tue, 02 Mar 2021 11:23:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiCDW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we analyse how a peer-to-peer sharing platform should price its service to maximize profit, when user participation increases the value of the service to others by causing positive externalities. Modelling the service as an excludable public good, we propose a bounded utility model to capture many infrastructure sharing applications with bounded network value, in which complete coverage generates finite user valuation (e.g., WiFi or hotspot). Unbounded utility models are used to capture the large-scale user interactions in social media, where the network value follows Metcalfe's or Zipf's law. For these utility models, we analyze the optimal pricing schemes in the case of heterogeneous users under complete and incomplete information of users' service valuations. We propose the concept of `price of information' (PoI) to characterize the profit loss due to lack of information, and present asymptotic PoI bounds for different utility models. We also show that the difficult-to-implement differentiated pricing scheme, which is optimal under incomplete user information, can be replaced by a simple uniform price scheme that is asymptotic optimal. Finally, we extend our pricing schemes to a two-sided market by including a new group of `pure' service users who do not contribute to the public good, and show that the platform may charge zero price to the original group of users in order to attract this pure user group.}
}


@article{DBLP:journals/ton/LuoSLWCW21,
	author = {Chuanwen Luo and
                  Meghana N. Satpute and
                  Deying Li and
                  Yongcai Wang and
                  Wenping Chen and
                  Weili Wu},
	title = {Fine-Grained Trajectory Optimization of Multiple UAVs for Efficient
                  Data Gathering from WSNs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {162--175},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027555},
	doi = {10.1109/TNET.2020.3027555},
	timestamp = {Thu, 02 Dec 2021 16:44:52 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoSLWCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing availability of autonomous small-size Unmanned Aerial Vehicles (UAVs) has provided a promising way for data gathering from Wireless Sensor Networks (WSNs) with the advantages of high mobility, flexibility, and good speed. However, few works considered the situations that multiple UAVs are collaboratively used and the fine-grained trajectory plans of multiple UAVs are devised for collecting data from network including detailed traveling and hovering plans of them in the continuous space. In this paper, we investigate the problem of the Fine-grained Trajectory Plan for multi-UAVs (FTP), in which m UAVs are used to collect data from a given WSN, where m ≥ 1. The problem entails not only to find the flight paths of multiple UAVs but also to design the detailed hovering and traveling plans on their paths for efficient data gathering from WSN. The objective of the problem is to minimize the maximum flight time of UAVs such that all sensory data of WSN is collected by the UAVs and transported to the base station. We first propose a mathematical model of the FTP problem and prove that the problem is NP-hard. To solve the FTP problem, we first study a special case of the FTP problem when m = 1, called FTP with Single UAV (FTPS) problem. Then we propose a constant-factor approximation algorithm for the FTPS problem. Based on the FTPS problem, an approximation algorithm for the general version of the FTP problem when m > 1 is further proposed, which can guarantee a constant factor of the optimal solution. Afterwards, the proposed algorithms are verified by extensive simulations.}
}


@article{DBLP:journals/ton/XuLXPPLJD21,
	author = {Wenzheng Xu and
                  Weifa Liang and
                  Zichuan Xu and
                  Jian Peng and
                  Dezhong Peng and
                  Tang Liu and
                  Xiaohua Jia and
                  Sajal K. Das},
	title = {Approximation Algorithms for the Generalized Team Orienteering Problem
                  and its Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {176--189},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027434},
	doi = {10.1109/TNET.2020.3027434},
	timestamp = {Wed, 07 Apr 2021 15:58:18 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuLXPPLJD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article we study a generalized team orienteering problem (GTOP), which is to find service paths for multiple homogeneous vehicles in a network such that the profit sum of serving the nodes in the paths is maximized, subject to the cost budget of each vehicle. This problem has many potential applications in IoTs and smart cities, such as dispatching energy-constrained mobile chargers to charge as many energy-critical sensors as possible to prolong the network lifetime. In this article, we first formulate the GTOP problem, where each node can be served by different vehicles, and the profit of serving the node is a submodular function of the number of vehicles serving it. We then propose a novel (1 - (1/e) 1/2+e )-approximation algorithm for the problem, where ε is a given constant with 0 <; ε ≤ 1 and e is the base of the natural logarithm. In particular, the approximation ratio is about 0.33 when ε = 0.5. In addition, we devise an improved approximation algorithm for a special case of the problem where the profit is the same by serving a node once and multiple times. We finally evaluate the proposed algorithms with simulation experiments, and the results of which are very promising. Especially, the profit sums delivered by the proposed algorithms are up to 14% higher than those by existing algorithms, and about 93.6% of the optimal solutions.}
}


@article{DBLP:journals/ton/ZhangSZN21,
	author = {Zuyuan Zhang and
                  Fang{-}Ming Shao and
                  Nan Zhang and
                  Yi{-}Feng Niu},
	title = {Maximizing k-Terminal Network Reliability in Some Sparse Graphs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {190--202},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3030819},
	doi = {10.1109/TNET.2020.3030819},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangSZN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {k-terminal network reliability is the probability that k terminal vertices are connected given that edges in the network fail independently while vertices do not fail. It depends on the distribution of these terminal vertices as well as network topology. The problem of computing k-terminal reliability is NP-hard. Previous literature mainly focus on designing efficient algorithms to compute it in different graphs, but are lacking in the analysis for optimal distribution of k terminal vertices in sparse graphs, within which those with n nodes and m edges, where m ≤ n + 1, are most basic classes. Hence, it is of great significance to investigate the optimal distribution of terminal vertices in these classes of graphs before considering general cases. In this paper, we prove that k-terminal network reliability obtains the maximum if k terminal vertices induce a connected subgraph. Further, we give equations of maximum reliability for all possible graphs in the above classes. The experiments illustrate the variation, with several parameters and according to our theoretical results, of maximal k-terminal reliability, and provide observations for graphs with any number of edges.}
}


@article{DBLP:journals/ton/HuangYLS21,
	author = {Huanhuan Huang and
                  Tong Ye and
                  Tony Tong Lee and
                  Weiqiang Sun},
	title = {Delay and Stability Analysis of Connection-Based Slotted-Aloha},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {203--219},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3029774},
	doi = {10.1109/TNET.2020.3029774},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangYLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, connection-based slotted-Aloha (CS-Aloha) has been proposed to improve the performance of random access networks. In this protocol, each node attempts to send a request to the access point (AP) before packet transmission. Once this attempt is successful, the node can transmit up to M packets to the AP. Previous works indicated that the CS-Aloha can achieve a higher throughput than the classical slotted Aloha (S-Aloha), if M is large enough. However, the impact of M on the delay performance and stability is still unknown. To solve this problem, we model each node of the CS-Aloha as a vacation queueing system with limited service discipline, where we consider each batch of packet transmissions as a busy period, and the attempt process between two successive busy periods as a vacation period. We derive the delay distribution, which is turned out to be a geometric distribution. From this result, we further obtain the mean delay, the delay jitter, and the bounded delay region. Our analysis shows that increasing M can accelerate the clean-up of the buffer in each node and thus decrease the attempt rate, which can reduce the average time needed by a node to make a successful attempt. As a result, a large M can decrease the mean delay and the delay jitter, and enlarge the bounded delay region. Also, we obtain the condition to achieve the minimum mean delay under different values of M .}
}


@article{DBLP:journals/ton/PsychasG21,
	author = {Konstantinos Psychas and
                  Javad Ghaderi},
	title = {High-Throughput Bin Packing: Scheduling Jobs With Random Resource
                  Demands in Clusters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {220--233},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3034022},
	doi = {10.1109/TNET.2020.3034022},
	timestamp = {Wed, 07 Apr 2021 15:58:19 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PsychasG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a natural scheduling problem which arises in many distributed computing frameworks. Jobs with diverse resource demands (e.g. memory requirements) arrive over time and must be served by a cluster of servers. To improve throughput and delay, the scheduler can pack as many jobs as possible in each server, however the sum of the jobs' resource demands cannot exceed the server's capacity. Motivated by the increasing complexity of workloads in shared clusters, we consider a setting where jobs' resource demands belong to a very large set of diverse types, or in the extreme case even infinitely many types, i.e. resource demands are drawn from a general unknown distribution over a possibly continuous support. The application of classical scheduling approaches that crucially rely on a predefined finite set of types is discouraging in this high (or infinite) type setting. We first characterize a fundamental limit on the maximum throughput in such setting. We then develop oblivious scheduling algorithms, based on Best-Fit and Universal Partitioning, that have low complexity and can achieve at least 1/2 and 2/3 of the maximum throughput respectively, without the knowledge of the resource demand distribution. Extensive simulation results, using both synthetic and real traffic traces, are presented to verify the performance of our algorithms.}
}


@article{DBLP:journals/ton/KumarK21,
	author = {B. R. Vinay Kumar and
                  Navin Kashyap},
	title = {Probabilistic Forwarding of Coded Packets on Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {234--247},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3031467},
	doi = {10.1109/TNET.2020.3031467},
	timestamp = {Thu, 16 Sep 2021 17:57:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KumarK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a scenario of broadcasting information over a network of nodes connected by noiseless communication links. A source node in the network has some data packets to broadcast. It encodes these data packets into\nn\ncoded packets in such a way that any node in the network that receives any\nk\nout of the\nn\ncoded packets will be able to retrieve all the original data packets. The source transmits the\nn\ncoded packets to its one-hop neighbours. Every other node in the network follows a probabilistic forwarding protocol, in which it forwards a previously unreceived packet to all its neighbours with a certain probability\np\n. We say that the information from the source undergoes a “near-broadcast” if the expected fraction of nodes that receive at least\nk\nof the\nn\ncoded packets is close to 1. The forwarding probability\np\nis chosen so as to minimize the expected total number of transmissions needed for a near-broadcast. We study how, for a given\nk\n, this minimum forwarding probability and the associated expected total number of packet transmissions varies with\nn\n. We specifically analyze the probabilistic forwarding of coded packets on two network topologies: binary trees and square grids. For trees, our analysis shows that for fixed\nk\n, the expected total number of transmissions increases with\nn\n. On the other hand, on grids, a judicious choice of\nn\nsignificantly reduces the expected total number of transmissions needed for a near-broadcast. Behaviour similar to that of the grid is also observed in other well-connected network topologies such as random geometric graphs and random regular graphs.}
}


@article{DBLP:journals/ton/LiuXZQFYH21,
	author = {Jianchun Liu and
                  Hongli Xu and
                  Gongming Zhao and
                  Chen Qian and
                  Xingpeng Fan and
                  Xuwei Yang and
                  He Huang},
	title = {Incremental Server Deployment for Software-Defined NFV-Enabled Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {248--261},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3030298},
	doi = {10.1109/TNET.2020.3030298},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuXZQFYH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Function Virtualization (NFV) is a new paradigm to enable service innovation through virtualizing traditional network functions. To construct a new NFV-enabled network, there are two critical requirements: minimizing server deployment cost and satisfying switch resource constraints. However, prior work mostly focuses on the server deployment cost, while ignoring the switch resource constraints (e.g., switch's flow-table size). It thus results in a large number of rules on switches and leads to massive control overhead. To address this challenge, we propose an incremental server deployment (INSD) problem for construction of scalable NFV-enabled networks. We prove that the INSD problem is NP-Hard, and there is no polynomial-time algorithm with approximation ratio of (1- ϵ)· ln m, where ϵ is an arbitrarily small value and m is the number of requests in the network. We then present an efficient algorithm with an approximation ratio of 2 · H(q · p), where q is the number of VNF's categories and p is the maximum number of requests through a switch. We evaluate the performance of our algorithm with experiments on physical platform (Pica8), Open vSwitches, and large-scale simulations. Both experimental results and simulation results show high scalability of the proposed algorithm. For example, our solution can reduce the control and rule overhead by about 88% with about 5% additional server deployment, compared with the existing solutions.}
}


@article{DBLP:journals/ton/SchullerACH21,
	author = {Timmy Sch{\"{u}}ller and
                  Nils Aschenbruck and
                  Markus Chimani and
                  Martin Horneffer},
	title = {Failure Resiliency With Only a Few Tunnels - Enabling Segment Routing
                  for Traffic Engineering},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {262--274},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3030543},
	doi = {10.1109/TNET.2020.3030543},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SchullerACH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic engineering is an important concept that allows Internet Service Providers (ISPs) to utilize their existing routing hardware more efficiently. One technology that can be used is Segment Routing (SR). In this paper, we address the use of SR to increase the resilience against failure scenarios. In addition, we develop solutions that are manageable and, thus, deployable in a tier 1 ISP network. We propose a post-convergence aware SR based optimization model. With it, we can proactively find a single SR configuration that is beneficial in all predefined failure scenarios, including single link failures, shared risk link group failures, and node failures. In addition to this use-case, we also extend the optimization model to include other important practical requirements such as keeping the number of SR tunnels to a minimum, avoiding arbitrary traffic splitting, or meeting latency bounds. We evaluate our approaches with recently measured data from a tier 1 ISP and show that we can improve over state of the art routing approaches.}
}


@article{DBLP:journals/ton/DemianiukGNK21,
	author = {Vitalii Demianiuk and
                  Sergey Gorinsky and
                  Sergey I. Nikolenko and
                  Kirill Kogan},
	title = {Robust Distributed Monitoring of Traffic Flows},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {275--288},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3034890},
	doi = {10.1109/TNET.2020.3034890},
	timestamp = {Tue, 02 Mar 2021 11:23:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DemianiukGNK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unrelenting traffic growth, device heterogeneity, and load unevenness create scalability challenges for traffic monitoring. In this paper, we propose Robust Distributed Computation (RoDiC), a new approach that addresses these challenges by shifting a portion of the monitoring-task execution from an overloaded network element to another element that has spare resources. Moving the entire execution of the task away from the overloaded element might be infeasible because execution on multiple elements is inherent in the task or requires at least partial participation by the designated overloaded element. Furthermore, distributed execution of a stateful task has to be resilient to network noise in the form of packet reordering and loss. The RoDiC approach relies on two main principles of packet grouping and state overlap to support exact robust distributed monitoring of traffic flows under network noise. RoDiC uses an open-loop paradigm that does not add any control packets, communicates flow state in-band by appending few control bits to packets of monitored flows, and keeps measurement latency low. We apply RoDiC to the problem of flow-size computation and discuss how to instantiate our general technique for real-time packet-loss telemetry. The paper develops robust algorithms, proves their correctness and performance properties, and reports an evaluation driven by realistic traffic traces. The RoDiC algorithms successfully distribute the monitoring-task load while keeping the memory and computation overhead low.}
}


@article{DBLP:journals/ton/QiaoWL21,
	author = {Chunyu Qiao and
                  Jiliang Wang and
                  Yunhao Liu},
	title = {Beyond QoE: Diversity Adaptation in Video Streaming at the Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {289--302},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3032416},
	doi = {10.1109/TNET.2020.3032416},
	timestamp = {Tue, 20 Dec 2022 21:20:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiaoWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive bitrate (ABR) algorithms are critical techniques for high quality-of-experience (QoE) Internet video delivery. Early ABR algorithms conducting the overall QoE function of fixed parameters are limited by the fact that the QoE of end-users are diverse such that the video bitrate is often chosen in a misleading way. State-of-the-art ABR algorithms like MPC and Pensieve utilize offline modeling techniques and result in performance degradation for online QoE diversity adaptation. To address this issue, we propose Elephanta, an online ABR algorithm for edge users, which incorporates user QoE perception interface and adaptation algorithm with flexible parameters. In order to avoid overhead from updating parameters online, we model video streaming as a renewal system and formulate the specific QoE function into flexible formats by setting constraints on corresponding QoE metrics. To validate parameter settings, we emulate Elephanta under 1500 throughput traces, including FCC broadband, 3G\nHSDPA data set from the Internet, as well as the 4G\n/LTE data set we collect. Evaluation results show that Elephanta achieves QoE improvement of 7% over MPC and 3% over Pensieve under QoE diversity in part because of its superior adaptability to QoE diversity. We implemented Elephanta in dash.js at the client side for subjective experiments. We observed the diverse QoE preferences across users and 19/21 users (strongly) agree that Elephanta is responsive to parameter changes while watching videos.}
}


@article{DBLP:journals/ton/CastiglioneFPRS21,
	author = {Luca Maria Castiglione and
                  Paolo Falcone and
                  Alberto Petrillo and
                  Simon Pietro Romano and
                  Stefania Santini},
	title = {Cooperative Intersection Crossing Over 5G},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {303--317},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3032652},
	doi = {10.1109/TNET.2020.3032652},
	timestamp = {Wed, 07 Apr 2021 15:58:19 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CastiglioneFPRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous driving is a safety critical application of sensing and decision-making technologies. Communication technologies extend the awareness capabilities of vehicles, beyond what is achievable with the on-board systems only. Nonetheless, issues typically related to wireless networking must be taken into account when designing safe and reliable autonomous systems. The aim of this work is to present a control algorithm and a communication paradigm over 5G networks for negotiating traffic junctions in urban areas. The proposed control framework has been shown to converge in a finite time and the supporting communication software has been designed with the objective of minimizing communication delays. At the same time, the underlying network guarantees reliability of the communication. The proposed framework has been successfully deployed and tested, in partnership with Ericsson AB, at the AstaZero proving ground in Goteborg, Sweden. In our experiments, three heterogeneous autonomous vehicles successfully drove through a 4-way intersection of 235 square meters in an urban scenario.}
}


@article{DBLP:journals/ton/EjazPS21,
	author = {Ahsen Ejaz and
                  Vassilis Papaefstathiou and
                  Ioannis Sourdis},
	title = {HighwayNoC: Approaching Ideal NoC Performance With Dual Data Rate
                  Routers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {318--331},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3034581},
	doi = {10.1109/TNET.2020.3034581},
	timestamp = {Tue, 02 Mar 2021 11:23:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/EjazPS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper describes HighwayNoC, a Network-on-chip (NoC) that approaches ideal network performance using a Dual Data Rate (DDR) datapath. Based on the observation that routers datapath is faster than control, a DDR NoC allows flits to be routed at DDR improving throughput to rates defined solely by the datapath, rather than by the control. DDR NoCs can use pipeline bypassing to reduce packet latency at low traffic load. However, existing DDR routers offer bypassing only on in-network, non-turning hops to simplify the required logic. HighwayNoC extends bypassing support of DDR routers to local ports, allowing flits to enter and exit the network faster. Moreover, it simplifies the DDR switch allocation and the interface of router ports reducing power and area costs. Post place and route results in 28 nm technology show that HighwayNoC performs better than current state of the art NoCs. Compared to previous DDR NoCs, HighwayNoC reduces average packet latency by 7.3-27% and power consumption by 1-10%, without affecting throughput. Compared to existing Single Data Rate NoCs, HighwayNoC achieves 17-22% higher throughput, has similar or up to 13.8% lower packet latency, and mixed energy efficiency results.}
}


@article{DBLP:journals/ton/ZhangZXYLLWSH21,
	author = {Peng Zhang and
                  Fangzheng Zhang and
                  Shimin Xu and
                  Zuoru Yang and
                  Hao Li and
                  Qi Li and
                  Huanzhao Wang and
                  Chao Shen and
                  Chengchen Hu},
	title = {Network-Wide Forwarding Anomaly Detection and Localization in Software
                  Defined Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {332--345},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3033588},
	doi = {10.1109/TNET.2020.3033588},
	timestamp = {Sat, 09 Apr 2022 12:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZXYLLWSH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A crucial requirement for Software Defined Network (SDN) is that data plane forwarding behaviors should always agree with control plane policies. Such requirement cannot be met when there are forwarding anomalies , where packets deviate from the paths specified by the controller. Most anomaly detection methods for SDN install dedicated rules to collect statistics of each flow, and check whether the statistics conform to the “flow conservation principle”. We find these methods have a limited detection scope: they look at one flow each time, thus can only check a small number of flows simultaneously. In addition, dedicated rules for statistics collection can impose a large overhead on flow tables of SDN switches. To this end, this paper presents FOCES, a network-wide forwarding anomaly detection and localization method in SDN. Different from previous methods, FOCES applies a new kind of flow conservation principle at network wide, and can check forwarding behaviors of all flows in the network simultaneously, without installing any dedicated rules. Finally, FOCES applies a voting-based method to localize malicious switches when anomalies are detected. Experiments with four network topologies show that FOCES can achieve a detection precision higher than 90%, when the packet loss rate is no larger than 10%, and a localization accuracy of around 80% when the packet loss rate is no larger than 5%.}
}


@article{DBLP:journals/ton/KassirGVWWP21,
	author = {Saadallah Kassir and
                  Pablo Caballero Garces and
                  Gustavo de Veciana and
                  Nannan Wang and
                  Xi Wang and
                  Paparao Palacharla},
	title = {An Analytical Model and Performance Evaluation of Multihomed Multilane
                  VANETs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {346--359},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3032324},
	doi = {10.1109/TNET.2020.3032324},
	timestamp = {Tue, 21 Sep 2021 09:12:03 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KassirGVWWP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the potentially high downlink traffic demands of commuters in future autonomous vehicles, we study a network architecture where vehicles use Vehicle-to-Vehicle (V2V) links to form relay network clusters, which in turn use Vehicle-to-Infrastructure (V2I) links to connect to one or more Road Side Units (RSUs). Such cluster-based multihoming offers improved performance, e.g., in coverage and per user shared rate, but depends on the penetration of V2V+V2I capable vehicles and possible blockage, by legacy vehicles, of line of sight based V2V links, such as those based on millimeter-wave and visible light technologies. This paper provides a performance analysis of a typical vehicle’s connectivity and throughput on a highway in the free-flow regime, exploring its dependence on vehicle density, sensitivity to blockages, number of lanes and heterogeneity across lanes. The results, backed up by simulations of realistic vehicular traffic, show that even with moderate vehicle densities and penetration of V2V+V2I capable vehicles, such architectures can achieve substantial improvements in connectivity and reduction in per-user rate variability as compared to V2I based networks. The typical vehicle’s performance is also shown to improve considerably in the multilane highway setting as compared to a single lane road. This paper also sheds light on how the network performance is affected when vehicles can control their relative positions, by characterizing the connectivity-throughput tradeoff faced by the clusters of vehicles.}
}


@article{DBLP:journals/ton/XuYTTYWX21,
	author = {Zhiyuan Xu and
                  Dejun Yang and
                  Jian Tang and
                  Yinan Tang and
                  Tongtong Yuan and
                  Yanzhi Wang and
                  Guoliang Xue},
	title = {An Actor-Critic-Based Transfer Learning Framework for Experience-Driven
                  Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {360--371},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3037231},
	doi = {10.1109/TNET.2020.3037231},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuYTTYWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Experience-driven networking has emerged as a new and highly effective approach for resource allocation in complex communication networks. Deep Reinforcement Learning (DRL) has been shown to be a useful technique for enabling experience-driven networking. In this paper, we focus on a practical and fundamental problem for experience-driven networking: when network configurations are changed, how to train a new DRL agent to effectively and quickly adapt to the new environment. We present an Actor-Critic-based Transfer learning framework for the Traffic Engineering (TE) problem using policy distillation, which we call ACT-TE. ACT-TE effectively and quickly trains a new DRL agent to solve the TE problem in a new network environment, using both old knowledge (i.e., distilled from the existing agent) and new experience (i.e., newly collected samples). We implement ACT-TE in ns-3, and compare it with commonly-used baselines using packet-level simulations on three representative network topologies: NSFNET, ARPANET and random topology. The extensive simulation results show that 1) The existing well-trained DRL agents do not work well in new network environments; 2) ACT-TE significantly outperforms both two straightforward methods (training from scratch and fine-tuning based on an existing DRL agent) and several widely-used traditional methods in terms of network utility, throughput and delay.}
}


@article{DBLP:journals/ton/TongWLTW21,
	author = {Xinyu Tong and
                  Yang Wan and
                  Qianru Li and
                  Xiaohua Tian and
                  Xinbing Wang},
	title = {{CSI} Fingerprinting Localization With Low Human Efforts},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {372--385},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3035210},
	doi = {10.1109/TNET.2020.3035210},
	timestamp = {Tue, 02 Mar 2021 11:23:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TongWLTW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fingerprinting indoor localization systems exploit wireless signal propagation features to estimate the location of wireless devices, where the major challenge in practice is the all-consuming training process: it requires site survey to establish the mapping between the signal feature and the location where the feature is observed. In this paper, we present a Wi-Fi localization scheme based on channel state information (CSI) of wireless signals, which manages to relieve time-consuming site survey. In particular, we first propose how to automatically generate the theoretical fingerprints database based on the signal propagation model and geometric methods. Localization with the theoretical fingerprints database yields accuracy close to existing methods. Second, we improve localization accuracy by parsing the user's trajectory instead of restricting to the single spot, where human movement features introduce more information for localization. Third, we present an automatic update scheme for the theoretical fingerprints database to improve time efficiency for localization, which can save 94 - 98% processing time for utilizing the CSI fingerprints database. We implement a prototype with COTS devices and conduct comprehensive experiments to verify proposed mechanisms. Results show that our design achieves 80% localization errors within 0.3m, which is 3× accuracy compared with the state-of-the-art design leveraging CSI.}
}


@article{DBLP:journals/ton/GuoCW21,
	author = {Jianxiong Guo and
                  Tiantian Chen and
                  Weili Wu},
	title = {A Multi-Feature Diffusion Model: Rumor Blocking in Social Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {386--397},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3032893},
	doi = {10.1109/TNET.2020.3032893},
	timestamp = {Thu, 02 Dec 2021 16:44:52 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online social networks provide a convenient platform for the spread of rumors, which could lead to serious aftermaths such as economic losses and public panic. The classical rumor blocking problem aims to launch a set of nodes as a positive cascade to compete with misinformation in order to limit the spread of rumors. However, most of the related researches were based on a one-dimensional diffusion model. In reality, there is more than one feature associated with an object. A user's impression on this object is determined not just by one feature but by her overall evaluation of all features associated with it. Thus, the influence spread of this object can be decomposed into the spread of multiple features. Based on that, we design a multi-feature diffusion model (MF-model) in this paper and formulate a multi-feature rumor blocking (MFRB) problem on a multi-layer network structure according to this model. To solve the MFRB problem, we design a creative sampling method called Multi-Sampling, which can be applied to this multi-layer network structure. Then, we propose a Revised-IMM algorithm and obtain a satisfactory approximate solution to MFRB. Finally, we evaluate our proposed algorithm by conducting experiments on real datasets, which shows the effectiveness of our Revised-IMM and its advantage to their baseline algorithms.}
}


@article{DBLP:journals/ton/DinhTNHBZG21,
	author = {Canh T. Dinh and
                  Nguyen Hoang Tran and
                  Minh N. H. Nguyen and
                  Choong Seon Hong and
                  Wei Bao and
                  Albert Y. Zomaya and
                  Vincent Gramoli},
	title = {Federated Learning Over Wireless Networks: Convergence Analysis and
                  Resource Allocation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {398--409},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3035770},
	doi = {10.1109/TNET.2020.3035770},
	timestamp = {Wed, 20 Mar 2024 10:34:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DinhTNHBZG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an increasing interest in a fast-growing machine learning technique called Federated Learning (FL), in which the model training is distributed over mobile user equipment (UEs), exploiting UEs' local computation and training data. Despite its advantages such as preserving data privacy, FL still has challenges of heterogeneity across UEs' data and physical resources. To address these challenges, we first propose FEDL, a FL algorithm which can handle heterogeneous UE data without further assumptions except strongly convex and smooth loss functions. We provide a convergence rate characterizing the trade-off between local computation rounds of each UE to update its local model and global communication rounds to update the FL global model. We then employ FEDL in wireless networks as a resource allocation optimization problem that captures the trade-off between FEDL convergence wall clock time and energy consumption of UEs with heterogeneous computing and power resources. Even though the wireless resource allocation problem of FEDL is non-convex, we exploit this problem's structure to decompose it into three sub-problems and analyze their closed-form solutions as well as insights into problem design. Finally, we empirically evaluate the convergence of FEDL with PyTorch experiments, and provide extensive numerical results for the wireless resource allocation sub-problems. Experimental results show that FEDL outperforms the vanilla FedAvg algorithm in terms of convergence rate and test accuracy in various settings.}
}


@article{DBLP:journals/ton/CaoWLMMM21,
	author = {Zhichao Cao and
                  Jiliang Wang and
                  Daibo Liu and
                  Qiang Ma and
                  Xin Miao and
                  XuFei Mao},
	title = {Chase++: Fountain-Enabled Fast Flooding in Asynchronous Duty Cycle
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {410--422},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3034251},
	doi = {10.1109/TNET.2020.3034251},
	timestamp = {Mon, 13 Feb 2023 07:59:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CaoWLMMM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to limited energy supply on many Internet of Things (IoT) devices, asynchronous duty cycle radio management is widely adopted to save energy. Flooding is a critical way to disseminate messages through the whole network. Capture effect enabled concurrent broadcast is appealing to accelerate network flooding in asynchronous duty cycle networks. However, when the flooding payload's size is large, the concurrent broadcast performance is far from efficient due to the frequently unsatisfied capture effect. Intuitively, senders can send a short packet containing partial flooding payload to keep concurrent broadcast efficiency. In practice, we still face two challenges. Considering packet loss, a receiver needs an effective way to recover the entire flooding payload from several received packets as soon as possible. Moreover, considering different channel states of different senders, how a sender chooses the optimal packet length to guarantee high channel utilization is not easy. In this paper, we propose Chase++ a Fountain-code based concurrent broadcast control layer to enable fast flooding in asynchronous duty cycle networks. Chase++ uses Fountain code to alleviate the negative influence of a certain part of the flooding payload's continuous loss. Moreover, Chase++ adaptively selects packet length with the local estimation of channel utilization. Specifically, Chase++ partitions long payload into several short payload blocks, further encoded into many encoded payload blocks by Fountain-code. Then, with temporal and spatial features of the sampled RSS (received signal strength) sequence, a sender estimates the number of concurrent senders. Finally, according to the estimated number of concurrent senders, the sender determines the optimal number of encoded payload blocks in a packet and assembles the encoded payload blocks as lots of packets. Then, the concurrent broadcast layer continuously transmits these packets. Receivers can recover the original flooding payload after several independent encoded payload blocks are collected. We implement Chase++ in TinyOS with TelosB nodes. We further evaluate Chase++ on Local testbed with 50 nodes and Indriya testbed with 95 nodes. The improvement of network flooding speed can reach 23.6% and 13.4%, respectively.}
}


@article{DBLP:journals/ton/ZaveCFRMZ21,
	author = {Pamela Zave and
                  Fabr{\'{\i}}cio B. Carvalho and
                  Ronaldo A. Ferreira and
                  Jennifer Rexford and
                  Masaharu Morimoto and
                  Xuan Kelvin Zou},
	title = {A Verified Session Protocol for Dynamic Service Chaining},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {423--437},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3037049},
	doi = {10.1109/TNET.2020.3037049},
	timestamp = {Sat, 30 Sep 2023 10:29:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZaveCFRMZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Middleboxes are crucial for improving network security and performance, but only if the right traffic goes through the right middleboxes at the right time. Existing traffic-steering techniques rely on a central controller to install fine-grained forwarding rules in network elements-at the expense of a large number of rules, a central point of failure, challenges in ensuring all packets of a session traverse the same middleboxes, and difficulties with middleboxes that modify the “five tuple.” We argue that a session-level protocol is a fundamentally better approach to traffic steering, while naturally supporting host mobility and multihoming in an integrated fashion. In addition, a session-level protocol can enable new capabilities like dynamic service chaining, where the sequence of middleboxes can change during the life of a session, e.g., to remove a load-balancer that is no longer needed, replace a middlebox undergoing maintenance, or add a packet scrubber when traffic looks suspicious. Our Dysco protocol steers the packets of a TCP session through a service chain, and can dynamically reconfigure the chain for an ongoing session. Dysco requires no changes to end-host and middlebox applications, host TCP stacks, or IP routing. Dysco's distributed reconfiguration protocol handles the removal of proxies that terminate TCP connections, middleboxes that change the size of a byte stream, and concurrent requests to reconfigure different parts of a chain. Through formal verification using Spin and experiments with our prototype, we show that Dysco is provably correct, highly scalable, and able to reconfigure service chains across a range of middleboxes.}
}


@article{DBLP:journals/ton/ZhaoTFGZQ21,
	author = {Yangming Zhao and
                  Chen Tian and
                  Jingyuan Fan and
                  Tong Guan and
                  Xiaoning Zhang and
                  Chunming Qiao},
	title = {Joint Reducer Placement and Coflow Bandwidth Scheduling for Computing
                  Clusters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {438--451},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3037064},
	doi = {10.1109/TNET.2020.3037064},
	timestamp = {Wed, 07 Apr 2021 15:58:19 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoTFGZQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reducing Coflow Completion Time (CCT) has a significant impact on application performance in data-parallel frameworks. Most existing works assume that the endpoints of constituent flows in each coflow are predetermined. We argue that CCT can be further optimized by treating flows' destinations as an additional optimization dimension via reducer placement. In this article, we propose and implement RPC, a joint online Reducer Placement and Coflow bandwidth scheduling framework, to minimize the average CCT in cloud clusters. We first develop a 2-approximation algorithm to minimize the CCT of a single coflow, and then schedule all the coflows following the Shortest Remaining Time First (SRTF) principle. We use real testbed experiments and extensive large-scale simulations to demonstrate that RPC can reduce the average CCT by 64.98% compared with the state-of-the-art technologies.}
}


@article{DBLP:journals/ton/ChenLH21,
	author = {Weiwei Chen and
                  Yunhuai Liu and
                  Tian He},
	title = {SoftHM: {A} Software-Based Hierarchical Modulation Design for Wireless
                  System},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {452--464},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3040006},
	doi = {10.1109/TNET.2020.3040006},
	timestamp = {Tue, 02 Mar 2021 11:23:49 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical Modulation (HM) is quite efficient for multimedia broadcast in wireless networks. It exploits the wireless broadcast advantage and allows a transmission to reach different users with various qualities. However, existing HM related schemes are based on specially designed hardware, leading to extra hardware and deployment cost in practice. As a remedy for this, we design and implement a software-based HM scheme, named as SoftHM. By carefully tuning the Physical Layer input of a packet (a sequence of “0” or “1” bits, known as PHY payload), a transmitter simply broadcasts the packet with a single rate (e.g., 54Mbps, the highest rate in 802.11g WLAN), while diverse receivers can decode and extract different amount of information (with different reception rate, e.g., 12Mbps or 54Mbps) from it based on their link qualities. Reverse engineering techniques are adopted so that SoftHM works in a plug-in mode, offering it high potentials for wireless systems without any changes in existing modulation and coding modules. We implement SoftHM based on Universal Software Radio Peripheral (USRP) B210 and Commercial of the Shelf (COTS) Wi-Fi devices. Both rigorous analysis on its performance and comprehensive implementation-based experiments show that compared with the traditional time-sharing approach, to support low-quality users with the same bandwidth, high-quality users in SoftHM achieve 111.1% to 616.7% higher throughput. Likewise, to serve high-quality users with the same bandwidth, low quality users in softHM achieve 176.9% to 390.9% higher throughput.}
}


@article{DBLP:journals/ton/LiSLLWJ21,
	author = {Fengjiao Li and
                  Yu Sang and
                  Zhongdong Liu and
                  Bin Li and
                  Huasen Wu and
                  Bo Ji},
	title = {Waiting But Not Aging: Optimizing Information Freshness Under the
                  Pull Model},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {1},
	pages = {465--478},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3041654},
	doi = {10.1109/TNET.2020.3041654},
	timestamp = {Tue, 25 Jan 2022 14:05:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiSLLWJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Age-of-Information is an important metric for investigating the timeliness performance in information-update systems. In this paper, we study the AoI minimization problem under a new Pull model with replication schemes, where a user proactively sends a replicated request to multiple servers to “pull” the information of interest. Interestingly, we find that under this new Pull model, replication schemes capture a novel tradeoff between different values of the AoI across the servers (due to the random updating processes) and different response times across the servers, which can be exploited to minimize the expected AoI at the user's side. Specifically, assuming Poisson updating process for the servers and exponentially distributed response time, we derive a closed-form formula for computing the expected AoI and obtain the optimal number of responses to wait for to minimize the expected AoI. Then, we extend our analysis to the setting where the user aims to maximize the AoI-based utility, which represents the user's satisfaction level with respect to freshness of the received information. Furthermore, we consider a more realistic scenario where the user has no prior knowledge of the system. In this case, we reformulate the utility maximization problem as a stochastic Multi-Armed Bandit problem with side observations and leverage a special linear structure of side observations to design learning algorithms with improved performance guarantees. Finally, we conduct extensive simulations to elucidate our theoretical results and compare the performance of different algorithms. Our findings reveal that under the Pull model, waiting does not necessarily lead to aging; waiting for more than one response can often significantly reduce the AoI and improve the AoI-based utility in most scenarios.}
}


@article{DBLP:journals/ton/BaiHCTX21,
	author = {Wei Bai and
                  Shuihai Hu and
                  Kai Chen and
                  Kun Tan and
                  Yongqiang Xiong},
	title = {One More Config is Enough: Saving {(DC)TCP} for High-Speed Extremely
                  Shallow-Buffered Datacenters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {489--502},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3032999},
	doi = {10.1109/TNET.2020.3032999},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BaiHCTX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The link speed in production datacenters is growing fast, from 1 Gbps to 40 Gbps or even 100 Gbps. However, the buffer size of commodity switches increases slowly, e.g., from 4 MB at 1 Gbps to 16 MB at 100 Gbps, thus significantly outpaced by the link speed. In such extremely shallow-buffered networks, today's TCP/ECN solutions, such as DCTCP, suffer from either excessive packet losses or significant throughput degradation. Motivated by this, we introduce BCC, 1 a simple yet effective solution that requires only one more ECN configuration (i.e., shared buffer ECN/RED) at commodity switches. BCC operates upon real-time global shared buffer utilization. When available buffer space suffices, BCC delivers both high throughput and low packet loss rate as prior work; When it gets insufficient, BCC automatically triggers the shared buffer ECN to prevent packet loss at the cost of sacrificing a small amount of throughput. BCC is readily deployable with existing commodity switches. We validate BCC's efficacy in a 100G testbed and evaluate its performance using extensive simulations. Our results show that BCC maintains low packet loss rate persistently while only slightly degrading throughput when the buffer becomes insufficient. For example, compared to current practice, BCC achieves up to 94.4% lower 99th percentile flow completion time (FCT) for small flows while only degrading average FCT for large flows by up to 3%.}
}


@article{DBLP:journals/ton/LakiNGFHTKK21,
	author = {S{\'{a}}ndor Laki and
                  Szilveszter N{\'{a}}das and
                  Gergo Gombos and
                  Ferenc Fejes and
                  P{\'{e}}ter Hudoba and
                  Zolt{\'{a}}n Richard Tur{\'{a}}nyi and
                  Zolt{\'{a}}n Kiss and
                  Csaba Keszei},
	title = {Core-Stateless Forwarding With QoS Revisited: Decoupling Delay and
                  Bandwidth Requirements},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {503--516},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3041235},
	doi = {10.1109/TNET.2020.3041235},
	timestamp = {Sun, 16 May 2021 00:12:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LakiNGFHTKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network QoS, fairness and resource sharing control are not completely solved problems. Available solutions lack scalability due to maintaining flow state, require re-tuning if traffic changes, focus on a limited set of networking scenarios or require complex, centralized controllers and feedback loops. In this paper, we propose a core-stateless solution for closed network domains like access, enterprise and data center networks that handles resource sharing and provides guarantees for per-hop latency, independently. The proposed method enables controlled resource sharing by encoding the utility function of flows to Packet Value markings. This allows expressing resource sharing policies for all possible congestion situations, while operation is completely flow unaware inside the network. In addition, it also satisfies per-hop delay requirements for traffic flows independently. The separation of the delay requirements of the packets from their importance has not generally been possible by existing methods so far. The performance of the proposed method has thoroughly been analyzed by large number of simulations covering both static and dynamic scenarios and was implemented in a cloud-native virtual router implementing all the policies needed for a Broadband Network Gateway, showing good performance and better scalability than existing weighted queuing-based solutions.}
}


@article{DBLP:journals/ton/MendelsonVBLKO21,
	author = {Gal Mendelson and
                  Shay Vargaftik and
                  Katherine Barabash and
                  Dean H. Lorenz and
                  Isaac Keslassy and
                  Ariel Orda},
	title = {AnchorHash: {A} Scalable Consistent Hash},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {517--528},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3039547},
	doi = {10.1109/TNET.2020.3039547},
	timestamp = {Sun, 02 Oct 2022 15:51:57 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MendelsonVBLKO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consistent hashing is a central building block in many networking applications, such as maintaining connection affinity of TCP flows. However, current consistent hashing solutions do not ensure full consistency under arbitrary changes or scale poorly in terms of memory footprint, update time and key lookup complexity. We present AnchorHash, a scalable and fully-consistent hashing algorithm. AnchorHash achieves high key lookup rate, low memory footprint and low update time. We formally establish its strong theoretical guarantees, and present an advanced implementation with a memory footprint of only a few bytes per resource. Moreover, evaluations indicate that AnchorHash scales on a single core to 100 million resources while still achieving a key lookup rate of more than 15 million keys per second.}
}


@article{DBLP:journals/ton/AbolhassaniTE21,
	author = {Bahman Abolhassani and
                  John Tadrous and
                  Atilla Eryilmaz},
	title = {Delay Gain Analysis of Wireless Multicasting for Content Distribution},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {529--542},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3039634},
	doi = {10.1109/TNET.2020.3039634},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AbolhassaniTE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we provide a comprehensive analysis of stability properties and delay gains that wireless multicasting capabilities, as opposed to more traditional unicast transmissions, can provide for content distribution in mobile networks. In particular, we propose a model and characterize the average queue-length (and hence average delay) performance of unicasting and various multicasting strategies for serving a dynamic user population at the wireless edge. First, we show that optimized static randomized multicasting (we call it `blind multicasting') leads to stable-everywhere operation irrespective of the network loading factor (given by the ratio of the demand rate to the service rate) and the content popularity distribution. In contrast, traditional unicasting suffers from unstable operation when the loading factor approaches one, although it outperforms blind multicasting at small loading factor levels. This motivates us to study `work-conserving multicast' policies next that always outperform unicasting while still offering stable-everywhere operation. Then, in the worst-case of uniformly-distributed content popularity, we explicitly characterize the scaling of the average queue-length (and hence delay) under a first-come-first-serve multicast strategy as a function of the database size and the loading factor. Consequently, this work provides the fundamental limits, as well as the guidelines, for the design and performance analysis of efficient multicasting strategies for wireless content distribution.}
}


@article{DBLP:journals/ton/ChengWZY21,
	author = {Fan Cheng and
                  Congtao Wang and
                  Xingyi Zhang and
                  Yun Yang},
	title = {A Local-Neighborhood Information Based Overlapping Community Detection
                  Algorithm for Large-Scale Complex Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {543--556},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3038756},
	doi = {10.1109/TNET.2020.3038756},
	timestamp = {Mon, 04 Mar 2024 15:39:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChengWZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the size of available networks is continuously increasing (even with millions of nodes), large-scale complex networks are receiving significant attention. While existing overlapping-community detection algorithms are quite effective in analyzing complex networks, most of these algorithms suffer from scalability issues when applied to large-scale complex networks, which can have more than 1,000,000 nodes. To address this problem, we propose an efficient local-expansion-based overlapping-community detection algorithm using local-neighborhood information (OCLN). During the iterative expansion process, only neighbors of nodes added in the last iteration (rather than all neighbors) are considered to determine whether they can join the community. This significantly reduces the computational cost and enhances the scalability for community detection in large-scale networks. A belonging coefficient is also proposed in OCLN to filter out incorrectly identified nodes. Theoretical analysis demonstrates that the computational complexity of the proposed OCLN is linear with respect to the size of the network to be detected. Experiments on large-scale LFR benchmark and real-world networks indicate the effectiveness of OCLN for overlapping-community detection in large-scale networks, in terms of both computational efficiency and detected-community quality.}
}


@article{DBLP:journals/ton/LiOFH21,
	author = {Wenjie Li and
                  Sharief M. A. Oteafy and
                  Marwan Fayed and
                  Hossam S. Hassanein},
	title = {Quality of Experience in {ICN:} Keep Your Low- Bitrate Close and High-Bitrate
                  Closer},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {557--570},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3044995},
	doi = {10.1109/TNET.2020.3044995},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiOFH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies into streaming media delivery suggest that performance gains from ubiquitous caching in Information-Centric Networks (ICN) may be negated by Dynamic Adaptive Streaming (DAS), the de facto method for retrieving multimedia content. Bitrate adaptation mechanisms, that drive video streaming, clash with caching mechanisms in ways that affect users' Quality of Experience (QoE). Cache performance also diminishes as consumers dynamically select content encoded at different bitrates. In this article we use this evidence to draw a novel insight: in adaptive streaming over ICN, bitrates should be prioritized alongside popularity and hit rates. We build on this insight to propose RippleCache as a family of cache placement schemes that safeguard high-bitrate content at the edge and push low-bitrate content into the network core. Doing so reduces contention of cache resources, as well as congestion in the network. To validate RippleCache claims we construct two separate implementations. We design RippleClassic as a benchmark solution that optimizes content placement by maximizing a measure for ICNs shown to have high correlation with QoE. In addition, our lighter-weight RippleFinder is then re-designed with distributed execution for application in large-scale systems. RippleCache performance gains are reinforced by evaluations in NS-3 against state-of-the-art baseline approaches, using standard measures of QoE as defined by the DASH Industry Forum. Our results demonstrate that RippleClassic and RippleFinder deliver content that suffers less oscillation and rebuffering, all while achieving the highest levels of video quality; thus indicating overall improvements to QoE.}
}


@article{DBLP:journals/ton/XiangYALGKY21,
	author = {Qiao Xiang and
                  Haitao Yu and
                  James Aspnes and
                  Franck Le and
                  Chin Guok and
                  Linghe Kong and
                  Yang Richard Yang},
	title = {Optimizing in the Dark: Learning Optimal Network Resource Reservation
                  Through a Simple Request Interface},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {571--584},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3045595},
	doi = {10.1109/TNET.2020.3045595},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiangYALGKY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network resource reservation systems are being developed and deployed, driven by the demand and substantial benefits of providing performance predictability for modern distributed applications. However, existing systems suffer limitations: They either are inefficient in finding the optimal resource reservation, or cause private information (e.g., from the network infrastructure) to be exposed (e.g., to the user). In this paper, we design BoxOpt, a novel system that leverages efficient oracle construction techniques in optimization and learning theory to automatically, and swiftly learn the optimal resource reservations without exchanging any private information between the network and the user. In BoxOpt, we first model the simple reservation interface adopted in most reservation systems as a resource membership oracle. Second, we develop an efficient algorithm that constructs a resource separation oracle by a linear number of calls on resource membership oracle. Third, we develop a generic framework to construct a resource optimization oracle by iteratively calling the resource separation oracle, and then develop three novel, efficient algorithms under this generic framework, the best of which computes the optimal resource reservation by a linear number of calls on resource separation oracle. As such, BoxOpt can discover the optimal resource reservation with O(n 2 ) calls on the resource membership oracle. We implement a prototype of BoxOpt with and demonstrate its efficiency and efficacy via extensive experiments using real network topology and a 7-day trace from a large operational federation network. Results show that (1) BoxOpt has a 100% correctness ratio by comparing with a state-of-the-art optimization solver, and (2) for 90% of requests, BoxOpt learns the optimal resource reservation within 10 seconds.}
}


@article{DBLP:journals/ton/WangXM21,
	author = {Xin Wang and
                  Yinlong Xu and
                  Richard T. B. Ma},
	title = {Paid Peering, Settlement-Free Peering, or Both?},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {585--594},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3045220},
	doi = {10.1109/TNET.2020.3045220},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangXM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of congestion-sensitive and data-intensive applications, traditional settlement-free peering agreements with best-effort delivery often do not meet the QoS requirements of content providers (CPs). Meanwhile, Internet access providers (IAPs) feel that revenues from end-users are not sufficient to recoup the upgrade costs of network infrastructures. Consequently, some IAPs have begun to offer CPs a new type of peering agreement, called paid peering, under which they provide CPs with better data delivery quality for a fee. In this article, we model a network platform where an IAP makes decisions on the peering types offered to CPs and the prices charged to CPs and end-users. We study the optimal peering schemes for the IAP, i.e., to offer CPs both the paid and settlement-free peering to choose from or only one of them, as the objective is profit or welfare maximization. Our results show that 1) the IAP should always offer the paid and settlement-free peering under the profit-optimal and welfare-optimal schemes, respectively, 2) whether to simultaneously offer the other peering type is largely driven by the type of data traffic, e.g., text or video, and 3) regulators might want to encourage the IAP to allocate more network capacity to the settlement-free peering for increasing user welfare.}
}


@article{DBLP:journals/ton/ZengCZYZ21,
	author = {Liekang Zeng and
                  Xu Chen and
                  Zhi Zhou and
                  Lei Yang and
                  Junshan Zhang},
	title = {CoEdge: Cooperative {DNN} Inference With Adaptive Workload Partitioning
                  Over Heterogeneous Edge Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {595--608},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3042320},
	doi = {10.1109/TNET.2020.3042320},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZengCZYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices' computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5% ~ 66.9% energy reduction for four widely-adopted CNN models.}
}


@article{DBLP:journals/ton/CohenEFS21,
	author = {Itamar Cohen and
                  Gil Einziger and
                  Roy Friedman and
                  Gabriel Scalosub},
	title = {Access Strategies for Network Caching},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {609--622},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3043280},
	doi = {10.1109/TNET.2020.3043280},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CohenEFS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Having multiple data stores that can potentially serve content is common in modern networked applications. Data stores often publish approximate summaries of their content to enable effective utilization. Since these summaries are not entirely accurate, forming an efficient access strategy to multiple data stores becomes a complex risk management problem. This paper formally models this problem as a cost minimization problem, while taking into account both access costs, the inaccuracy of the approximate summaries, as well as the penalties incurred by failed requests. We introduce practical algorithms with guaranteed approximation ratios and further show that they are optimal in various settings. We also perform an extensive simulation study based on real data and show that our algorithms are more robust than existing heuristics. That is, they exhibit near-optimal performance in various settings, whereas the efficiency of existing approaches depends upon system parameters that may change over time, or be otherwise unknown.}
}


@article{DBLP:journals/ton/ZhangLXBXGW21,
	author = {Menghao Zhang and
                  Guanyu Li and
                  Lei Xu and
                  Jiasong Bai and
                  Mingwei Xu and
                  Guofei Gu and
                  Jianping Wu},
	title = {Control Plane Reflection Attacks and Defenses in Software-Defined
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {623--636},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3040773},
	doi = {10.1109/TNET.2020.3040773},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLXBXGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN) continues to be deployed spanning from enterprise data centers to cloud computing with the proliferation of various SDN-enabled hardware switches and dynamic control plane applications. However, state-of-the-art SDN-enabled hardware switches have rather limited downlink message processing capability, especially for Flow-Mod and Statistic Query, which may not suffice the huge need of dynamic control plane applications. In this paper, we systematically study the interactions between the control plane applications and the data plane switches, and present two new attacks, namely Control Plane Reflection Attacks, to exploit the limited processing capability of SDN-enabled hardware switches. The reflection attacks adopt direct and indirect data plane events to force the control plane to issue massive expensive downlink messages towards SDN switches. Moreover, we propose a two-phase probing-triggering attack strategy, which makes the reflection attacks much more efficient and powerful. Experiments on a testbed with 3 different physical OpenFlow switches demonstrate that the attacks can lead to catastrophic results such as hurting the establishment of new flows and even disruption of connection between SDN controller and switches. To mitigate such attacks, we present several countermeasures from different perspectives. In particular, we propose a novel, systematical defense framework, SwitchGuard, to detect anomalies of downlink messages and prioritize these messages based on a novel monitoring granularity, i.e., host-application pair (HAP). Implementations and evaluations demonstrate that SwitchGuard can effectively reduce the latency for legitimate hosts and applications under the control plane reflection attacks with only minor overheads.}
}


@article{DBLP:journals/ton/ChiesaSABKNS21,
	author = {Marco Chiesa and
                  Roshan Sedar and
                  Gianni Antichi and
                  Michael Borokhovich and
                  Andrzej Kamisinski and
                  Georgios Nikolaidis and
                  Stefan Schmid},
	title = {Fast ReRoute on Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {637--650},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3045293},
	doi = {10.1109/TNET.2020.3045293},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChiesaSABKNS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Highly dependable communication networks usually rely on some kind of Fast Re-Route (FRR) mechanism which allows to quickly re-route traffic upon failures, entirely in the data plane. This paper studies the design of FRR mechanisms for emerging reconfigurable switches. Our main contribution is an FRR primitive for programmable data planes, PURR, which provides low failover latency and high switch throughput, by avoiding packet recirculation. PURR tolerates multiple concurrent failures and comes with minimal memory requirements, ensuring compact forwarding tables, by unveiling an intriguing connection to classic “string theory” (i.e., stringology), and in particular, the shortest common supersequence problem. PURR is well-suited for high-speed match-action forwarding architectures (e.g., PISA) and supports the implementation of a broad variety of FRR mechanisms. Our simulations and prototype implementation (on an FPGA and a Tofino switch) show that PURR improves TCAM memory occupancy by a factor of 1.5 ×- 10.8 × compared to a naïve encoding when implementing state-of-the-art FRR mechanisms. PURR also improves the latency and throughput of datacenter traffic up to a factor of 2.8 ×- 5.5 × and 1.2 ×- 2 ×, respectively, compared to approaches based on recirculating packets.}
}


@article{DBLP:journals/ton/CasaleG21,
	author = {Giuliano Casale and
                  Nicolas Gast},
	title = {Performance Analysis Methods for List-Based Caches With Non-Uniform
                  Access},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {651--664},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3042869},
	doi = {10.1109/TNET.2020.3042869},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CasaleG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {List-based caches can offer lower miss rates than single-list caches, but their analysis is challenging due to state space explosion. In this setting, we propose novel methods to analyze performance for a general class of list-based caches with tree structure, non-uniform access to items and lists, and random or first-in first-out replacement policies. Even though the underlying Markov process is shown to admit a product-form solution, this is difficult to exploit for large caches. Thus, we develop novel approximations for cache performance metrics, in particular by means of a singular perturbation method and a refined mean field approximation. We compare the accuracy of these approaches to simulations, finding that our new methods rapidly converge to the equilibrium distribution as the number of items and the cache capacity grow in a fixed ratio. We find that they are much more accurate than fixed point methods similar to prior work, with mean average errors typically below 1.5% even for very small caches. Our models are also generalized to account for synchronous requests, fetch latency, and item sizes, extending the applicability of approximations for list-based caches.}
}


@article{DBLP:journals/ton/HouSSXK21,
	author = {Jing Hou and
                  Li Sun and
                  Tao Shu and
                  Yong Xiao and
                  Marwan Krunz},
	title = {Economics of Strategic Network Infrastructure Sharing: {A} Backup
                  Reservation Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {665--680},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3044875},
	doi = {10.1109/TNET.2020.3044875},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HouSSXK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In transitioning to 5G, the high infrastructure cost, the need for fast rollout of new services, and the frequent technology/system upgrades triggered wireless operators to consider adopting the cost-effective network infrastructure sharing (NIS), even among competitors, to gain technology and market access. NIS is a bargaining mechanism whose terms and conditions must be carefully determined based on mutual benefits in a market with uncertainties. In this work, we propose a strategic NIS framework for contractual backup reservation between a small/local network operator with limited resources and uncertain demands, and a more resourceful operator with excessive capacity. The backup reservation agreement requires the local operator (say, operator A) to reserve a certain amount of resources (e.g., spectrum) for future sharing from the resource-owning operator (say, operator B). In return, operator B guarantees availability of its reserved resources to meet the need of operator A. We characterize the bargaining between the operators in terms of the optimal reservation prices and quantities with and without consideration of their competitions in market share, respectively. The conditions under which competing operators have incentive to cooperate are explored. The impact of competition intensity and redundant capacity on performance under backup reservation are also investigated. Our study shows that NIS through backup reservation improves both resource utilization and profits of operators, with the potential to support higher target service levels for end users. We also find that, under certain conditions, operator B may still have the incentive to share its resources even at the risk of impinging on its own users.}
}


@article{DBLP:journals/ton/LiuX21,
	author = {Libin Liu and
                  Hong Xu},
	title = {Elasecutor: Elastic Executor Scheduling in Data Analytics Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {681--694},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3050927},
	doi = {10.1109/TNET.2021.3050927},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data analytics systems use long-running executors to run an application’s entire DAG. Executors exhibit salient time-varying resource requirements. Yet, existing schedulers simply reserve resources for executors statically, and use the peak resource demand to guide executor placement. This leads to low utilization and poor application performance. We present Elasecutor, a novel executor scheduler for data analytics systems. Elasecutor dynamically allocates and explicitly sizes resources to executors over time according to the predicted time/varying resource demands. Rather than placing executors using their peak demand, Elasecutor strategically assigns them to machines based on a concept called dominant remaining resource to minimize resource fragmentation. Elasecutor further adaptively reprovisions resources in order to tolerate inaccurate demand prediction and reschedules tasks to deal with inadequate reprovisioning resources on one machine. Testbed evaluation on a 35-node cluster with our Spark-based prototype implementation shows that Elasecutor reduces makespan by more than 36% on average, and improves cluster utilization by up to 55% compared to existing work.}
}


@article{DBLP:journals/ton/HeRLY21,
	author = {Lin He and
                  Gang Ren and
                  Ying Liu and
                  Jiahai Yang},
	title = {{PAVI:} Bootstrapping Accountability and Privacy to IPv6 Internet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {695--708},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3047667},
	doi = {10.1109/TNET.2020.3047667},
	timestamp = {Wed, 20 Sep 2023 08:24:10 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HeRLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accountability and privacy are considered valuable but conflicting properties in the Internet, which at present does not provide native support for either. Past efforts to balance accountability and privacy in the Internet have unsatisfactory deployability due to the introduction of new communication identifiers, and because of large-scale modifications to fully deployed infrastructures and protocols. The IPv6 is being deployed around the world and this trend will accelerate. In this paper, we propose a private and accountable proposal based on IPv6 called PAVI that seeks to bootstrap accountability and privacy to the IPv6 Internet without introducing new communication identifiers and large-scale modifications to the deployed base. A dedicated quantitative analysis shows that the proposed PAVI achieves satisfactory levels of accountability and privacy. The results of the evaluation of a PAVI prototype show that it incurs little performance overhead, and is widely deployable.}
}


@article{DBLP:journals/ton/MaYH21,
	author = {Qian Ma and
                  Edmund Yeh and
                  Jianwei Huang},
	title = {Selfish Caching Games on Directed Graphs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {709--722},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3047940},
	doi = {10.1109/TNET.2020.3047940},
	timestamp = {Tue, 01 Feb 2022 08:03:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaYH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Caching networks can reduce the routing costs of accessing contents by caching contents closer to users. However, cache nodes may belong to different entities and behave selfishly to maximize their own benefits, which often lead to performance degradation for the overall network. While there has been extensive literature on allocating contents to caches to maximize the social welfare, the analysis of selfish caching behaviors remains largely unexplored. In this paper, we model the selfish behaviors of cache nodes as selfish caching games on arbitrary directed graphs with heterogeneous content popularity. We study the existence of a pure strategy Nash equilibrium (PSNE) in selfish caching games, and analyze its efficiency in terms of social welfare. We show that a PSNE does not always exist in arbitrary-topology caching networks. However, if the network does not have a mixed request loop, i.e., a directed loop in which each edge is traversed by at least one content request, we show that a PSNE always exists and can be found in polynomial time. Furthermore, we can avoid mixed request loops by properly choosing request forwarding paths. We then show that the efficiency of Nash equilibria, captured by the price of anarchy (PoA), can be arbitrarily poor if we allow arbitrary content request patterns, and adding extra cache nodes can make the PoA worse, i.e., cache paradox happens. However, when cache nodes have homogeneous request patterns, we show that the PoA is bounded even allowing arbitrary topologies. We further analyze the selfish caching games for cache nodes with limited computational capabilities, and show that an approximate PSNE exists with bounded PoA in certain cases of interest. Simulation results show that increasing the cache capacity in the network improves the efficiency of Nash equilibria, while adding extra cache nodes can degrade the efficiency of Nash equilibria.}
}


@article{DBLP:journals/ton/MengGSCZWZXSH21,
	author = {Zili Meng and
                  Yaning Guo and
                  Yixin Shen and
                  Jing Chen and
                  Chao Zhou and
                  Minhu Wang and
                  Jia Zhang and
                  Mingwei Xu and
                  Chen Sun and
                  Hongxin Hu},
	title = {Practically Deploying Heavyweight Adaptive Bitrate Algorithms With
                  Teacher-Student Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {723--736},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3048666},
	doi = {10.1109/TNET.2020.3048666},
	timestamp = {Sat, 22 Apr 2023 00:14:05 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MengGSCZWZXSH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Major commercial client-side video players employ adaptive bitrate (ABR) algorithms to improve the user quality of experience (QoE). With the evolvement of ABR algorithms, increasingly complex methods such as neural networks have been adopted to pursue better performance. However, these complex methods are too heavyweight to be directly deployed in client devices with limited resources, such as mobile phones. Existing solutions suffer from a trade-off between algorithm performance and deployment overhead. To make the deployment of sophisticated ABR algorithms practical, we propose PiTree , a general , high-performance , and scalable framework that can faithfully convert sophisticated ABR algorithms into decision trees with teacher-student learning. In this way, network operators can train complex models offline and deploy converted lightweight decision trees online. We also present theoretical analysis on the conversion and provide two upper bounds of the prediction error during the conversion and the generalization loss after conversion. Evaluation on three representative ABR algorithms with both trace-driven emulation and real-world experiments demonstrates that PiTree could convert ABR algorithms into decision trees with < 3% average performance degradation. Moreover, compared to original deployment solutions, PiTree could save considerable operating expenses for content providers.}
}


@article{DBLP:journals/ton/JoshiK21,
	author = {Gauri Joshi and
                  Dhruva Kaushal},
	title = {Synergy via Redundancy: Adaptive Replication Strategies and Fundamental
                  Limits},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {737--749},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3047513},
	doi = {10.1109/TNET.2020.3047513},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JoshiK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The maximum possible throughput (or the rate of job completion) of a multi-server system is typically the sum of the service rates of individual servers. Recent work shows that launching multiple replicas of a job and canceling them as soon as one copy finishes can boost the throughput, especially when the service time distribution has high variability. This means that redundancy can, in fact, create synergy among servers such that their overall throughput is greater than the sum of individual servers. This work seeks to find the fundamental limit of the throughput boost achieved by job replication and the optimal replication policy to achieve it. While most previous works consider upfront replication policies, we expand the set of possible policies to delayed launch of replicas. The search for the optimal adaptive replication policy can be formulated as a Markov Decision Process, using which we propose two myopic replication policies, MaxRate and AdaRep, to adaptively replicate jobs. In order to quantify the optimality gap of these and other policies, we derive upper bounds on the service capacity, which provide fundamental limits on the throughput of queueing systems with redundancy.}
}


@article{DBLP:journals/ton/BaeLC21,
	author = {Jeongmin Bae and
                  Joohyun Lee and
                  Song Chong},
	title = {Learning to Schedule Network Resources Throughput and Delay Optimally
                  Using Q\({}^{\mbox{+}}\)-Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {750--763},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3051663},
	doi = {10.1109/TNET.2021.3051663},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BaeLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As network architecture becomes complex and the user requirement gets diverse, the role of efficient network resource management becomes more important. However, existing throughput-optimal scheduling algorithms such as the max-weight algorithm suffer from poor delay performance. In this paper, we present reinforcement learning-based network scheduling algorithms for a single-hop downlink scenario which achieve throughput-optimality and converge to minimal delay. To this end, we first formulate the network optimization problem as a Markov decision process (MDP) problem. Then, we introduce a new state-action value function called\nQ\n+\n-function and develop a reinforcement learning algorithm called\nQ\n+\n-learning with UCB (Upper Confidence Bound) exploration which guarantees small performance loss during a learning process. We also derive an upper bound of the sample complexity in our algorithm, which is more efficient than the best known bound from Q-learning with UCB exploration by a factor of\nγ\n2\nwhere\nγ\nis the discount factor of the MDP problem. Finally, via simulation, we verify that our algorithm shows a delay reduction of up to 40.8% compared to the max-weight algorithm over various scenarios. We also show that the Q + -learning with UCB exploration converges to an\nϵ\n-optimal policy 10 times faster than Q-learning with UCB.}
}


@article{DBLP:journals/ton/LiHLJHLR21,
	author = {Shaoran Li and
                  Yan Huang and
                  Chengzhang Li and
                  Brian Jalaian and
                  Y. Thomas Hou and
                  Wenjing Lou and
                  Stephen Russell},
	title = {Maximize Spectrum Efficiency in Underlay Coexistence With Channel
                  Uncertainty},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {764--778},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3047760},
	doi = {10.1109/TNET.2020.3047760},
	timestamp = {Thu, 11 May 2023 21:27:18 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiHLJHLR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider an underlay coexistence scenario where secondary users (SUs) must keep their interference to the primary users (PUs) under control. However, the channel gains from the PUs to the SUs are uncertain due to a lack of cooperation between the PUs and the SUs. Under this circumstance, it is preferable to allow the interference threshold of each PU to be violated occasionally as long as such violation stays below a probability. In this article, we employ Chance-Constrained Programming (CCP) to exploit this idea of occasional interference threshold violation. We assume the uncertain channel gains are only known by their mean and covariance. These quantities are slow-changing and easy to estimate. Our main contribution is to introduce a novel and powerful mathematical tool called Exact Conic Reformulation (ECR), which reformulates the intractable chance constraints into tractable convex constraints. Further, ECR guarantees an equivalent reformulation from linear chance constraints into deterministic conic constraints without the limitations associated with Bernstein Approximation, on which our research community has been fixated on for years. Through extensive simulations, we show that our proposed solution offers a significant improvement over existing approaches in terms of performance and ability to handle channel correlations (where Bernstein Approximation is no longer applicable).}
}


@article{DBLP:journals/ton/FarhadiMHPKWCP21,
	author = {Vajiheh Farhadi and
                  Fidan Mehmeti and
                  Ting He and
                  Thomas F. La Porta and
                  Hana Khamfroush and
                  Shiqiang Wang and
                  Kevin S. Chan and
                  Konstantinos Poularakis},
	title = {Service Placement and Request Scheduling for Data-Intensive Applications
                  in Edge Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {779--792},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3048613},
	doi = {10.1109/TNET.2020.3048613},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FarhadiMHPKWCP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing provides the opportunity for wireless users to exploit the power of cloud computing without a large communication delay. To serve data-intensive applications (e.g., video analytics, machine learning tasks) from the edge, we need, in addition to computation resources, storage resources for storing server code and data as well as network bandwidth for receiving user-provided data. Moreover, due to time-varying demands, the code and data placement needs to be adjusted over time, which raises concerns of system stability and operation cost. In this paper, we address these issues by proposing a two-time-scale framework that jointly optimizes service (code and data) placement and request scheduling, while considering storage, communication, computation, and budget constraints. First, by analyzing the hardness of various cases, we completely characterize the complexity of our problem. Next, we develop a polynomial-time service placement algorithm by formulating our problem as a set function optimization, which attains a constant-factor approximation under certain conditions. Furthermore, we develop a polynomial-time request scheduling algorithm by computing the maximum flow in a carefully constructed auxiliary graph, which satisfies hard resource constraints and is provably optimal in the special case where requests have homogeneous resource demands. Extensive synthetic and trace-driven simulations show that the proposed algorithms achieve 90% of the optimal performance.}
}


@article{DBLP:journals/ton/HuangWJZ21,
	author = {Yong Huang and
                  Wei Wang and
                  Tao Jiang and
                  Qian Zhang},
	title = {Detecting Colluding Sybil Attackers in Robotic Networks Using Backscatters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {793--804},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3048126},
	doi = {10.1109/TNET.2020.3048126},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangWJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the openness of wireless medium, robotic networks that consist of many miniaturized robots are susceptible to Sybil attackers, who can fabricate myriads of fictitious robots. Such detrimental attacks can overturn the fundamental trust assumption in robotic collaboration and thus impede widespread deployments of robotic networks in many collaborative tasks. Existing solutions rely on bulky multi-antenna systems to passively obtain fine-grained physical layer signatures, making them unaffordable to miniaturized robots. To overcome this limitation, we present ScatterID, a lightweight system that attaches featherlight and batteryless backscatter tags to single-antenna robots for Sybil attack mitigation. Instead of passively “observing” signatures, ScatterID actively “manipulates” multipath propagation by exploiting backscatter tags to intentionally create rich multipath signatures obtainable to single-antenna robots. Particularly, these signatures are used to carefully construct similarity vectors to thwart advanced Sybil attackers, who further trigger power-scaling and colluding attacks to generate dissimilar signatures. Then, a customized random forest model is developed to accurately infer the identity legitimacy of each robot. We implement ScatterID on the iRobot Create platform and evaluate it under various Sybil attacks in real-world environments. The experimental results show that ScatterID achieves a high AUROC of 0.987 and obtains an overall accuracy of 95.4% under basic and advanced Sybil attacks. Specifically, it can successfully detect 96.1% of fake robots while mistakenly rejecting just 5.7% of legitimate ones.}
}


@article{DBLP:journals/ton/ShabtaiRS21,
	author = {Galia Shabtai and
                  Danny Raz and
                  Yuval Shavitt},
	title = {Risk Aware Stochastic Placement of Cloud Services},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {805--820},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3052962},
	doi = {10.1109/TNET.2021.3052962},
	timestamp = {Sun, 25 Jul 2021 11:35:24 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShabtaiRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Allocating the right amount of resources to each service in any of the datacenters in a cloud environment is a very difficult task. This task becomes much harder due to the dynamic nature of the workload and the fact that while long term statistics about the demand may be known, it is impossible to predict the exact demand in each point in time. As a result, service providers either over allocate resources and hurt the service cost efficiency, or run into situation where the allocated local resources are insufficient to support the current demand. In these cases, the service providers deploy overflow mechanisms such as redirecting traffic to a remote datacenter or temporarily leasing additional resources (at a higher price) from the cloud infrastructure owner. The additional cost is in many cases proportional to the amount of overflow demand. In this paper we study this approach and develop a novel mechanism to assign services to datacenters based on the available resources in each datacenter and the distribution of the demand for each service. We use comprehensive analysis to prove that the overall overflow cost is almost optimal for arbitrary demand distributions, as long as there are no dependencies among the services. We further show, using simulation based on real data that the scheme performs very well on realistic service workloads.}
}


@article{DBLP:journals/ton/ChiLSHZ21,
	author = {Zicheng Chi and
                  Yan Li and
                  Hongyu Sun and
                  Zhichuan Huang and
                  Ting Zhu},
	title = {Simultaneous Bi-Directional Communications and Data Forwarding Using
                  a Single ZigBee Data Stream},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {821--833},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3054339},
	doi = {10.1109/TNET.2021.3054339},
	timestamp = {Thu, 29 Dec 2022 16:08:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChiLSHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the exponentially increasing number of Internet of Things (IoT) devices and the huge volume of data generated by these devices, there is a pressing need to investigate a more efficient communication method in both frequency and time domains at the edge of IoT networks. In this paper, we present Amphista, a novel cross-layer design for IoT communication and data forwarding that can more efficiently utilize the ever increasingly crowded 2.4 GHz spectrum near the gateway. Specifically, to enable the communication from ZigBee to WiFi, we leverage WiFi's fine-grained channel state information to extract the concurrently transmitted ZigBee-to-WiFi message from time overlapped ZigBee and WiFi packet. We further leverage this unique feature and design a novel forwarding protocol that can simultaneously forward uplink (e.g., collecting sensing data) and downlink (e.g., disseminating control messages) data by using a single ZigBee data stream. Our extensive experimental results show that Amphista significantly improves throughput (by up to 400x) and reduces the latency.}
}


@article{DBLP:journals/ton/FanMHYZ21,
	author = {Fujie Fan and
                  Hangyu Meng and
                  Bing Hu and
                  Kwan L. Yeung and
                  Zhifeng Zhao},
	title = {Roulette Wheel Balancing Algorithm With Dynamic Flowlet Switching
                  for Multipath Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {834--847},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3051995},
	doi = {10.1109/TNET.2021.3051995},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FanMHYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Load balance is an important issue in datacenter networks. The flowlet-based algorithms can balance the traffic with fine granularity and does not suffer the packet mis-sequencing problem. But their performances are rather limited or require extra communication overhead. In this paper, we propose a local load-aware algorithm called Dynamic Roulette Wheel (DRW). In DRW, the roulette wheel is adopted to select a new path for the flowlet according to the local load. Each source of multipath balances the traffic to all its egress links without the communication overhead. Moreover, the granularity of flowlet can be dynamically tuned from a single packet to the whole flow. Finally, the Capacity Aggregation (CA) mechanism is designed for the case of link or switch failure. We prove in theory that DRW can achieve the optimal global load balancing. The simulation results also show that DRW provides almost the best delay performance and the least packet out-of-order proportion overall among all existing flowlet switching algorithms.}
}


@article{DBLP:journals/ton/JahanianR21,
	author = {Mohammad Jahanian and
                  K. K. Ramakrishnan},
	title = {Name Space Analysis: Verification of Named Data Network Data Planes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {848--861},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3050769},
	doi = {10.1109/TNET.2021.3050769},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JahanianR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named Data Networking (NDN) has many forwarding behaviors, strategies, and protocols to enable the benefits of Information-Centric Networking. This additional functionality introduces complexity, motivating the need for a tool to help reason about and verify that basic properties of an NDN data plane are guaranteed. This paper proposes Name Space Analysis (NSA), a network verification framework to model and analyze NDN data planes. NSA can take as input one or more snapshots, each representing a state of the data plane. It then provides the verification result against specified properties. NSA builds on the theory of Header Space Analysis, and extends it in a number of ways, e.g., supporting variable-sized headers with flexible formats, introduction of name space functions, allowing for name-based properties such as content reachability and name leakage-freedom, and multi-snapshot verification such as equivalence checks. These important additions reflect the behavior and requirements of NDN, requiring modeling and verification foundations fundamentally different from those of traditional host-centric networks. As a case study, we show how NSA can detect name space conflicts in NDN, which can be often hard to catch. Leveraging the learning from this study, we outline a conflict detection and resolution protocol and a name space registry to avoid such conflicts. We have implemented NSA and identified a number of optimizations to enhance the efficiency of verification. Results from our evaluations, using snapshots from various synthetic test cases and the real-world NDN testbed, show how NSA is effective, in finding errors, has good performance, and is scalable.}
}


@article{DBLP:journals/ton/YuHCL21,
	author = {Che{-}Hao Yu and
                  Lin Huang and
                  Cheng{-}Shang Chang and
                  Duan{-}Shin Lee},
	title = {Poisson Receivers: {A} Probabilistic Framework for Analyzing Coded
                  Random Access},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {862--875},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3050485},
	doi = {10.1109/TNET.2021.3050485},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuHCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we develop a probabilistic framework for analyzing coded random access. Our framework is based on a new abstract receiver (decoder), called a Poisson receiver, that is characterized by a success probability function of a tagged packet subject to a Poisson offered load. We show that various coded slotted ALOHA (CSA) systems are Poisson receivers. Moreover, Poisson receivers have two elegant closure properties: (i) Poisson receivers with packet routing are still Poisson receivers, and (ii) Poisson receivers with packet coding are still Poisson receivers. These two closure properties enable us to use smaller Poisson receivers as building blocks for analyzing a larger Poisson receiver. As such, we can analyze complicated systems that are not possible by the classical tree evaluation method. In particular, for CSA systems with both spatial diversity and temporal diversity, we can use the framework of Poisson receivers to compute the exact (asymptotic) throughput. We demonstrate that our framework can be used to provide differentiated services between ultra-reliable low-latency communication (URLLC) traffic and enhanced mobile broadband (eMBB) traffic. By conducting extensive simulations, we also verify that our theoretical results match extremely well with the simulation results.}
}


@article{DBLP:journals/ton/ShiLF21,
	author = {Ming Shi and
                  Xiaojun Lin and
                  Sonia Fahmy},
	title = {Competitive Online Convex Optimization With Switching Costs and Ramp
                  Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {876--889},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3053910},
	doi = {10.1109/TNET.2021.3053910},
	timestamp = {Wed, 10 May 2023 14:38:12 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiLF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate competitive online algorithms for online convex optimization (OCO) problems with linear in-stage costs, switching costs and ramp constraints. While OCO problems have been extensively studied in the literature, there are limited results on the corresponding online solutions that can attain small competitive ratios. We first develop a powerful computational framework that can compute an optimized competitive ratio based on the class of affine policies. Our computational framework can handle a fairly general class of costs and constraints. Compared with other competitive results in the literature, a key feature of our proposed approach is that it can handle scenarios where infeasibility may arise due to hard feasibility constraints. Second, we design a robustification procedure to produce an online algorithm that can attain good performance for both average-case and worst-case inputs. We conduct a case study on Network Functions Virtualization (NFV) orchestration and scaling to demonstrate the effectiveness of our proposed methods.}
}


@article{DBLP:journals/ton/ShiCWWHXQ21,
	author = {Xiaofeng Shi and
                  Haofan Cai and
                  Minmei Wang and
                  Ge Wang and
                  Baiwen Huang and
                  Junjie Xie and
                  Chen Qian},
	title = {TagAttention: Mobile Object Tracing With Zero Appearance Knowledge
                  by Vision-RFID Fusion},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {890--903},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3052805},
	doi = {10.1109/TNET.2021.3052805},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiCWWHXQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose to study mobile object tracing, which allows a mobile system to report the shape, location, and trajectory of the mobile objects appearing in a video camera and identifies each of them with its cyber-identity (ID), even if the appearances of the objects are not known to the system. Existing tracking methods either cannot match objects with their cyber-IDs or rely on complex vision modules pre-learned from vast and well-annotated datasets including the appearances of the target objects, which may not exist in practice. We design and implement TagAttention, a vision-RFID fusion system that achieves mobile object tracing without the knowledge of the target object appearances and hence can be used in many applications that need to track arbitrary un-registered objects. TagAttention adopts the visual attention mechanism, through which RF signals can direct the visual system to detect and track target objects with unknown appearances. Experiments show TagAttention can actively discover, identify, and track the target objects while matching them with their cyber-IDs by using commercial sensing devices in complex environments with various multipath reflectors. It only requires around one second to detect and localize a new mobile target appearing in the video and keeps tracking it accurately over time.}
}


@article{DBLP:journals/ton/TongLTW21,
	author = {Xinyu Tong and
                  Hao Li and
                  Xiaohua Tian and
                  Xinbing Wang},
	title = {Wi-Fi Localization Enabling Self-Calibration},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {904--917},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3051998},
	doi = {10.1109/TNET.2021.3051998},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TongLTW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Channel state information (CSI) based Wi-Fi localization can achieve admirable decimeter-level accuracy; however, such systems require labor-intensive site survey to calibrate the AP position and the antenna array direction, which hinders practical large-scale deployment. In this article, we reveal an interesting finding that the calibration efforts for deploying the CSI localization system can be significantly reduced by simply replacing the ordinary linear antenna layout of the AP with the non-linear layout. In particular, we first present an autonomous self-calibrating method to significantly facilitate site survey for deploying CSI localization systems. Then we propose a systematical evaluation mechanism to show the fundamental reason why linear antenna layout usually leads to serious errors and why non-linear antenna layout is better off. Finally, we build a testbed with COTS devices and conduct comprehensive experiments. Results show that triangular antenna layout can achieve 80% angle of arrival (AoA) measurement error within 9° for any direction in contrast to 16° based on linear antenna layout. Moreover, we can realize promising localization accuracy as previous works even without labor-intensive site survey, where 80% localization error is within\n0.60m\n.}
}


@article{DBLP:journals/ton/ZhangNJWXZRCWY21,
	author = {Yuchao Zhang and
                  Xiaohui Nie and
                  Junchen Jiang and
                  Wendong Wang and
                  Ke Xu and
                  Youjian Zhao and
                  Martin J. Reed and
                  Kai Chen and
                  Haiyang Wang and
                  Guang Yao},
	title = {{BDS+:} An Inter-Datacenter Data Replication System With Dynamic Bandwidth
                  Separation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {918--934},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3054924},
	doi = {10.1109/TNET.2021.3054924},
	timestamp = {Thu, 29 Apr 2021 15:11:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangNJWXZRCWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many important cloud services require replicating massive data from one datacenter (DC) to multiple DCs. While the performance of pair-wise inter-DC data transfers has been much improved, prior solutions are insufficient to optimize bulk-data multicast, as they fail to explore the rich inter-DC overlay paths that exist in geo-distributed DCs, as well as the remaining bandwidth reserved for online traffic under fixed bandwidth separation scheme. To take advantage of these opportunities, we present BDS+ , a near-optimal network system for large-scale inter-DC data replication. BDS+ is an application-level multicast overlay network with a fully centralized architecture, allowing a central controller to maintain an up-to-date global view of data delivery status of intermediate servers, in order to fully utilize the available overlay paths. Furthermore, in each overlay path, it leverages dynamic bandwidth separation to make use of the remaining available bandwidth reserved for online traffic. By constantly estimating online traffic demand and rescheduling bulk-data transfers accordingly, BDS+ can further speed up the massive data multicast. Through a pilot deployment in one of the largest online service providers and large-scale real-trace simulations, we show that BDS+ can achieve 3-\n5×\nspeedup over the provider’s existing system and several well-known overlay routing baselines of static bandwidth separation. Moreover, dynamic bandwidth separation can further reduce the completion time of bulk data transfers by 1.2 to 1.3 times.}
}


@article{DBLP:journals/ton/WuCZCZ21,
	author = {Qiong Wu and
                  Xu Chen and
                  Zhi Zhou and
                  Liang Chen and
                  Junshan Zhang},
	title = {Deep Reinforcement Learning With Spatio-Temporal Traffic Forecasting
                  for Data-Driven Base Station Sleep Control},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {935--948},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3053771},
	doi = {10.1109/TNET.2021.3053771},
	timestamp = {Thu, 29 Apr 2021 15:11:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WuCZCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the ever increasing mobile traffic demand in 5G era, base stations (BSs) have been densely deployed in radio access networks (RANs) to increase the network coverage and capacity. However, as the high density of BSs is designed to accommodate peak traffic, it would consume an unnecessarily large amount of energy if BSs are on during off-peak time. To save the energy consumption of cellular networks, an effective way is to deactivate some idle base stations that do not serve any traffic demand. In this paper, we develop a traffic-aware dynamic BS sleep control framework, named DeepBSC, which presents a novel data-driven learning approach to determine the BS active/sleep modes while meeting lower energy consumption and satisfactory Quality of Service (QoS) requirements. Specifically, the traffic demands are predicted by the proposed GS-STN model, which leverages the geographical and semantic spatial-temporal correlations of mobile traffic. With accurate mobile traffic forecasting, the BS sleep control problem is cast as a Markov Decision Process that is solved by Actor-Critic reinforcement learning methods. To reduce the variance of cost estimation in the dynamic environment, we propose a benchmark transformation method that provides robust performance indicator for policy update. To expedite the training process, we adopt a Deep Deterministic Policy Gradient (DDPG) approach, together with an explorer network, which can strengthen the exploration further. Extensive experiments with a real-world dataset corroborate that our proposed framework significantly outperforms the existing methods.}
}


@article{DBLP:journals/ton/ZouOS21,
	author = {Peng Zou and
                  Omur Ozel and
                  Suresh Subramaniam},
	title = {Optimizing Information Freshness Through Computation-Transmission
                  Tradeoff and Queue Management in Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {2},
	pages = {949--963},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3053937},
	doi = {10.1109/TNET.2021.3053937},
	timestamp = {Wed, 20 Sep 2023 08:31:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZouOS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing applications typically require generated data to be preprocessed at the source and then transmitted to an edge server. In such cases, transmission time and preprocessing time are coupled, yielding a tradeoff between them to achieve the targeted objective. This paper presents analysis of such a system with the objective of optimizing freshness of received data at the edge server. We model this system as two queues in tandem whose service times are independent but the transmission service time is monotonically dependent on the computation service time in mean value. This dependence captures the natural decrease in transmission time due to lower offloaded computation. We analyze various queue management schemes in this tandem queue where the compute queue has a single server, Poisson packet arrivals, general independent service and no extra buffer to save incoming packets. The transmit queue has a single server receiving packets from the compute queue with memoryless service time. We consider the transmit queue in two forms: (i) No data buffer and (ii) One unit data buffer and last come first serve with discarding. We analyze various non-preemptive as well as preemptive cases. We perform stationary distribution analysis and obtain closed form expressions for average age of information (AoI) and average peak AoI. Our numerical results illustrate analytical findings on how computation and transmission times could be traded off to optimize AoI and reveal a consequent tradeoff between average AoI and average peak AoI.}
}


@article{DBLP:journals/ton/RubyZELW21,
	author = {Rukhsana Ruby and
                  Shuxin Zhong and
                  Basem M. ElHalawany and
                  Hanjiang Luo and
                  Kaishun Wu},
	title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {965--978},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056772},
	doi = {10.1109/TNET.2021.3056772},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RubyZELW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.}
}


@article{DBLP:journals/ton/FarkianiBMWVT21,
	author = {Behrooz Farkiani and
                  Bahador Bakhshi and
                  S. Ali MirHassani and
                  Tim Wauters and
                  Bruno Volckaert and
                  Filip De Turck},
	title = {Prioritized Deployment of Dynamic Service Function Chains},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {979--993},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3055074},
	doi = {10.1109/TNET.2021.3055074},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FarkianiBMWVT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service Function Chaining and Network Function Virtualization are enabling technologies that provide dynamic network services with diverse QoS requirements. Regarding the limited infrastructure resources, service providers need to prioritize service requests and even reject some of low-priority requests to satisfy the requirements of high-priority services. In this paper, we study the problem of deployment and reconfiguration of a set of chains with different priorities with the objective of maximizing the service provider's profit; wherein, we also consider management concerns including the ability to control the migration of virtual functions. We show the problem is more practical and comprehensive than the previous studies, and propose an MILP formulation of it along with two solving algorithms. The first algorithm is a fast polynomial-time heuristic that calculates an initial feasible solution to the problem. The second algorithm is an exact method that utilizes the initial feasible solution to achieve the optimal solution quickly. Using extensive simulations, we evaluate the algorithms and show the proposed heuristic can find a feasible solution in at least 83% of the simulation runs in less than 7 seconds, and the exact algorithm can achieve 25% more profit 8 times faster than the state-of-the-art MILP solving methods.}
}


@article{DBLP:journals/ton/YuZZSLC21,
	author = {Dongxiao Yu and
                  Yifei Zou and
                  Yong Zhang and
                  Hao Sheng and
                  Weifeng Lv and
                  Xiuzhen Cheng},
	title = {An Exact Implementation of the Abstract {MAC} Layer via Carrier Sensing
                  in Dynamic Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {994--1007},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3057890},
	doi = {10.1109/TNET.2021.3057890},
	timestamp = {Thu, 10 Mar 2022 09:31:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuZZSLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present the first algorithm to precisely implement the abstract MAC (absMAC) layer under the physical SINR model in dynamic networks. The absMac layer, first presented by (Kuhn et al., 2009), provides reliable local broadcast communications, with timing guarantees stated in terms of a collection of abstract delay functions, based on which high-level algorithms can be designed, independent of specific channel behaviors. The implementation of absMAC requires the design of a distributed algorithm for the local broadcast communication primitives over a particular communication model that defines concrete channel behaviors, and the objective is to minimize the bounds of the abstract delay functions. Halldórsson et al. (2015) showed that under the standard SINR model (synchronous communications without physical carrier sensing or location information), there exist no efficient exact implementations. In this work, we demonstrate that physical carrier sensing, a commonly seen function performed by wireless devices, can help get efficient exact implementation algorithms. Specifically, we propose an algorithm that precisely implements the absMAC layer under the SINR model in dynamic networks. The algorithm provides asymptotically optimal bounds for both acknowledgement and progress functions defined in the absMAC layer. Our algorithm leads to many new faster algorithms for solving high-level problems under the SINR model in dynamic networks. We demonstrate this by exemplifying problems of Consensus, Multi-Message Broadcast, and Single-Message Broadcast. It deserves to point out that our implementation algorithm is designed based on an optimal algorithm for a General Local Broadcast (GLB) problem, which takes the number of distinct messages into consideration for the first time. The GLB algorithm can handle many communication scenarios apart from those defined in the absMAC layer. Simulation results show that our proposed algorithms perform well in reality.}
}


@article{DBLP:journals/ton/LouYKT21,
	author = {Jiadong Lou and
                  Xu Yuan and
                  Sastry Kompella and
                  Nian{-}Feng Tzeng},
	title = {Boosting or Hindering: AoI and Throughput Interrelation in Routing-Aware
                  Multi-Hop Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1008--1021},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3059694},
	doi = {10.1109/TNET.2021.3059694},
	timestamp = {Mon, 25 Sep 2023 12:16:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LouYKT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While considerable work has addressed the optimal AoI under different circumstances in single-hop networks, the exploration of AoI in multi-hop wireless networks is rarely attempted. More importantly, the inherent relationships between AoI and throughput are yet to be explored, especially in multi-hop networks. This paper studies AoI in multi-hop wireless networks and explores its potential relationships with throughput for the very first time, particularly focusing on the impacts of flexible routes on the two metrics, i.e., AoI and throughput. By developing a rigorous mathematical model with interference, channel allocation, link scheduling, and routing path selection taken into consideration, we build the interrelation between AoI and throughput in multi-hop networks. A multi-criteria optimization problem is formulated with the goal of simultaneously minimizing AoI and maximizing network throughput. By qualitatively analyzing their relationships, we exhibit that the two metrics may conflict with each other, implying the optimal solutions for the multi-criteria problem will include a set of Pareto-optimal points rather than a single point existing in the traditional optimization problem. We resort to a novel approach by transforming the multi-criteria problem into a single objective one so as to find the weakly Pareto-optimal points iteratively, thereby allowing us to screen all Pareto-optimal points for the solution. Through formal proof, our solution is demonstrated to be able to identify all Pareto-optimal points and terminate in a finite number of iterations. We conduct the simulation evaluation to identify the optimal tradeoff points of AoI and throughput, demonstrating that one performance metric may improve at the expense of degrading the other, with the routing path found as one of the key factors in determining such a tradeoff.}
}


@article{DBLP:journals/ton/YangPA21,
	author = {Chien{-}Sheng Yang and
                  Ramtin Pedarsani and
                  Amir Salman Avestimehr},
	title = {Edge Computing in the Dark: Leveraging Contextual-Combinatorial Bandit
                  and Coded Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1022--1031},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058685},
	doi = {10.1109/TNET.2021.3058685},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangPA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent advancements in edge computing capabilities, there has been a significant increase in utilizing the edge cloud for event-driven and time-sensitive computations. However, large-scale edge computing networks can suffer substantially from unpredictable and unreliable computing resources which can result in high variability of service quality. We consider the problem of computation offloading over unknown edge cloud networks with a sequence of timely computation jobs. Motivated by the MapReduce computation paradigm, we assume that each computation job can be partitioned to smaller Map functions which are processed at the edge, and the Reduce function is computed at the user after the Map results are collected from the edge nodes. We model the service quality of each edge device as function of context. The user decides the computations to offload to each device with the goal of receiving a recoverable set of computation results in the given deadline. By leveraging the coded computing framework in order to tackle failures or stragglers in computation, we formulate this problem using contextual-combinatorial multi-armed bandits (CC-MAB), and aim to maximize the cumulative expected reward. We propose an online learning policy called online coded edge computing policy, which provably achieves asymptotically-optimal performance in terms of regret loss compared with the optimal offline policy for the proposed CC-MAB problem. In terms of the cumulative reward, it is shown that the online coded edge computing policy significantly outperforms other benchmarks via numerical studies.}
}


@article{DBLP:journals/ton/AliLPK21,
	author = {Kamran Ali and
                  Alex X. Liu and
                  Ioannis Pefkianakis and
                  Kyu{-}Han Kim},
	title = {Distributed Spectrum Sharing for Enterprise Powerline Communication
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1032--1045},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056512},
	doi = {10.1109/TNET.2021.3056512},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AliLPK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As powerline communication (PLC) technology does not require dedicated cabling and network setup, it can be used to easily connect multitude of IoT devices deployed in enterprise environments for sensing and control related applications. IEEE has standardized the PLC protocol in IEEE 1901, also known as HomePlug AV (HPAV) which has been widely adopted in mainstream PLC devices. A key weakness of HPAV protocol is that it does not support spectrum sharing. Currently, each link in an HPAV PLC network operates over the whole available spectrum, and only one link can operate at any time within a single collision domain. In this work, through an extensive measurement study of HPAV PLCs in a real enterprise environment using commodity off-the-shelf (COTS) HPAV PLC devices, we discover that spectrum sharing can significantly benefit enterprise level PLC networks. To this end, we propose a distributed spectrum sharing technique for enterprise HPAV PLC networks, and show that fine-grained distributed spectrum sharing on top of current HPAV MAC protocols can significantly boost the aggregated and per-link throughput, by allowing multiple PLC links to communicate concurrently, while requiring only a few modifications to the existing HPAV devices and protocols.}
}


@article{DBLP:journals/ton/ZhangYHLZ21,
	author = {Mengyuan Zhang and
                  Lei Yang and
                  Shibo He and
                  Ming Li and
                  Junshan Zhang},
	title = {Privacy-Preserving Data Aggregation for Mobile Crowdsensing With Externality:
                  An Auction Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1046--1059},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056490},
	doi = {10.1109/TNET.2021.3056490},
	timestamp = {Wed, 01 Jun 2022 07:44:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangYHLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We develop an auction framework for privacy-preserving data aggregation in mobile crowdsensing, where the platform plays the role as an auctioneer to recruit workers for sensing tasks. The workers are allowed to report noisy versions of their data for privacy protection; and the platform selects workers by taking into account their sensing capabilities to ensure the accuracy level of the aggregated result. Observe that when moving the control of data privacy from the data aggregator to the workers, the data aggregator has limited market power in the sense that it can only partially control the noise by judiciously choosing a subset of workers based on workers’ privacy preferences. This introduces externalities because the privacy of each worker depends on the total noise in the aggregated result that in turn relies on which workers are selected. Specifically, we first consider a privacy-passive scenario where workers participate if their privacy loss can be adequately compensated by the rewards. We explicitly characterize the externalities and the hidden monotonicity property of the problem, making it possible to design a truthful, individually rational and computationally efficient incentive mechanism. We then extend the results to a privacy-proactive scenario where workers have individual requirements for their perceivable data privacy levels. Our proposed mechanisms for both scenarios can select a subset of workers to (nearly) minimize the cost of purchasing their private sensing data subject to the accuracy requirement of the aggregated result. We validate the proposed scheme through theoretical analysis as well as extensive simulations.}
}


@article{DBLP:journals/ton/SahaAKM21,
	author = {Gourav Saha and
                  Alhussein A. Abouzeid and
                  Zaheer Khan and
                  Marja Matinmikko{-}Blue},
	title = {On the Optimal Duration of Spectrum Leases in Exclusive License Markets
                  With Stochastic Demand},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1060--1073},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3060088},
	doi = {10.1109/TNET.2021.3060088},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SahaAKM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the following question which is of interest in designing efficient exclusive-use spectrum licenses sold through spectrum auctions. Given a system model in which customer demand, revenue, and bids of wireless operators are characterized by stochastic processes and an operator is interested in joining the market only if its expected revenue is above a threshold and the lease duration is below a threshold, what is the optimal lease duration which maximizes the net customer demand served by the wireless operators? Increasing or decreasing lease duration has many competing effects; while shorter lease duration may increase the efficiency of spectrum allocation, longer lease duration may increase market competition by incentivizing more operators to enter the market. We formulate this problem as a two-stage Stackelberg game consisting of the regulator and the wireless operators and design efficient algorithms to find the Stackelberg equilibrium of the entire game. These algorithms can also be used to find the Stackelberg equilibrium under some generalizations of our model. Using these algorithms, we obtain important numerical results and insights that characterize how the optimal lease duration varies with respect to market parameters in order to maximize the spectrum utilization. A few of our numerical results are non-intuitive as they suggest that increasing market competition may not necessarily improve spectrum utilization. To the best of our knowledge, this paper presents the first mathematical approach to optimize the lease duration of spectrum licenses.}
}


@article{DBLP:journals/ton/AbdelmoniemB21,
	author = {Ahmed Mohamed Abdelmoniem and
                  Brahim Bensaou},
	title = {T-RACKs: {A} Faster Recovery Mechanism for {TCP} in Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1074--1087},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3059913},
	doi = {10.1109/TNET.2021.3059913},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AbdelmoniemB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud interactive data-driven applications generate swarms of small TCP flows that compete for the small switch buffer space in data-center. Such applications require a small flow completion time (FCT) to be effective. Unfortunately, TCP is myopic with respect to the composite nature of application data. In addition it tends to artificially inflate the FCT of individual flows by several orders of magnitude, because of its Internet-centric design, that fixes the retransmission timeout (RTO) to be at least hundreds of milliseconds. To better understand this problem, in this paper, we use empirical measurements in a small data center testbed to study, at a microscopic level, the effects of various types of packet losses on TCP's performance. In particular, we single out packet losses that impact the tail end of small flows, as well as bursty losses that span a significant fraction of small TCP congestion windows, and show a non-negligible effect of such losses on the FCT. Based on this, we propose the so-called, timely-retransmitted ACKs (or T-RACKs), a simple loss recovery mechanism that conceals the drawbacks of the long RTO even in the presence of heavy packet losses. Interestingly enough, T-RACKS achieves this transparently to TCP itself as it does not require any change to TCP in the tenant's virtual machine (VM) or container. T-RACKs can be implemented as a software shim layer in the hypervisor between the VMs and the server's NIC or in hardware as a networking function in a SmartNIC. Simulation and real testbed results show remarkable performance improvements.}
}


@article{DBLP:journals/ton/ZhaoSLZ21,
	author = {Tianming Zhao and
                  Weisheng Si and
                  Wei Li and
                  Albert Y. Zomaya},
	title = {Optimizing the Maximum Vertex Coverage Attacks Under Knapsack Constraint},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1088--1104},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056450},
	doi = {10.1109/TNET.2021.3056450},
	timestamp = {Wed, 19 Oct 2022 17:21:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoSLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Only when we understand how hackers think, can we defend against their attacks. Towards this end, this paper studies the cyber-attacks that aim to remove nodes or links from network topologies. We particularly focus on one type of such attacks called Maximum Vertex Coverage Attacks under Knapsack constraint (MVCAK), in which a hacker has a fixed budget to remove nodes from a network with the nodes involving different costs for removal, and the hacker's goal is to maximize the number of links incident to the nodes removed. Since the MVCAK problem is NP-hard, we firstly propose an optimal solution by Integer Linear Program formulation. Secondly, we give an approximate solution by Linear Programming relaxation that achieves an approximation ratio of 3/4, outperforming the existing 1 - 1/sqrt(e) (about 0.39). Thirdly, since the straightforward implementation of our approximate solution has a high time complexity, we propose two heuristics to significantly reduce its complexity while preserving the approximation ratio. We formally prove the correctness and the effectiveness of these two heuristics. Finally, we conduct extensive experiments on both artificial and real-world networks, showing that our approximate solution produces almost the same results as the optimal solution in practice and has an acceptable running time.}
}


@article{DBLP:journals/ton/RicardoTNS21,
	author = {Guilherme Iecker Ricardo and
                  Alina Tuholukova and
                  Giovanni Neglia and
                  Thrasyvoulos Spyropoulos},
	title = {Caching Policies for Delay Minimization in Small Cell Networks With
                  Coordinated Multi-Point Joint Transmissions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1105--1115},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3062269},
	doi = {10.1109/TNET.2021.3062269},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RicardoTNS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 5G and beyond network architectures, operators and content providers base their content distribution strategies on Heterogeneous Networks, where macro and small cells are combined to offer better Quality of Service to wireless users. On top of such networks, edge caching and Coordinated Multi-Point (CoMP) joint transmissions are used to further improve performance. In this paper, we address the average delay minimization problem by first formulating it as a static optimization problem. Even though the problem is NP-hard we are able to solve it via an efficient algorithm that guarantees a 1/2-approximation ratio. We then proceed to propose two fully distributed and dynamic caching policies for the same problem. The first one asymptotically converges to the static optimal solution under the Independent Reference Model (IRM). The second one provides better results in practice under real (non-stationary) request processes. Our online policies outperform existing dynamic solutions that are PHY-unaware.}
}


@article{DBLP:journals/ton/RozicS21,
	author = {Ciril Rozic and
                  Galen H. Sasaki},
	title = {Optical Protection Cost of Loop Free Alternates on Completely Connected
                  {IP} Networks Over Optical Rings},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1116--1127},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3061515},
	doi = {10.1109/TNET.2021.3061515},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RozicS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider protection switching in an IP over optical network. There is IP Fast Reroute Loop-free Alternates (IP FRR LFA) at the IP layer, and protection switching at the optical layer. Our network model assumes a completely connected IP network over an optical ring network that is required to protect a predetermined fraction of network bandwidth. This model leads to analytical formulas on network protection costs which we show are accurate. We also show that network cost is at or near zero when the network is required to protect at most 33% of traffic. A cost comparison to networks using MPLS Fast Reroute (MPLS FRR) further indicates that LFA is a feasible protection mechanism.}
}


@article{DBLP:journals/ton/JinHJL21,
	author = {Meng Jin and
                  Yuan He and
                  Chengkun Jiang and
                  Yunhao Liu},
	title = {Parallel Backscatter: Channel Estimation and Beyond},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1128--1140},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058977},
	doi = {10.1109/TNET.2021.3058977},
	timestamp = {Tue, 20 Dec 2022 21:20:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JinHJL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As backscatter-based IoT applications get proliferated, how to exploit backscattered signals for efficient sensing becomes a significant issue. Backscatter-based sensing requires accurate estimation of a backscatter channel (phase and amplitude), which is distorted when multiple signals collide with each other. As a result, the state of the arts is limited to either parallel decoding of collided signal or channel estimation with clean signal. Motivated by the need of high sensing capacity, we in this article present Fireworks, the first approach for channel estimation of parallel backscattered signals. The insight of Fireworks is that although the channel is distorted due to collision, the movements of the ON-OFF Keying modulated signal still preserve the channel properties of the respective tags. By modeling the relationship between the channels and the signal's moving trajectory in the IQ domain, one can make accurate estimation of the channels directly from the collision. We address practical problems of Fireworks, such as the high computing complexity and the compatibility with the commercial MAC protocol, and implement Fireworks. The results show that Fireworks is able to estimate the channels of up to five tags in parallel. When applied to the tracking application, Fireworks achieves 2 ~ 4× improvement in the tracking accuracy, compared with the state-of-the-art approach.}
}


@article{DBLP:journals/ton/DemianiukKN21,
	author = {Vitalii Demianiuk and
                  Kirill Kogan and
                  Sergey I. Nikolenko},
	title = {Approximate Packet Classifiers With Controlled Accuracy},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1141--1154},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056948},
	doi = {10.1109/TNET.2021.3056948},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DemianiukKN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing exact computations can require significant resources. Approximate computing allows to alleviate resource constraints, sacrificing the accuracy of results. In this work, we consider a generalization of the classical packet classification problem. Our major contribution is to introduce representations of approximate packet classifiers with controlled accuracy and optimization techniques to reduce classifier sizes exploiting this new level of flexibility. In this work, we propose methods constructing efficient approximate representations for both LPM (longest prefix match) classifiers and classifiers with general ternary-bit filters. We validate our theoretical results with a comprehensive evaluation study showing that a small error in the actions of a classifier can lead to significant memory reductions, often comparable to the best possible theoretical reduction in the trivial case when all rules have the same action.}
}


@article{DBLP:journals/ton/BocheSP21,
	author = {Holger Boche and
                  Rafael F. Schaefer and
                  H. Vincent Poor},
	title = {On the Algorithmic Solvability of Channel Dependent Classification
                  Problems in Communication Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1155--1168},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3059920},
	doi = {10.1109/TNET.2021.3059920},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BocheSP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For communication systems there is a recent trend towards shifting functionalities from the physical layer to higher layers by enabling software-focused solutions. Having obtained a (physical layer-based) description of the communication channel, such approaches exploit this knowledge to enable various services by subsequently processing it on higher layers. For this it is a crucial task to first find out in which state the underlying communication channel is. This paper develops a framework based on Turing machines and studies whether or not it is in principle possible to algorithmically solve such classification tasks, i.e., to decide in which state the communication system is. Turing machines have no limitations on computational complexity, computing capacity and storage, and can simulate any given algorithm and therewith are a simple but very powerful model of computation. They characterize the fundamental performance limits for today's digital computers. It is shown that there exists no Turing machine that takes the physical description of the communication channel as an input and solves a non-trivial classification task. Subsequently, this general result is used to study communication under adversarial attacks and it is shown that it is impossible to algorithmically detect denial-of-service (DoS) attacks on the transmission. Jamming attacks on ACK/NACK feedback cannot be detected as well and, in addition, ACK/NACK feedback is shown to be useless for the detection of DoS on the actual message transmission. Further applications are discussed including DoS attacks on the Post Shannon task of identification, and on physical layer security and resilience by design.}
}


@article{DBLP:journals/ton/XiaZLWM21,
	author = {Dan Xia and
                  Xiaolong Zheng and
                  Liang Liu and
                  Chaoyu Wang and
                  Huadong Ma},
	title = {c-Chirp: Towards Symmetric Cross-Technology Communication Over Asymmetric
                  Channels},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1169--1182},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3061083},
	doi = {10.1109/TNET.2021.3061083},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaZLWM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-Technology Communication (CTC) is an emerging technique that enables direct interconnection among incompatible wireless technologies. However, CTC channels are inherently asymmetric because of either the one-way nature of emulation or the asymmetric communication range caused by the asymmetric transmission power. In this paper, we focus on establishing symmetric CTC over asymmetric CTC channels. The bottleneck is the short communication range from the low-power and narrow-band technology to the high-power and wide-band technology. To compensate the inevitable distortions, we take advantage of the channel asymmetry and construct chirps in WiFi Channel State Information (CSI) to extend the communication range from ZigBee to WiFi. We build the theoretical model of CSI chirp based CTC and design c-Chirp, a novel CTC from ZigBee to WiFi. Due to channel asymmetry and discreteness, the WiFi receiver can only observe partial and distorted CSI chirps. To cope with this issue, we design a matching based chirp decoding method as well as an adaptation algorithm to reliably decode the symbols. We further extend c-Chirp to one-to-multiple concurrent transmission scenario. The evaluation results show that c-Chirp can achieve a communication range of 60m, which is 6× longer than ZigFi, an existing representative CTC from ZigBee to WiFi.}
}


@article{DBLP:journals/ton/HuangLLWH21,
	author = {Jiawei Huang and
                  Wenjun Lyu and
                  Weihe Li and
                  Jianxin Wang and
                  Tian He},
	title = {Mitigating Packet Reordering for Random Packet Spraying in Data Center
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1183--1196},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3056601},
	doi = {10.1109/TNET.2021.3056601},
	timestamp = {Thu, 11 Aug 2022 15:50:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangLLWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data center networks are usually constructed in multi-rooted tree topologies, which require the highly efficient multi-path load balancing to achieve high link utilization. Recent packet-level load balancer obtains high throughput by spraying packets to all paths, but it easily leads to the packet reordering under network asymmetry. The flow-level or flowlet-level load balancer avoids the packet reordering, while reducing the link utilization due to their inflexibility. To solve these problems, we design a Queueing Delay Aware Packet Spraying (QDAPS), that effectively mitigates the packet reordering for packet-level load balancer. QDAPS selects paths for packets according to the queueing delay of output buffer, and lets the packet arriving earlier be forwarded before the later packets to avoid packet reordering. Moreover, we adopt the “power-of-\nn\n-choices” paradigm on QDAPS to alleviate the impact of herd behavior under multiple forwarding engines. We compare QDAPS with ECMP, LetFlow and RPS through NS2 simulation and Mininet implementation. The test results show that QDAPS reduces flow completion time (FCT) by ~30%-50% over the state-of-the-art load balancing mechanism.}
}


@article{DBLP:journals/ton/YangCC21,
	author = {Yang Yang and
                  Yanjiao Chen and
                  Fei Chen},
	title = {A Compressive Integrity Auditing Protocol for Secure Cloud Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1197--1209},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058130},
	doi = {10.1109/TNET.2021.3058130},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread application of cloud storage, ensuring the integrity of user outsourced data catches more and more attention. To remotely check the integrity of cloud storage, plenty of protocols have been proposed, implemented by checking the equation constructed by the aggregated blocks, tags, and indices. However, the verifier only has the knowledge of the indices of the audited blocks and tags, which thus requires the cloud to store both data blocks and tags for integrity verification. In this article, we present a compressive secure cloud storage protocol inspired by Goldreich-Goldwasser-Halevi (GGH) cryptosystem. Since the aggregated blocks can be reconstructed from the aggregated tags without the help of data indices, the cloud can only store data tags for providing the verifiable integrity proof. In this way, communication and storage costs can be hugely reduced and user private information can be hidden from the cloud. Furthermore, the proposed protocol only contains a few basic algebraic operations, making it highly efficient. We also provide formal security proof of the proposed protocol regarding forge, replay and replace attacks. In addition, we explore a new technique to support data dynamics. Furthermore, we establish a generic framework of compressive secure cloud storage protocols. Finally, we provide the theoretical analysis and experimental results, which further validate the effectiveness of the proposed protocol.}
}


@article{DBLP:journals/ton/NaveenS21,
	author = {Kolar Purushothama Naveen and
                  Rajesh Sundaresan},
	title = {Double-Auction Mechanisms for Resource Trading Markets},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1210--1223},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058251},
	doi = {10.1109/TNET.2021.3058251},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NaveenS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a double-auction mechanism, which was recently proposed in the context of rate allocation in mobile data-offloading markets; our mechanism is also applicable to the problem of bandwidth allocation in network slicing markets. Network operators (users) derive benefit from offloading their traffic to third party WiFi or femtocell networks (link-suppliers). Link-suppliers experience costs for the additional capacity that they provide. Users and link-suppliers (collectively referred to as agents) have their pay-offs and cost functions as private knowledge. A network-manager decomposes the problem into a network problem (with surrogate pay-offs and surrogate cost functions) and agent problems (one per agent). The surrogate pay-offs and cost functions are modulated by the agents' bids. Agents' payoffs and costs are then determined by the allocations and prices set by the network-manager. Under this design, so long as the agents do not anticipate the effect of their actions on the prices set by the network-manager (i.e., price-taking agents), a competitive equilibrium exists as a solution to the network and agent problems, and this equilibrium optimizes the sum utility of all agents. However, this design fails when the agents (including the link-supplier) are all strategic (price-anticipating). Specifically, the presence of a strategic link-supplier drives the system to an undesirable equilibrium with zero participation resulting in an efficiency loss of 100%. This is in stark contrast to an earlier setting where the users alone are strategic but the link-supplier is not - the efficiency loss is known to be at most 34%. The paper then proposes the following Stackelberg game modification with asymmetric information structures for link-supplier and users in order to alleviate the efficiency-loss problem: the network-manager first announces the allocation and payment functions; he then invites the link-supplier to announce its bid, following which the users are invited to respond with their bids. The resulting Stackelberg games' efficiency losses can be characterized in terms of the link-supplier's cost function when the users' pay-off functions are linear. Specifically, when the link-supplier's cost function is quadratic, the worst case efficiency loss is 25%. Further, the loss in efficiency improves for polynomial cost functions of higher degree. For non-linear utility functions (e.g., α-fair and log utilities), we demonstrate the efficacy of the proposed mechanism via. a detailed numerical study.}
}


@article{DBLP:journals/ton/SinghK21,
	author = {Rahul Singh and
                  P. R. Kumar},
	title = {Adaptive {CSMA} for Decentralized Scheduling of Multi-Hop Networks
                  With End-to-End Deadline Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1224--1237},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3063626},
	doi = {10.1109/TNET.2021.3063626},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SinghK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consider a multihop wireless network serving multiple flows in which wireless interference constraints between links are described by a link-interference graph. The timely-throughput of a flow is defined as the throughput of packets of that flow that reach their destination node within a specified deadline, and the weighted timely throughput of the network is their weighted average over the flows with a given set of positive weights. The problem is particularly challenging, and has generally been open, when there is wireless interference between transmissions. We show that a modified CSMA routing-scheduling policy with an appropriate set of attempt probabilities is nearly optimal for maximizing weighted timely-throughput. This policy has the useful property that the routing-scheduling decision for an individual packet is solely a function of its location and time-to-deadline, and so a wireless node does not require knowledge of the global network state. It is easily implementable in a decentralized fashion by the nodes given the attempt probabilities. A gradient-based adaptive CSMA routing-scheduling policy to determine the optimal attempt probabilities is further provided. It moves along the gradient of the timely throughput and converges to a local maximum.}
}


@article{DBLP:journals/ton/SallamJ21,
	author = {Gamal Sallam and
                  Bo Ji},
	title = {Joint Placement and Allocation of {VNF} Nodes With Budget and Capacity
                  Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1238--1251},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058378},
	doi = {10.1109/TNET.2021.3058378},
	timestamp = {Tue, 25 Jan 2022 14:05:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SallamJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advent of Network Function Virtualization (NFV), network services that traditionally run on proprietary dedicated hardware can now be realized using Virtual Network Functions (VNFs) that are hosted on general-purpose commodity hardware. This new network paradigm offers a great flexibility to Internet service providers (ISPs) for efficiently operating their networks (collecting network statistics, enforcing management policies, etc.). However, introducing NFV requires an investment to deploy VNFs at certain network nodes (called VNF-nodes), which has to account for practical constraints such as the deployment budget and the VNF-node capacity. To that end, it is important to design a joint VNF-nodes placement and capacity allocation algorithm that can maximize the total amount of network flows that are fully processed by the VNF-nodes while respecting such practical constraints. In contrast to most prior work that often neglects either the budget constraint or the capacity constraint, we explicitly consider both of them. We prove that accounting for these constraints introduces several new challenges. Specifically, we prove that the studied problem is not only NP-hard but also non-submodular. To address these challenges, we introduce a novel relaxation method such that the objective function of the relaxed placement subproblem becomes submodular. Leveraging this useful submodular property, we propose two algorithms that achieve an approximation ratio of \\frac 12(1-1/e) and \\frac 13(1-1/e) for the original non-relaxed problem, respectively. Finally, we corroborate the effectiveness of the proposed algorithms through extensive evaluations using trace-driven simulations.}
}


@article{DBLP:journals/ton/AnYL21,
	author = {Zhenlin An and
                  Lei Yang and
                  Qiongzheng Lin},
	title = {Identifying {UHF} RFIDs in Range of Readers With WiFi},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1252--1265},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3057392},
	doi = {10.1109/TNET.2021.3057392},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AnYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in Cross-Technology Communication (CTC) have improved efficient cooperation among heterogeneous wireless devices. To date, however, even the most effective CTC systems require these devices to operate in the same ISM band (e.g., 2.4GHz) because of the conventional wisdom that wireless transceivers with different (fundamental) frequencies cannot communicate with one another. Our work, which is called TiFi, challenges this belief by allowing a 2.4GHz WiFi receiver (e.g., a smartphone) to identify UHF RFID tags, which operate at the spectrum between 840~920 MHz. TiFi does not require changing current smartphones or tags. Instead, it leverages the underlying harmonic backscattering of tags to open a second channel and uses it to communicate with WiFi receivers. We design and implement TiFi with commodity WiFi chipsets (e.g., Broadcom BCM43xx, Murata KM6D280 40, and Qualcomm WCN3990). Our comprehensive evaluation shows that TiFi allows WiFi receivers to identify UHF RFID tags within the range of 2 m and with a median goodput of 95%, which is comparable to today's mobile RFID readers.}
}


@article{DBLP:journals/ton/AlasmarCZP21,
	author = {Mohammed Alasmar and
                  Richard G. Clegg and
                  Nickolay Zakhleniuk and
                  George Parisis},
	title = {Internet Traffic Volumes are Not Gaussian - They are Log-Normal: An
                  18-Year Longitudinal Study With Implications for Modelling and Prediction},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1266--1279},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3059542},
	doi = {10.1109/TNET.2021.3059542},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AlasmarCZP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Getting good statistical models of traffic on network links is a well-known, often-studied problem. A lot of attention has been given to correlation patterns and flow duration. The distribution of the amount of traffic per unit time is an equally important but less studied problem. We study a large number of traffic traces from many different networks including academic, commercial and residential networks using state-of-the-art statistical techniques. We show that traffic obeys the log-normal distribution which is a better fit than the Gaussian distribution commonly claimed in the literature. We also investigate an alternative heavy-tailed distribution (the Weibull) and show that its performance is better than Gaussian but worse than log-normal. We examine anomalous traces which exhibit a poor fit for all distributions tried and show that this is often due to traffic outages or links that hit maximum capacity. We demonstrate that the data we look at is stationary if we consider samples of 15-minute long or even 1-hour long. This gives confidence that we can use the distributions for estimation and modelling purposes. We demonstrate the utility of our findings in two contexts: predicting that the proportion of time traffic will exceed a given level (for service level agreement or link capacity estimation) and predicting 95th percentile pricing. We also show that the log-normal distribution is a better predictor than Gaussian or Weibull distributions in both contexts.}
}


@article{DBLP:journals/ton/TanZXLHL21,
	author = {Haisheng Tan and
                  Chi Zhang and
                  Chao Xu and
                  Yupeng Li and
                  Zhenhua Han and
                  Xiang{-}Yang Li},
	title = {Regularization-Based Coflow Scheduling in Optical Circuit Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1280--1293},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058164},
	doi = {10.1109/TNET.2021.3058164},
	timestamp = {Thu, 20 Jan 2022 16:38:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TanZXLHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve the application-level data efficiency, the scheduling of coflows, defined as a collection of parallel flows sharing the same objective, is prevailing in recent data centers. Meanwhile, optical circuit switches (OCS) are gradually applied to provide high data rate with low power consumption. However, so far few research outputs have covered the flow, let alone the coflow, scheduling in the context of OCS. In this work, we investigate coflow scheduling in OCS-based data centers. We first derive a novel operation called regularization processed respectively on the flow traffic demands and the flow start times, which can be efficiently implemented and reduce the circuit reconfiguration frequency dramatically. We then propose a 2-approximation algorithm, called Reco-Sin, for single coflow scheduling to minimize the coflow completion time (CCT). For multiple coflows, we derive Reco-Mul to minimize the total weighted CCT, which can transform any non-preemptive multi-coflow scheduling in packet switches to a scheduling scheme in OCS. Reco-Mul can achieve a constant approximation under the assumption that no tiny flows will be transmitted in OCS. To get rid of this assumption, we present another multiple coflow scheduling scheme, named Reco-Mul+, which has an approximation ratio of O(K). Here, K is the total number of coflows. Extensive simulations based on Facebook data traces show that our approaches outperform state-of-the-art schemes significantly, i.e., one single coflow can be finished up to 1.97× faster with Reco-Sin, and multiple coflows can be completed up to more than 2× faster with Reco-Mul and Reco-Mul+.}
}


@article{DBLP:journals/ton/ChiuH21,
	author = {Cho{-}Chun Chiu and
                  Ting He},
	title = {Stealthy DGoS Attack: DeGrading of Service Under the Watch of Network
                  Tomography},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1294--1307},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058230},
	doi = {10.1109/TNET.2021.3058230},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChiuH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network tomography is a powerful tool to monitor the internal state of a closed network that cannot be measured directly, with broad applications in the Internet, overlay networks, and all-optical networks. However, existing network tomography solutions all assume that the measurements are trust-worthy, leaving open how effective they are in an adversarial environment with possibly manipulated measurements. To understand the fundamental limit of network tomography in such a setting, we formulate and analyze a novel type of attack that aims at maximally degrading the performance of targeted paths without being localized by network tomography. By analyzing properties of the optimal attack strategy, we formulate novel combinatorial optimizations to design the optimal attack strategy, which are then linked to well-known NP-hard problems and approximation algorithms. As a byproduct, our algorithms also identify approximations of the most vulnerable set of links that once manipulated, can inflict the maximum performance degradation. Our evaluations on real topologies demonstrate the large potential damage of such attacks, signaling the need of new defenses.}
}


@article{DBLP:journals/ton/GuoXSCQC21,
	author = {Deke Guo and
                  Junjie Xie and
                  Xiaofeng Shi and
                  Haofan Cai and
                  Chen Qian and
                  Honghui Chen},
	title = {{HDS:} {A} Fast Hybrid Data Location Service for Hierarchical Mobile
                  Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1308--1320},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3058401},
	doi = {10.1109/TNET.2021.3058401},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoXSCQC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The hierarchical mobile edge computing satisfies the stringent latency requirements of data access and processing for emerging edge applications. The data location service is a basic function to provide data storage and retrieval to enable these applications. However, it still lacks research of a scalable and low-latency data location service in the environment. The existing solutions, such as DNS and DHT, fail to meet the requirement of those latency-sensitive applications. Therefore, in this article, we present a low-latency hybrid data-sharing framework, HDS. The HDS divides the data location service into two parts: intra-region and inter-region. More precisely, we design a data sharing protocol called Cuckoo Summary to achieve fast data localization in intra-region. Furthermore, for the inter-region data sharing, we develop a geographic routing based scheme to achieve efficient data localization with only one overlay hop. The advantages of HDS include short response latency, low implementation overhead, and few false positives. We implement the HDS framework based on a P4 prototype. The experimental results show that, compared to the state-of-the-art solutions, our design achieves 50.21% shorter lookup paths and 92.75% fewer false positives.}
}


@article{DBLP:journals/ton/YangS21,
	author = {Feihong Yang and
                  Yuan Shen},
	title = {Critical Intensity for Unbounded Sequential Localizability},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1321--1334},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3059743},
	doi = {10.1109/TNET.2021.3059743},
	timestamp = {Mon, 03 Jan 2022 15:13:51 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Locations of mobile agents are often requisite information for wireless applications such as sensor networks and Internet of Things (IoT). As the network size increases, verifying the localizability of all nodes in a network quickly becomes intractable. In this article, we turn to analyzing the unbounded localizability of infinite stochastic networks under sequential localization methods. Specifically, we prove the existence of the phase transition on the probability of localizing an unbounded subnetwork from a bounded initial anchor set in Poisson point process networks. The phase transition occurs when the node intensity of the network reaches a critical intensity, which is determined by the adopted sequential localization method. Furthermore, we develop a simulation method to obtain tight upper and lower bounds of the critical intensity for two-dimensional (2-D) networks with high confidence, and provide the numerical bounds under several typical sequential localization methods. We also show by simulation that the percentage of localizable nodes increases rapidly near the critical intensity, which provides guidelines for network design and deployment.}
}


@article{DBLP:journals/ton/XinS21,
	author = {Liangxiao Xin and
                  David Starobinski},
	title = {Countering Cascading Denial of Service Attacks on Wi-Fi Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1335--1348},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3062363},
	doi = {10.1109/TNET.2021.3062363},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XinS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent work demonstrates that IEEE 802.11 networks are vulnerable to cascading DoS attacks, wherein a single node can remotely and suddenly congest an entire network. In this paper, we propose, analyze, simulate, and experimentally verify a counter-measure against such attacks. Our main idea is to optimize the duration of packet transmissions in order to weaken coupling effects between neighboring pairs of nodes. Toward that end, we propose a new theoretical model that relates the utilization of neighboring pairs of nodes using a sequence of iterative equations. The model captures important specifications of the IEEE 802.11 MAC layer. Through a fixed point analysis of the sequence, we show how to optimally set the packet duration so that, on one hand, cascading DoS attacks are avoided and, on the other hand, throughput is maximized. We validate the analysis through extensive ns-3 simulations and demonstrate the effectiveness of the mitigation through experiments with real Wi-Fi cards. A key insight is that IEEE 802.11 networks with relatively large MAC overhead are less susceptible to cascading DoS attacks than networks with smaller MAC overhead.}
}


@article{DBLP:journals/ton/LimbasiyaDD21,
	author = {Trupil Limbasiya and
                  Debasis Das and
                  Sajal K. Das},
	title = {MComIoV: Secure and Energy-Efficient Message Communication Protocols
                  for Internet of Vehicles},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1349--1361},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3062766},
	doi = {10.1109/TNET.2021.3062766},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LimbasiyaDD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Vehicles (IoV) offers an emerging paradigm that deals with interconnected vehicles interacting with the infrastructure, roadside units (RSUs), sensors, and mobile devices with a goal to sense, compute, store, and transmit vital information or data over a common channel while vehicles are moving. Secure and reliable communication and efficient on-device performance are thus crucial challenges in this paradigm, particularly in presence of limited computation resources. This paper presents a novel secure and energy-efficient message communication system, called MComIoV, using a one-way hash function and elliptic curve cryptography (ECC). We evaluate MComIoV through security proof and analysis against various attacks to verify its robustness. The proposed system is also implemented and tested on Raspberry Pi 3B+. Experimental results demonstrate the efficiency in computation time, storage cost, communication overhead, and energy consumption.}
}


@article{DBLP:journals/ton/MohantiBNCSC21,
	author = {Subhramoy Mohanti and
                  Elif Bozkaya and
                  M. Yousof Naderi and
                  Berk Canberk and
                  Gokhan Secinti and
                  Kaushik R. Chowdhury},
	title = {WiFED Mobile: WiFi Friendly Energy Delivery With Mobile Distributed
                  Beamforming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1362--1375},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3061082},
	doi = {10.1109/TNET.2021.3061082},
	timestamp = {Mon, 26 Jun 2023 20:53:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MohantiBNCSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless RF energy transfer for indoor sensors is an emerging paradigm ensuring continuous operation without battery limitations. However, high power radiation within ISM band interferes with packet reception for existing WiFi devices. The paper proposes the first effort in merging RF energy transfer within a standards compliant 802.11 protocol, realizing practical and WiFi-friendly Energy Delivery with Mobile Transmitters (WiFED Mobile). WiFED Mobile architecture is composed of a centralized controller coordinating the actions of multiple energy transmitters (ETs), and deployed sensors that periodically requires charging. The paper first describes 802.11 supported protocol features that can be exploited by sensors to request energy and for ETs to participate in energy transfer. Second, it devises a controller-driven bipartite matching algorithm, assigning appropriate number of ETs to sensors for efficient energy delivery. Thirdly, it detects outlier sensors (OS), which have limited power reception from static ETs and utilizes mobile ETs (METs) to satisfy their charging cycles. The proposed in-band and protocol supported coexistence in WiFED Mobile is validated via simulations and partly in a software defined radio testbed, showing that METs reduce latency by 42% and improve throughput by 83% in scenarios where using only static ETs fails to satisfy charging cycles of OS.}
}


@article{DBLP:journals/ton/HuCTYL21,
	author = {Chunqiang Hu and
                  Xiuzhen Cheng and
                  Zhi Tian and
                  Jiguo Yu and
                  Weifeng Lv},
	title = {Achieving Privacy Preservation and Billing via Delayed Information
                  Release},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1376--1390},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3063102},
	doi = {10.1109/TNET.2021.3063102},
	timestamp = {Thu, 10 Mar 2022 09:31:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuCTYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many applications such as smart metering and location based services pose strong privacy requirements but achieving privacy protection at the client side is a non-trial problem as payment for the services must be computed by the server at the end of each billing period. In this paper, we propose a privacy preservation and billing scheme termed PPDIR based on delayed information release. PPDIR relies on a novel group signature mechanism and the asymmetric Rabin cryptosystem to protect the privacy of the clients and their requests, to achieve accountability and non-repudiation, and to shift the computational complexity to the server side. It adopts a secret token for anonymity and the token is updated for each client at the beginning of each billing period and securely released only to the server at the end of the billing period. Such a strategy can prevent the server from linking a client's requests made at different billing periods. It also prevents any adversary from linking any request to any client. Note that the server is able to figure out all requests made by a client within a billing period after receiving the delayed token, which is unavoidable for billing purpose. We prove the security properties of the group signature scheme, and analyze the security strength of PPDIR. Our study indicates that PPDIR can achieve privacy-preservation, confidentiality, non-repudiation, accountability, and other security objectives. We also evaluate the performance of our scheme in terms of communication and computational overheads.}
}


@article{DBLP:journals/ton/GuanBDM21,
	author = {Zhangyu Guan and
                  Lorenzo Bertizzolo and
                  Emrecan Demirors and
                  Tommaso Melodia},
	title = {{WNOS:} Enabling Principled Software-Defined Wireless Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1391--1407},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3064824},
	doi = {10.1109/TNET.2021.3064824},
	timestamp = {Tue, 13 Jul 2021 13:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuanBDM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article investigates the basic design principles for a new Wireless Network Operating System (WNOS), a radically different approach to software-defined networking (SDN) for infrastructure-less wireless networks. Departing from well-understood approaches inspired by OpenFlow, WNOS provides the network designer with an abstraction hiding (i) the lower-level details of the wireless protocol stack and (ii) the distributed nature of the network operations. Based on this abstract representation, the WNOS takes network control programs written on a centralized, high-level view of the network and automatically generates distributed cross-layer control programs based on distributed optimization theory that are executed by each individual node on an abstract representation of the radio hardware. We first discuss the main architectural principles of WNOS. Then, we discuss a new approach to automatically generate solution algorithms for each of the resulting subproblems in an automated fashion. Finally, we illustrate a prototype implementation of WNOS on software-defined radio devices and test its effectiveness by considering specific cross-layer control problems. Experimental results indicate that, based on the automatically generated distributed control programs, WNOS achieves 18%, 56% and 80.4% utility gain in networks with low, medium and high levels of interference; maybe more importantly, we illustrate how the global network behavior can be controlled by modifying a few lines of code on a centralized abstraction.}
}


@article{DBLP:journals/ton/ToutMKT21,
	author = {Hanine Tout and
                  Azzam Mourad and
                  Nadjia Kara and
                  Chamseddine Talhi},
	title = {Multi-Persona Mobility: Joint Cost-Effective and Resource-Aware Mobile-Edge
                  Computation Offloading},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1408--1421},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3066558},
	doi = {10.1109/TNET.2021.3066558},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ToutMKT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-persona mobile computing has begun to make its way to determine the battle about practical strategy for adopting personal devices in workplace. Though its competency, multi-persona performance and viability are critically threatened by the limited resources of mobile devices. In recent years, mobile edge computing (MEC) has risen as promising paradigm within the internet of things era bringing benefits to the proximity of mobile terminals, leveraging intelligent computations offloading services to address the severity of their resource scarcity. Yet, embracing mobile edge-based services to augment personas resources and performance raises new concerns including determining what computations to offload for serving the highest number of mobile devices and reducing the remote execution fees imposed on the institution. In this context, we propose new cost-effective MEC-based solution to address these issues. We develop two-level multi-objective optimization realized through an intelligent offloading decision model able to settle both concerns, by minimizing processing, memory and energy while augmenting virtual mobile instances performance on a wide range of physical devices with minimal offloading service fees. We also propose a redesigned smart genetic-based method able to accelerate and reduce the overhead of offloading decision evaluation. Extensive analysis is performed and the results show that our proposition can get more quickly the offloading strategy than other schemes. The results also demonstrate the ability to enforce the virtual mobile devices by reducing local processing, memory usage, energy consumption and execution time along with acceptable minimal additional fees compared to other techniques.}
}


@article{DBLP:journals/ton/ChengWLC21,
	author = {Bo Cheng and
                  Ming Wang and
                  Xiangtao Lin and
                  Junliang Chen},
	title = {Context-Aware Cognitive QoS Management for Networking Video Transmission},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {3},
	pages = {1422--1434},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3066262},
	doi = {10.1109/TNET.2021.3066262},
	timestamp = {Tue, 13 Jul 2021 13:24:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChengWLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Context-aware QoS management is a new research field dealing with methods by which traditional QoS management algorithms in mobile and fixed networking systems can have more intelligent decision-making mechanisms by fully exploiting all context information being available in their network environment. Motivated by addressing the critical problem, we propose a novel context-aware cognitive QoS management approach, which contains three main steps for context discretization, context reduction and QoS guarantee, which can provide the quantitative end-to-end QoS guarantee and management. We developed a context-aware systematic end-to-end QoS management module in a real-life multimedia conferencing system and compared its performance with existing approaches. Experimental results show that our proposed approach outperforms the existing approaches in improving the mobile and fixed networking video transmission quality.}
}


@article{DBLP:journals/ton/LiuSCNYCC21,
	author = {Jinwei Liu and
                  Haiying Shen and
                  Hongmei Chi and
                  Husnu S. Narman and
                  Yongyi Yang and
                  Long Cheng and
                  Wingyan Chung},
	title = {A Low-Cost Multi-Failure Resilient Replication Scheme for High-Data
                  Availability in Cloud Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1436--1451},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2020.3027814},
	doi = {10.1109/TNET.2020.3027814},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuSCNYCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data availability is one of the most important performance factors in cloud storage systems. To enhance data availability, replication is a common approach to handle the machine failures. However, previously proposed replication schemes cannot effectively handle both correlated and non-correlated machine failures, especially while increasing the data availability with limited resources. The schemes for correlated machine failures must create a constant number of replicas for each data object, which often neglects diverse data popularities and does not utilize the resource to maximize the expected data availability. Also, the previous schemes neglect the consistency maintenance cost and the storage cost caused by replication. It is critical for cloud providers to maximize data availability (hence minimize SLA violations) while minimizing costs caused by replication in order to maximize the revenue. In this paper, we build a nonlinear integer programming model to maximize data availability in both types of failures, and therefore minimize the cost caused by replication. Based on the model’s solution for the replication degree of each data object, we propose a low-cost multi-failure (correlated and non-correlated machine failures) resilient replication scheme (MRR). MRR can effectively handle both correlated and non-correlated machine failures, considers data popularities to enhance data availability, and also tries to minimize consistency maintenance and storage cost. Extensive numerical results from trace parameters and experiments from real-world Amazon S3 demonstrate that MRR achieves high data availability, low data loss probability and low consistency maintenance and storage costs when compared to previous replication schemes.}
}


@article{DBLP:journals/ton/GuLS21,
	author = {Yan Gu and
                  Bo Liu and
                  Xiaojun Shen},
	title = {Asymptotically Optimal Online Scheduling With Arbitrary Hard Deadlines
                  in Multi-Hop Communication Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1452--1466},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3065703},
	doi = {10.1109/TNET.2021.3065703},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper firstly proposes a greedy online packet scheduling algorithm for the problem raised by Mao, Koksal and Shroff that allows arbitrary hard deadlines in multi-hop networks aiming at maximizing the total revenue. With the same assumption of \\rho _{M} / \\rho _{m}={O}(1)\nwhere \\rho _{M}\nand \\rho _{\\textit {}m}\nare the maximum and minimum revenue a packet may carry, our algorithm is {O}\n( {P} _{M}\n)-competitive improving on MKS algorithm by a factor of {O}\n(log {P} _{M}\n), where {P} _{M}\nis the length of the longest path a packet may travel in the network. We prove that it is asymptotically optimal by presenting a lower bound of {P} _{M}\non the competitiveness for this problem. Secondly, this paper studies the extension of this problem that includes routing as a part of the solution. We prove that using the fastest path algorithm for the routing part, the greedy online algorithm also achieves asymptotically optimal competitiveness for the extended problem. Furthermore, we present a non-greedy online algorithm that not only is asymptotically optimal, but also can adaptively achieve a better competitiveness when the network has a larger {C} _{\\textit {min}}\n, where {C} _{\\textit {min}}\nis the minimum link capacity in the network. Finally, simulation results are reported, showing that not only do the greedy online algorithms achieve asymptotically optimal bounds, but also practically achieve better performance than the previously proposed algorithms.}
}


@article{DBLP:journals/ton/Behrouzi-FarS21,
	author = {Amir Behrouzi{-}Far and
                  Emina Soljanin},
	title = {Efficient Replication for Fast and Predictable Performance in Distributed
                  Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1467--1476},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3062215},
	doi = {10.1109/TNET.2021.3062215},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Behrouzi-FarS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Master-worker distributed computing systems use task replication to mitigate the effect of slow workers on job compute time. The master node groups tasks into batches and assigns each batch to one or more workers. We first assume that the batches do not overlap. Using majorization theory, we show that a balanced replication of batches minimizes the average job compute time for a general class of service time distributions. We then show that the balanced assignment of non-overlapping batches achieves a lower average job compute time than the overlapping schemes proposed in the literature. Next, we derive the optimum redundancy level as a function of the task service time distribution. We show that the redundancy level that minimizes the average job compute time may not coincide with the redundancy level that maximizes job compute time predictability. Therefore, there is a trade-off in optimizing the two metrics. By running experiments on Google cluster traces, we observe that redundancy can reduce the job compute time by one order of magnitude. The optimum level of redundancy depends on the distribution of task service time.}
}


@article{DBLP:journals/ton/YeLL21,
	author = {Jiancheng Ye and
                  Ka{-}Cheong Leung and
                  Steven H. Low},
	title = {Combating Bufferbloat in Multi-Bottleneck Networks: Theory and Algorithms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1477--1493},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3066505},
	doi = {10.1109/TNET.2021.3066505},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YeLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bufferbloat is a phenomenon in computer networks where large router buffers are frequently filled up, resulting in high queueing delay and delay variation. More and more delay-sensitive applications on the Internet have made this phenomenon a pressing issue. Interacting with the Transmission Control Protocol (TCP), active queue management (AQM) algorithms run on routers play an important role in combating bufferbloat. However, AQM algorithms have not been widely deployed due to complicated manual parameter tuning. Moreover, they are often designed and analyzed based on network models with a single bottleneck link, rendering their performance and stability unclear in multi-bottleneck networks. In this paper, we propose a general framework to combat bufferbloat in multi-bottleneck networks. We first present an equilibrium analysis for a general multi-bottleneck TCP/AQM system and provide sufficient conditions for the uniqueness of an equilibrium point in the system. We then decompose the system into single-bottleneck subsystems and derive sufficient conditions for the local asymptotic stability of the subsystems. Using our framework, we develop an algorithm to compute the equilibrium point of the system. We further present a case study to analyze the stability of the recently proposed Controlled Delay (CoDel) in multi-bottleneck networks and devise Self-Tuning CoDel to improve the system stability. Extensive numerical and packet-level simulation results not only verify our theoretical studies but also show that our proposed Self-Tuning CoDel significantly stabilizes queueing delay in multi-bottleneck networks, thereby mitigating bufferbloat.}
}


@article{DBLP:journals/ton/GopalKCR21,
	author = {Sneihil Gopal and
                  Sanjit K. Kaul and
                  Rakesh Chaturvedi and
                  Sumit Roy},
	title = {Coexistence of Age and Throughput Optimizing Networks: {A} Spectrum
                  Sharing Game},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1494--1508},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3067900},
	doi = {10.1109/TNET.2021.3067900},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GopalKCR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the coexistence of an age optimizing network (AON) and a throughput optimizing network (TON) that share a common spectrum band. We consider two modes of long run coexistence: (a) networks compete with each other for spectrum access, causing them to interfere and (b) networks cooperate to achieve non-interfering access. To model competition, we define a non-cooperative stage game parameterized by the average age of the AON at the beginning of the stage, derive its mixed strategy Nash equilibrium (MSNE), and analyze the evolution of age and throughput over an infinitely repeated game in which each network plays the MSNE at every stage. Cooperation uses a coordination device that performs a coin toss during each stage to select the network that must access the medium. Networks use the grim trigger punishment strategy, reverting to playing the MSNE every stage forever if the other disobeys the device. We determine if there exists a subgame perfect equilibrium, i.e., the networks obey the device forever as they find cooperation beneficial. We show that networks choose to cooperate only when they consist of a sufficiently small number of nodes, otherwise they prefer to disobey the device and compete.}
}


@article{DBLP:journals/ton/LiZWLXCHGLW21,
	author = {Guanyu Li and
                  Menghao Zhang and
                  Shicheng Wang and
                  Chang Liu and
                  Mingwei Xu and
                  Ang Chen and
                  Hongxin Hu and
                  Guofei Gu and
                  Qi Li and
                  Jianping Wu},
	title = {Enabling Performant, Flexible and Cost-Efficient DDoS Defense With
                  Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1509--1526},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3062621},
	doi = {10.1109/TNET.2021.3062621},
	timestamp = {Thu, 22 Dec 2022 16:48:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZWLXCHGLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Denial-of-Service (DDoS) attacks have become a critical threat to the Internet. Due to the increasing number of vulnerable Internet of Things (IoT) devices, attackers can easily compromise a large set of nodes and launch high-volume DDoS attacks from the botnets. State-of-the-art DDoS defenses, however, have not caught up with the fast development of the attacks. Middlebox-based defenses can achieve high performance with specialized hardware; however, these defenses incur a high cost, and deploying new defenses typically requires a device upgrade. On the other hand, software-based defenses are highly flexible, but software-based packet processing leads to high performance overheads. In this article, we propose Poseidon, a system that addresses these limitations in today’s DDoS defenses. It leverages emerging programmable switches, which can be reconfigured in the field without additional hardware upgrades. Users of Poseidon can specify their defense strategies in a modular fashion in the form of a set of defense primitives; this can be further customized easily for each network and extended to include new defenses. Poseidon then maps the defense primitives to run on programmable switches—and when necessary, on server software—for effective defense. When attacks change, Poseidon can reconfigure the underlying defense primitives to respond to the new attack patterns. Evaluations using our prototype demonstrate that Poseidon can effectively defend against high-volume attacks, easily support customization of defense strategies, and adapt to dynamic attacks with low overheads.}
}


@article{DBLP:journals/ton/WuGZC21,
	author = {Guanhao Wu and
                  Xiaofeng Gao and
                  Jiaqi Zheng and
                  Guihai Chen},
	title = {Achieving Fast Loop-Free Updates With Ingress Port in Software-Defined
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1527--1539},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3068177},
	doi = {10.1109/TNET.2021.3068177},
	timestamp = {Wed, 22 Nov 2023 12:10:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WuGZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the distributed and asynchronous nature in data plane, the packets can be forwarded into a loop during routing updates. Software-Defined Networks (SDNs) enable a controller to schedule the update operations of routing rules in a loop-free manner. However, the existing rule replacement mechanism cannot give an efficient solution for large-scale update scenarios. In this paper, we make use of the ingress port field in forwarding rules to design a loop-free update scheme, Inport-Matching Update (IMU), for per-flow and destination-based unicast, which significantly improves efficiency of solving update scenarios and reduces update rounds. In both our proposed and previous mechanisms, a subproblem called Rule Replacement Scheduling (RRS) problem needs to be solved and we prove that the scale of RRS problem can be reduced at least by half. Moreover, we use the inclusion-exclusion principle to rigorously prove that the probability that an RRS problem under IMU has a trivial solution, i.e., one round, is always higher than 83.7%, given a random per-flow unicast update scenario with ingress port rules. Experimental results show that our proposed Inport-Matching Update mechanism remarkably reduces the average number of rounds and runtime for solving the RRS problem compared to the existing rule replacement mechanism.}
}


@article{DBLP:journals/ton/NevesHLB21,
	author = {Miguel C. Neves and
                  Bradley Huffaker and
                  Kirill Levchenko and
                  Marinho P. Barcellos},
	title = {Dynamic Property Enforcement in Programmable Data Planes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1540--1552},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3068339},
	doi = {10.1109/TNET.2021.3068339},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NevesHLB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network programmers can currently deploy an arbitrary set of protocols in forwarding devices through data plane programming languages such as P4. However, as any other type of software, P4 programs are subject to bugs and misconfigurations. Network verification tools have been proposed as a means of ensuring that the network behaves as expected, but these tools frequently face severe scalability issues. In this paper, we argue for a novel approach to this problem. Rather than statically inspecting a network configuration looking for bugs, we propose to enforce networking properties at runtime. To this end, we developed P4box, a system for deploying runtime monitors in programmable data planes. P4box allows programmers to easily express a broad range of properties (both program-specific and network-wide). Moreover, we provide an automated framework based on assertions and symbolic execution for ensuring monitor correctness. Our experiments on a SmartNIC show that P4box monitors represent a small overhead to network devices in terms of latency, throughput and power consumption.}
}


@article{DBLP:journals/ton/XuLL21,
	author = {Huanle Xu and
                  Yang Liu and
                  Wing Cheong Lau},
	title = {Optimal Job Scheduling With Resource Packing for Heterogeneous Servers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1553--1566},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3068201},
	doi = {10.1109/TNET.2021.3068201},
	timestamp = {Fri, 10 Nov 2023 21:09:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Jobs in modern computing clusters have highly diverse processing duration and heterogeneous resource requirements. In this paper, we consider the problem of job scheduling for a computing cluster comprised of multiple servers with heterogeneous computation resources, while taking the different resource demands of the jobs into account. Our focus is to achieve a low overall job response time for the system (which is also referred to as the job flowtime) while providing fairness between small and large jobs. Since the job flowtime minimization problem under multiple (even homogeneous) servers are known to be NP-hard, we propose an approximation algorithm to tackle the original online scheduling problem by adopting the recently-proposed notion of fractional job flowtime as a surrogate objective for minimization. For the general online job arrival case with multi-dimensional resource requirements, we apply Online Convex Optimization (OCO) techniques to design the corresponding scheduling algorithm with performance guarantees. In the single-dimensional resource setting, we show that the dynamic fit of the online version of our approximate algorithm grows only sublinearly with respect to time and derive a bound for its dynamic regret when comparing to its offline counterpart. While the baseline version of our proposed scheduling algorithm assumes the possibilities of job preemption and job migration across different servers, we show that the extent of job preemption and migration can be well controlled by augmenting the objective function with the corresponding switching costs.}
}


@article{DBLP:journals/ton/HeGZJ21,
	author = {Yuan He and
                  Xiuzhen Guo and
                  Jia Zhang and
                  Haotian Jiang},
	title = {{WIDE:} Physical-Level {CTC} via Digital Emulation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1567--1579},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3071782},
	doi = {10.1109/TNET.2021.3071782},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HeGZJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-Technology Communication (CTC) is an emerging technique that enables direct communication across different wireless technologies. Recent works achieve physical-level CTC by emulating the standard time-domain waveform of the receiver. This method faces the challenges of inherent unreliability due to the imperfect emulation. Different from analog emulation, we propose a novel concept named digital emulation, which stems from the following insight: The receiver relies on phase shift rather than the phase itself to decode signals. Instead of emulating the original time-domain waveform, the sender emulates the phase shift associated with the desired signals. Clearly there are multiple different phase sequences that correspond to the same signs of phase shifts. Digital emulation has flexibility in setting the phase values in the emulated signals, which is effective in reducing emulation errors and enhancing the reliability of CTC. The key point of digital emulation is generic and applicable to a set of CTCs, where the transmitter has a wider bandwidth for emulation and the receiver decoding is based on the phase shift. In this paper, we implement our proposal as WIDE, a physical-level CTC via digital emulation from WiFi to ZigBee. We conduct extensive experiments to evaluate the performance of WIDE. The results show that WIDE significantly improves the Packet Reception Ratio (PRR) from 41.7% to 86.2%, which is 2× of WEBee's, an existing representative physical-level CTC.}
}


@article{DBLP:journals/ton/ZhangMLL21,
	author = {Ziyao Zhang and
                  Liang Ma and
                  Kin K. Leung and
                  Franck Le},
	title = {More Is Not Always Better: An Analytical Study of Controller Synchronizations
                  in Distributed {SDN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1580--1590},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3066580},
	doi = {10.1109/TNET.2021.3066580},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangMLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed software-defined networks (SDN), consisting of multiple inter-connected network domains, each managed by one SDN controller, is an emerging networking architecture that offers balanced centralized control and distributed operations. In such networking paradigm, most existing works focus on designing sophisticated controller-synchronization strategies to improve joint controller-decision-making for inter-domain routing. However, there is still a lack of fundamental understanding of how the performance of distributed SDN is related to network attributes, thus impossible to justify the necessity of complicated strategies. In this regard, we analyse and quantify how the performance enhancement of distributed SDN architectures is influenced by inter-domain synchronization levels, in terms of the resulting number of abstracted routing clusters, and network structural properties. Based on a generic network model incorporating link preference for path constructions, we establish analytical lower bounds for quantifying the routing performance under any arbitrarily given network synchronization status. The significance of these performance bounds is that they can be used to quantify the contribution of controller synchronization levels in improving the network performance under different network parameters, which therefore serves as a fundamental guidance for future SDN performance analysis and protocol designs.}
}


@article{DBLP:journals/ton/ChenDKZ21,
	author = {Tingjun Chen and
                  Mahmood Baraani Dastjerdi and
                  Harish Krishnaswamy and
                  Gil Zussman},
	title = {Wideband Full-Duplex Phased Array With Joint Transmit and Receive
                  Beamforming: Optimization and Rate Gains},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1591--1604},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3069125},
	doi = {10.1109/TNET.2021.3069125},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenDKZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Full-duplex (FD) wireless and phased arrays are both promising techniques that can significantly improve data rates in future wireless networks. However, integrating FD with transmit (Tx) and receive (Rx) phased arrays is extremely challenging, due to the large number of self-interference (SI) channels. Previous work relies on either RF canceller hardware or on analog/digital Tx beamforming (TxBF) to achieve SI cancellation (SIC). However, Rx beamforming (RxBF) and the data rate gain introduced by FD nodes employing beamforming have not been considered yet. We study FD phased arrays with joint TxBF and RxBF with the objective of achieving improved FD data rates. The key idea is to carefully select the TxBF and RxBF weights to achieve wideband RF SIC in the spatial domain with minimal TxBF and RxBF gain losses. Essentially, TxBF and RxBF are repurposed, thereby not requiring specialized RF canceller circuitry. We formulate the corresponding optimization problem and develop an iterative algorithm to obtain an approximate solution with provable performance guarantees. Using SI channel measurements and datasets, we extensively evaluate the performance of the proposed approach in different use cases under various network settings. The results show that an FD phased array with 9/36/72 elements can cancel the total SI power to below the noise floor with sum TxBF and RxBF gain losses of 10.6/7.2/6.9dB, even at Tx power level of 30dBm. Moreover, the corresponding FD rate gains are at least 1.33/1.66/1.68 ×.}
}


@article{DBLP:journals/ton/ChenZWXXW21,
	author = {Zhe Chen and
                  Xu Zhang and
                  Sulei Wang and
                  Yuedong Xu and
                  Jie Xiong and
                  Xin Wang},
	title = {Enabling Practical Large-Scale {MIMO} in WLANs With Hybrid Beamforming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1605--1619},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3073160},
	doi = {10.1109/TNET.2021.3073160},
	timestamp = {Tue, 01 Aug 2023 17:16:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenZWXXW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In theory, the capacity of a wireless network grows linearly with the number of users and antennas equipped at the communication devices, and hence large-scale MU-MIMO can scale up the network throughput. However, three main challenges are impeding the implementation of this promising technology in the state-of-the-art WLANs. Firstly, the current large-scale MU-MIMO technology demands a large number of high-priced RF chains. Secondly, the wireless access points (APs) are overwhelmed by channel state information (CSI) feedback for nulling multi-user and -antenna interference. Thirdly, the lack of scalable user selection scheme limits the capability of APs to serve a large user population. To address these problems, we design BUSH, a large-scale MU-MIMO prototype that performs scalable beam user selection with hybrid beamforming for phased-array antennas in legacy WLANs. We design a low complexity algorithm that assigns each pair of RF chain and analog beam to the users to effectively reduce channel correlation and cross-talk interference without instantaneous CSI feedbacks. As a prerequisite of user selection, BUSH presents a low-overhead probing scheme in multi-carrier WLANs and designs a highly accurate blind Power Azimuth Spectrum (PAS) estimation algorithm using a single RF chain. For reducing the number of RF-chains used, the phased-array antennas use analog beamforming to steer beams toward each selected downlink user, and multiple RF chains use beamforming to further mitigate the interference among users. We implement BUSH on a software-defined radio platform and evaluate its performance in more than 30 indoor scenarios. The experimental results reveal that for throughput, BUSH outperforms the legacy 802.11ac by 2.08×, and an alternative benchmark system by 1.22× on average.}
}


@article{DBLP:journals/ton/ChangSL21,
	author = {Cheng{-}Shang Chang and
                  Jang{-}Ping Sheu and
                  Yi{-}Jheng Lin},
	title = {On the Theoretical Gap of Channel Hopping Sequences With Maximum Rendezvous
                  Diversity in the Multichannel Rendezvous Problem},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1620--1633},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3067643},
	doi = {10.1109/TNET.2021.3067643},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChangSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the literature, there are several well-known periodic channel hopping (CH) sequences that can achieve maximum rendezvous diversity in a cognitive radio network (CRN). For a CRN with N channels, it is known that the period of such a CH sequence is at least N 2 . The asymptotic approximation ratio, defined as the ratio of the period of a CH sequence to the lower bound N 2 when N → ∞, is still 2.5 for the best known CH sequence in the literature. An open question in the multichannel rendezvous problem is whether it is possible to construct a periodic CH sequence that has the asymptotic approximation ratio of 1. In this paper, we tighten the theoretical gap by proposing CH sequences, called IDEAL-CH, that have the asymptotic approximation ratio of 2. For a weaker requirement that only needs the two users to rendezvous on one commonly available channel in a period, we propose channel hopping sequences, called ORTHO-CH, with period (2 p+1) p, where p is the smallest prime not less than N.}
}


@article{DBLP:journals/ton/ChatterjeeWAO21,
	author = {Bijoy Chand Chatterjee and
                  Abdul Wadud and
                  Imran Ahmed and
                  Eiji Oki},
	title = {Priority-Based Inter-Core and Inter-Mode Crosstalk-Avoided Resource
                  Allocation for Spectrally-Spatially Elastic Optical Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1634--1647},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3068212},
	doi = {10.1109/TNET.2021.3068212},
	timestamp = {Mon, 05 Feb 2024 20:24:13 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChatterjeeWAO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrally-spatially elastic optical networks (SS-EONs) have been considered nowadays to overcome the physical barrier and enhance the transport capacity, where enhancing spectrum utilization while satisfying inter-core and inter-mode crosstalks is always challenging. This paper proposes a priority-based crosstalk-avoided core, mode and spectrum allocation scheme in SS-EONs, which enhances resource utilization while satisfying both constraints of inter-core crosstalk and inter-mode crosstalk. The proposed scheme creates different groups of cores and modes and assigns a priority to each of them. Core and mode are selected for serving lightpath requests based on their priority order. We define an optimization problem for routing, modulation assignment, spectrum, core, and mode allocation (RA-SCMA) in SS-EONs considering both constraints of inter-core crosstalk and inter-mode crosstalk simultaneously. The optimization problem is formulated as an integer linear programming problem. We prove that the decision version of RA-SCMA is NP-complete. We present crosstalk-avoided core-mode-spectrum allocation considering a dynamic scenario. Numerical results indicate that the proposed scheme reduces the blocking probability in SS-EONs and allows up to 40% increased traffic loads by utilizing the crosstalk-avoided unutilized slots compared to the conventional scheme that adopts a core-mode-spectrum first fit policy.}
}


@article{DBLP:journals/ton/VassTB21,
	author = {Bal{\'{a}}zs Vass and
                  J{\'{a}}nos Tapolcai and
                  Erika R. B{\'{e}}rczi{-}Kov{\'{a}}cs},
	title = {Enumerating Maximal Shared Risk Link Groups of Circular Disk Failures
                  Hitting k Nodes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1648--1661},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3070100},
	doi = {10.1109/TNET.2021.3070100},
	timestamp = {Thu, 14 Oct 2021 08:51:09 +0200},
	biburl = {https://dblp.org/rec/journals/ton/VassTB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many recent studies shed light on the vulnerability of networks against large-scale natural disasters. The corresponding network failures, called regional failures, are manifested at failing multiple network elements that are physically close to each other. The recovery mechanisms of current backbone networks protect failures listed as Shared Risk Link Groups (SRLGs). We aim to design an algorithm for the routing engines, which can generate a reasonable list of SRLGs based on the limited geometric information available. As a first step towards this direction, in this paper, we propose a limited geographic information failure model for the network topology that enables efficient algorithms to compute the set of links that are expected to be close to each other. More precisely, we work with (1) relative node positions without knowing the real distances, (2) an area in the map defines the route of each physical cable, and (3) a regional failure is a circular disk with k=0,1, ... nodes in its interior. We describe an efficient algorithm for listing SRLGs based on our limited geographic information failure model and show that under realistic assumptions, the obtained list of SRLGs is short, having approximately 1.2 n and 2.2n elements for k=0 and k=1, respectively, where n is the number of nodes of the network.}
}


@article{DBLP:journals/ton/ZouYYZDC21,
	author = {Yifei Zou and
                  Dongxiao Yu and
                  Jiguo Yu and
                  Yong Zhang and
                  Falko Dressler and
                  Xiuzhen Cheng},
	title = {Distributed Byzantine-Resilient Multiple-Message Dissemination in
                  Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1662--1675},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3069324},
	doi = {10.1109/TNET.2021.3069324},
	timestamp = {Thu, 10 Mar 2022 09:31:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZouYYZDC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The byzantine model is widely used to depict a variety of node faults in networks. Previous studies on byzantine-resilient protocols in wireless networks assume reliable communications and do not consider the jamming behavior of byzantine nodes. Such jamming, however, is a very critical and realistic behavior to be considered in modern wireless networks. In this paper, for the first time, we integrate the jamming behavior of byzantine nodes into the network setting. We show that, in this much more comprehensive and harsh model, efficient distributed communication protocols can be still devised with elaborate protocol design. In particular, we developed an algorithm that can accomplish the basic multiple-message dissemination task close to the optimal solution in terms of running time. Empirical results validate the byzantine-resilience and efficiency of our algorithm.}
}


@article{DBLP:journals/ton/ZhangY21,
	author = {Yuhui Zhang and
                  Dejun Yang},
	title = {RobustPay\({}^{\mbox{+}}\): Robust Payment Routing With Approximation
                  Guarantee in Blockchain-Based Payment Channel Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1676--1686},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3069725},
	doi = {10.1109/TNET.2021.3069725},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The past decade has witnessed an explosive growth in cryptocurrencies, but the blockchain-based cryptocurrencies have also raised many concerns, among which a crucial one is the scalability issue. Suffering from the large overhead of global consensus and security assurance, even the leading cryptocurrencies can only handle up to tens of transactions per second, which largely limits their applications in real-world scenarios. Among many proposals to improve the cryptocurrency scalability, one of the most promising and mature solutions is the payment channel network (PCN), which offers the off-chain settlement of transactions with minimal involvement of expensive blockchain operations. However, transaction failures may occur due to external attacks or unexpected conditions, e.g., an uncooperative user becoming unresponsive. In this paper, we present a distributed robust payment routing protocol RobustPay + to resist transaction failures, which achieves robustness, efficiency, distributedness and approximate optimization. Specifically, we investigate the problem of robust routing in PCNs from an optimization perspective, which is to find a pair of payment paths for a payment request, while minimizing the worst-case transaction fee, subject to the timeliness and feasibility constraints. We present a distributed 2-approximation algorithm for this problem. Moreover, we modify the original Hashed Time-lock Contract (HTLC) protocol and adapt it to the robust payment routing protocol to achieve robustness and efficiency. Extensive simulations demonstrate that RobustPay + significantly outperforms baseline algorithms in terms of the success ratio and the average accepted value.}
}


@article{DBLP:journals/ton/LiZLYC21,
	author = {Ning Li and
                  Zhaoxin Zhang and
                  Alex X. Liu and
                  Xin Yuan and
                  Yexia Cheng},
	title = {Pairwise-Based Multi-Attribute Decision Making Approach for Wireless
                  Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1687--1702},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3074002},
	doi = {10.1109/TNET.2021.3074002},
	timestamp = {Thu, 14 Oct 2021 08:51:10 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiZLYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In wireless network applications, such as routing decision, network selection, etc., the Multi-Attribute Decision Making (MADM) is widely used. The MADM approach can address the multi-objective decision making issues effectively. However, when the parameters vary greatly, the traditional MADM algorithm is not effective anymore. To solve this problem, in this paper, we propose the pairwise-based MADM algorithm. In the PMADM, only two nodes' utilities are calculated and compared at each time. The PMADM algorithm is much more accurate than the traditional MADM algorithm. Moreover, we also prove that the PMADM algorithm is sensitive to the parameters which vary seriously and insensitive to the parameters which change slightly. This property is better than that of the traditional MADM algorithm. Additionally, the PMADM algorithm is more stable than traditional MADM algorithm. For reducing the computational complexity of the PMADM algorithm, we propose the low-complexity PMADM algorithm. For analyzing the computational complexity of the l PMADM algorithm, we propose the tree-based decomposing algorithm in this paper. The l PMADM algorithm has the same properties and performances as that of the PMADM algorithm; however, it is simpler than the PMADM algorithm. The simulation results show that the PMADM and l PMADM algorithms are much more effective than the traditional MADM algorithm.}
}


@article{DBLP:journals/ton/TsanikidisG21,
	author = {Christos Tsanikidis and
                  Javad Ghaderi},
	title = {On the Power of Randomization for Scheduling Real-Time Traffic in
                  Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1703--1716},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3072279},
	doi = {10.1109/TNET.2021.3072279},
	timestamp = {Thu, 16 Sep 2021 17:57:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TsanikidisG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider the problem of scheduling real-time traffic in wireless networks under a conflict-graph interference model and single-hop traffic. The objective is to guarantee that at least a certain fraction of packets of each link are delivered within their deadlines, which is referred to as delivery ratio. This problem has been studied before under restrictive frame-based traffic models, or greedy maximal scheduling schemes like LDF (Largest-Deficit First) that can lead to poor delivery ratio for general traffic patterns. In this paper, we pursue a different approach through randomization over the choice of maximal links that can transmit at each time. We design randomized policies in collocated networks, multi-partite networks, and general networks, that can achieve delivery ratios much higher than what is achievable by LDF. Further, our results apply to any traffic (arrival and deadline) process that evolves as an unknown positive recurrent Markov chain. Hence, this work is an improvement with respect to both efficiency and traffic assumptions compared to the past work. We further present extensive simulation results over various traffic patterns and interference graphs to illustrate the gains of our randomized policies over LDF variants.}
}


@article{DBLP:journals/ton/ChenZZGL21,
	author = {Si Chen and
                  Maolin Zhang and
                  Jia Zhao and
                  Wei Gong and
                  Jiangchuan Liu},
	title = {Reliable and Practical Bluetooth Backscatter With Commodity Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1717--1729},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3068865},
	doi = {10.1109/TNET.2021.3068865},
	timestamp = {Mon, 10 Jun 2024 20:41:09 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenZZGL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently backscatter communication with commodity radios has received significant attention since specialized hardware is no longer needed. The state-of-the-art BLE backscatter system, FreeRider, realizes ultra-low-power BLE backscatter communication entirely using commodity devices. It, however, suffers from several key reliability issues, including unreliable two-step modulation, productive-data dependency, and lack of interference countermeasures. To address these problems, we propose RBLE, a robust BLE backscatter system that works with an excitation BLE device and a single BLE receiver. First, it uses BLE signals with partial single tones as excitations, making single-bit modulation much more robust. Then it designs dynamic channel configuration that enables channel hopping to avoid interfered channels. Moreover, it presents BLE packet regeneration that uses adaptive encoding to further enhance reliability for various channel conditions. The prototype is implemented using TI BLE radios, iPhones, Android phones, and customized tags with FPGAs. Empirical results demonstrate that RBLE achieves more than 17x uplink goodput gains over FreeRider under indoor LoS, NLoS, and outdoor environments. We also show that RBLE can realize uplink ranges of up to 25 m for indoors and 56 m for outdoors.}
}


@article{DBLP:journals/ton/VardoyanHT21,
	author = {Gayane Vardoyan and
                  C. V. Hollot and
                  Don Towsley},
	title = {Towards Stability Analysis of Data Transport Mechanisms: {A} Fluid
                  Model and Its Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1730--1744},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075837},
	doi = {10.1109/TNET.2021.3075837},
	timestamp = {Wed, 01 Sep 2021 12:44:14 +0200},
	biburl = {https://dblp.org/rec/journals/ton/VardoyanHT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Transmission Control Protocol (TCP) utilizes a congestion avoidance and control mechanism as a preventive measure against congestive collapse and as an adaptive measure in the presence of changing network conditions. The set of available congestion control algorithms is diverse, and while many have been studied from empirical and simulation perspectives, there is a notable lack of analytical work for some variants. To gain more insight into the dynamics of these algorithms, we: (1) propose a general modeling scheme consisting of a set of functional differential equations of retarded type (RFDEs) and of the congestion window as a function of time; (2) apply this scheme to TCP Reno and demonstrate its equivalence to a previous, well known model for TCP Reno; (3) show applications of the new framework to the widely-deployed congestion control algorithm TCP CUBIC, for which analytical models are few and limited; as well as to H-TCP, another high-speed congestion control algorithm; and (4) validate the model using simulations. Our modeling framework yields a fluid model for window- or rate-based congestion control variants. From a theoretical analysis of this model with TCP CUBIC, we discover that CUBIC is locally uniformly asymptotically stable - a property of the algorithm previously unknown. Through further analysis, we derive a sufficient condition for H-TCP's stability, but observe via a numerical analysis and simulations that H-TCP rarely converges to an equilibrium and is usually not asymptotically stable in practical high-speed settings.}
}


@article{DBLP:journals/ton/MaHBLC21,
	author = {Qian Ma and
                  Jianwei Huang and
                  Tamer Basar and
                  Ji Liu and
                  Xudong Chen},
	title = {Reputation and Pricing Dynamics in Online Markets},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1745--1759},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3071506},
	doi = {10.1109/TNET.2021.3071506},
	timestamp = {Tue, 01 Feb 2022 08:03:41 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaHBLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the economic interactions among sellers and buyers in online markets. In such markets, buyers have limited information about the product quality, but can observe the sellers' reputations which depend on their past transaction histories and ratings from past buyers. Sellers compete in the same market through pricing, while considering the impact of their heterogeneous reputations. We consider sellers with limited as well as unlimited capacities, which correspond to different practical market scenarios. In the unlimited seller capacity scenario, buyers prefer the seller with the highest reputation-price ratio. If the gap between the highest and second highest seller reputation levels is large enough, then the highest reputation seller dominates the market as a monopoly. If sellers' reputation levels are relatively close to each other, then those sellers with relatively high reputations will survive at the equilibrium, while the remaining relatively low reputation sellers will get zero market share. In the limited seller capacity scenario, we further consider two different cases. If each seller can only serve one buyer, then it is possible for sellers to set their monopoly prices at the equilibrium while all sellers gain positive market shares; if each seller can serve multiple buyers, then it is possible for sellers to set maximum prices at the equilibrium. Simulation results show that the dynamics of reputations and prices in the longer-term interactions will converge to stable states, and the initial buyer ratings of the sellers play the critical role in determining sellers' reputations and prices at the stable state.}
}


@article{DBLP:journals/ton/ZhangSLTM21,
	author = {Jianan Zhang and
                  Abhishek Sinha and
                  Jaime Llorca and
                  Antonia M. Tulino and
                  Eytan H. Modiano},
	title = {Optimal Control of Distributed Computing Networks With Mixed-Cast
                  Traffic Flows},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1760--1773},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3070699},
	doi = {10.1109/TNET.2021.3070699},
	timestamp = {Fri, 19 Apr 2024 13:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangSLTM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed computing networks, tasked with both packet transmission and processing, require the joint optimization of communication and computation resources. We develop a dynamic control policy that determines both routes and processing locations for packets upon their arrival at a distributed computing network. The proposed policy, referred to as Universal Computing Network Control (UCNC), guarantees that packets i) are processed by a specified chain of service functions, ii) follow cycle-free routes between consecutive functions, and iii) are delivered to their corresponding set of destinations via proper packet duplications. UCNC is shown to be throughput-optimal for any mix of unicast and multicast traffic, and is the first throughput-optimal policy for non-unicast traffic in distributed computing networks with both communication and computation constraints. Moreover, simulation results suggest that UCNC yields substantially lower average packet delay compared with existing control policies for unicast traffic.}
}


@article{DBLP:journals/ton/Kesavareddigari21,
	author = {Himaja Kesavareddigari and
                  Atilla Eryilmaz},
	title = {Counter-Intuitive Characteristics of Rational Decision-Making Using
                  Biased Inputs in Information Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1774--1785},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075430},
	doi = {10.1109/TNET.2021.3075430},
	timestamp = {Wed, 01 Sep 2021 12:44:14 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Kesavareddigari21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider an information network comprised of nodes that are: rational-information-consumers (RICs) and/or biased-information-providers (BIPs). Making the reasonable abstraction that any external event is reported as an answer to a logical statement, we model each node's information-sharing behavior as a binary channel. For various reasons, malicious or otherwise, BIPs might share incorrect reports of the event regardless of their private beliefs. In doing so, a BIP might favor one of the two outcomes, exhibiting intentional or unintentional bias (e.g. human cognitive biases). Inspired by the limitations of humans and low-memory devices in information networks, we previously investigated a graph-blind rational-information-consumer interested in identifying the ground truth. We concluded that to minimize its error probability, graph-blind RIC follows a counter-intuitive but tractable rule. In this work, we build on this foundational knowledge: “graph-blind RICs prefer the combination of information-providers that are all fully-biased against the a-priori likely input, over all other combinations.” Upon studying RICs with partial knowledge of the network graph, we find that they act similar to graph-blind RICs when their BIPs “listen to” sufficiently many information-providers of their own. Furthermore, if a common node is informing/influencing all n BIPs of a partially-aware RIC, that RIC anticipates its discovery of the “influential node” to diminish the average error probability by a factor that increases exponentially with n. However, from the partially-aware RIC's perspective, choosing n fully-, similarly-biased BIPs outweighs the discovery of influential nodes among its BIPs' sources. These insights might inform the design of consumer-centric information networks.}
}


@article{DBLP:journals/ton/KirnerP21,
	author = {Raimund Kirner and
                  Peter P. Puschner},
	title = {A Quantitative Analysis of Interfaces to Time-Triggered Communication
                  Buses},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1786--1797},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3073460},
	doi = {10.1109/TNET.2021.3073460},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KirnerP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nodes connected to a time-triggered (TT) network can access the network interface in two different ways, synchronously or asynchronously, which greatly impacts communication timing and message lifespans (i.e., the time from writing a message to its send buffer till the time when the message is read by the receiver). In this paper we present a clear timing model to reason about the timing variation possible with TT interfaces. This model facilitates the quantitative analysis of the message lifespans of synchronous and asynchronous TT interfaces. Further, we develop a tool to search for node and network configurations that minimise or maximise message lifespans. We show that choosing the right configuration for synchronous interface access can reduce message lifespan significantly (we observed a factor of 9 even for small scenarios). While industrial practice typically is to choose a slot allocation a priory, we show that optimising the slot allocation in coordination with task scheduling gives an extra edge in obtaining minimal message lifespans. For nodes with synchronous interface access, the tool determines the parameters needed to obtain minimal message lifespan and jitter.}
}


@article{DBLP:journals/ton/LiWTSWSL21,
	author = {Qiang Li and
                  Zhihao Wang and
                  Dawei Tan and
                  Jinke Song and
                  Haining Wang and
                  Limin Sun and
                  Jiqiang Liu},
	title = {GeoCAM: An IP-Based Geolocation Service Through Fine-Grained and Stable
                  Webcam Landmarks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1798--1812},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3073926},
	doi = {10.1109/TNET.2021.3073926},
	timestamp = {Tue, 18 Jun 2024 20:16:46 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiWTSWSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IP-based geolocation is essential for various location-aware Internet applications, such as online advertisement, content delivery, and online fraud prevention. Achieving accurate geolocation enormously relies on the number of high-quality (i.e., the fine-grained and stable over time) landmarks. However, the previous efforts of garnering landmarks have been impeded by the limited visible landmarks on the Internet and manual time cost. In this paper, we leverage the availability of numerous online webcams used to monitor physical surroundings as a rich source of promising high-quality landmarks for serving IP-based geolocation. In particular, we present a new framework called GeoCAM, which is designed to automatically generate qualified landmarks from online webcams, providing an IP-based geolocation service with high accuracy and wide coverage. GeoCAM periodically monitors websites hosting live webcams and uses the natural language processing technique to extract the IP addresses and latitude/longitude of webcams for generating landmarks at a large-scale. Given latency and topology constraints among webcam landmarks, GeoCAM uses the maximum likelihood estimation to approximately pinpoint the geolocation of a target host. We develop a prototype of GeoCAM and conduct real-world experiments for validating its efficacy. Our results show that GeoCam can detect 282,902 live webcams hosted in webpages with 94.2% precision and 90.4% recall, and then generate 16,863 stable and fine-grained landmarks, which are two orders of magnitude more than the landmarks used in prior works. To demonstrate the superiority of using large-scale webcams as landmarks, we implement four different geolocation algorithms and compare their performance between webcam landmarks and open-source landmarks. The evaluation results show that all the algorithms can significantly improve geolocation accuracy by using webcam landmarks.}
}


@article{DBLP:journals/ton/TangH21,
	author = {Ming Tang and
                  Jianwei Huang},
	title = {How Do You Earn Money on Live Streaming Platforms? - {A} Study of
                  Donation-Based Markets},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1813--1826},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3071488},
	doi = {10.1109/TNET.2021.3071488},
	timestamp = {Sat, 22 Apr 2023 17:17:09 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TangH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Donation-based markets have been implemented by many online platforms, such as live streaming platforms. In these markets, producers provide services without mandatory charges, and customers enjoy the services and voluntarily donate money to the producers. The donation is split between the producers and platform with a pre-agreed fraction. To gain insights into the market operation, we use a two-stage game to capture the sequential decision process between the platform and producers. In Stage I, the platform decides a donation-split-fraction (DSF), i.e., the fraction of donation kept by the producers. In Stage II, producers decide whether to participate in the platform and (if yes) how to choose their service attributes considering the DSF as well as the producers' and customers' preferences. We prove that the Stage II game is a potential game with a counter-intuitive equilibrium result: although a larger DSF leads to more producer participation and a better match between the producers' choices and the customers' preferences, it does not necessarily lead to more total donation. The Stage I problem, nevertheless, is challenging to solve analytically due to its non-convexity. To gain insights regarding the optimal DSF that maximizes the platform's payoff, we characterize both its upper-bound and lower-bound. We show numerically that the platform's optimal payoff always decreases with the mismatch between the producers' and customers' preferences. Finally, we conduct a case study with the dataset from Twitch and demonstrate the approach of computing the platform's optimal DSF without the producers' inherent preferences.}
}


@article{DBLP:journals/ton/XieTHL21,
	author = {Ning Xie and
                  Hai Jun Tan and
                  Lei Huang and
                  Alex X. Liu},
	title = {Physical-Layer Authentication in Wirelessly Powered Communication
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1827--1840},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3071670},
	doi = {10.1109/TNET.2021.3071670},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieTHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the problem of authenticating the transmitter device in wirelessly powered communications networks (WPCNs). We proposed a physical-layer authentication scheme for a WPCN. In comparison with upper-layer authentication schemes, the proposed scheme has low complexity, low power consumption, low overhead, and high security. For comprehensively analyzing the performance of the proposed scheme by considering the requirements of transmission delay, security, and reliability together, we put forward an analytical framework by considering the probabilities of transmission, security outage, connection outage, and joint security-connection outage. Based on the proposed analytical framework, we further introduced a new systematic metric by calculating the achievable throughput of all users with considering the performance of transmission-delay, security, and reliability together. We defined this new systematic metric as the overall transmission efficiency (OTE) of a WPCN, which can effectively quantize the average efficiency of message transmission in the WPCN with physical layer authentication. We analyzed the events represented by these probability factors over random fading channels and explicitly derive their closed-form expressions. For defending against jamming attacks, we further proposed another metric to estimate which user has the high probability of suffering from jamming attacks. The new metric represents that the adversary has the largest attacking gain with the minimum cost. We implemented our scheme and conducted extensive performance comparisons through simulations. Our experimental results show that the proposed scheme accurately detects an impersonating attack and drops its contribution to the sum of long-term throughput.}
}


@article{DBLP:journals/ton/TanJHL21,
	author = {Haisheng Tan and
                  Shaofeng H.{-}C. Jiang and
                  Zhenhua Han and
                  Mingxia Li},
	title = {Asymptotically Optimal Online Caching on Multiple Caches With Relaying
                  and Bypassing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1841--1852},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3077115},
	doi = {10.1109/TNET.2021.3077115},
	timestamp = {Thu, 16 Sep 2021 17:57:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TanJHL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by practical scenarios in areas such as Mobile Edge Computing (MEC) and Content Delivery Networks (CDNs), we study online file caching on multiple caches, where a file request might be relayed to other caches or bypassed directly to the memory when a cache miss happens. We can also choose to fetch files from the memory to caches and conduct file replacement if necessary. We take the relaying, bypassing and fetching costs altogether into consideration. We first show the inherent difficulty of the problem even when the online requests are of uniform costs. We propose an O(logK)-competitive randomized online multiple caching algorithm (named Camul) and an O(K)-competitive deterministic algorithm (named Camul-Det), where K is the total number of slots in all caches. Both of them achieve asymptotically optimal competitive ratios. Moreover, our algorithms can be implemented efficiently such that each request is processed in amortized constant time. We conduct extensive simulations on production data traces from Google and a benchmark workload from Yahoo. It shows that our algorithms dramatically outperform state-of-the-art schemes, i.e., reducing the total cost by 85% and 43% respectively compared with important baselines and their strengthened versions with request relaying. More importantly, Camul achieves such a good total cost without sacrificing other performance measures, e.g., the hit ratio, and performs consistently well on various settings of experiment parameters.}
}


@article{DBLP:journals/ton/HouSSL21,
	author = {Jing Hou and
                  Li Sun and
                  Tao Shu and
                  Husheng Li},
	title = {The Value of Traded Target Information in Security Games},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1853--1866},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3073273},
	doi = {10.1109/TNET.2021.3073273},
	timestamp = {Thu, 16 Sep 2021 17:57:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HouSSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ample evidence has confirmed the importance of information in security. While much research on security game has assumed the attackers' limited capabilities to obtain target information, few studies consider the possibility that the information can be acquired from a data broker, not to mention exploring the attackers' profit-seeking behaviors in the shrouded underground society. This paper studies the role of information in the security problem when the target information is sold by a data broker to multiple attackers. We formulate a novel multi-stage game model to characterize both the cooperative and competitive interactions of the data broker and attackers. The attackers' competition with correlated purchasing and attacking decisions is modeled as a two-stage stochastic model, and the bargaining process between the data broker and the attackers is analyzed in a Stackelberg game. The study contributes to the literature by exploring the behaviors of the attackers with labor specialization, and providing quantitative measures of information value from an economic perspective. The proposed frameworks characterize both the attackers' competitive equilibrium solutions and the data broker's pricing strategies under different market parameters. We also show how factors such as the quality of information, the heterogeneity in attackers' utilities, and their cooperative purchasing strategy would have an impact on the results.}
}


@article{DBLP:journals/ton/HuangBGWSYL21,
	author = {Xi Huang and
                  Simeng Bian and
                  Xin Gao and
                  Weijie Wu and
                  Ziyu Shao and
                  Yang Yang and
                  John C. S. Lui},
	title = {Online {VNF} Chaining and Predictive Scheduling: Optimality and Trade-Offs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1867--1880},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3072423},
	doi = {10.1109/TNET.2021.3072423},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangBGWSYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For NFV systems, the key design space includes the function chaining for network requests and the resource scheduling for servers. The problem is challenging since NFV systems usually require multiple (often conflicting) design objectives and the computational efficiency of real-time decision making with limited information. Furthermore, the benefits of predictive scheduling to NFV systems still remain unexplored. In this article, we propose POSCARS, an efficient predictive and online service chaining and resource scheduling scheme that achieves tunable trade-offs among various system metrics with stability guarantee. Through a careful choice of granularity in system modeling, we acquire a better understanding of the trade-offs in our design space. By a non-trivial transformation, we decouple the complex optimization problem into a series of online sub-problems to achieve the optimality with only limited information. By employing randomized load balancing techniques, we propose three variants of POSCARS to reduce the overheads of decision making. Theoretical analysis and simulations show that POSCARS and its variants require only mild-value of future information to achieve near-optimal system cost with an ultra-low request response time.}
}


@article{DBLP:journals/ton/DOroBRM21,
	author = {Salvatore D'Oro and
                  Leonardo Bonati and
                  Francesco Restuccia and
                  Tommaso Melodia},
	title = {Coordinated 5G Network Slicing: How Constructive Interference Can
                  Boost Network Throughput},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1881--1894},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3073272},
	doi = {10.1109/TNET.2021.3073272},
	timestamp = {Thu, 22 Feb 2024 08:14:17 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DOroBRM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio access network (RAN) slicing is a virtualization technology that partitions radio resources into multiple autonomous virtual networks. Since RAN slicing can be tailored to provide diverse performance requirements, it will be pivotal to achieve the high-throughput and low-latency communications that next-generation (5G) systems have long yearned for. To this end, effective RAN slicing algorithms must (i) partition radio resources so as to leverage coordination among multiple base stations and thus boost network throughput; and (ii) reduce interference across different slices to guarantee slice isolation and avoid performance degradation. The ultimate goal of this paper is to design RAN slicing algorithms that address the above two requirements. First, we show that the RAN slicing problem can be formulated as a 0-1 Quadratic Programming problem, and we prove its NP-hardness. Second, we propose an optimal solution for small-scale 5G network deployments, and we present three approximation algorithms to make the optimization problem tractable when the network size increases. We first analyze the performance of our algorithms through simulations, and then demonstrate their performance through experiments on a standard-compliant LTE testbed with 2 base stations and 6 smartphones. Our results show that not only do our algorithms efficiently partition RAN resources, but also improve network throughput by 27% and increase by 2× the signal-to-interference-plus-noise ratio.}
}


@article{DBLP:journals/ton/WuXJYWL21,
	author = {Di Wu and
                  He Xu and
                  Zhongkai Jiang and
                  Weiren Yu and
                  Xuetao Wei and
                  Jiwu Lu},
	title = {EdgeLSTM: Towards Deep and Sequential Edge Computing for IoT Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {4},
	pages = {1895--1908},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075468},
	doi = {10.1109/TNET.2021.3075468},
	timestamp = {Thu, 16 Sep 2021 17:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WuXJYWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The time series data generated by massive sensors in Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent. It poses great challenges (e.g. accuracy, reliability, stability) on the real-time analysis and decision making for different IoT applications. In this paper, we design, implement and evaluate EdgeLSTM, a unified data-driven system to enhance IoT computing at the network edge. The EdgeLSTM leverages the grid long short-term memory (Grid LSTM) to provide an agile solution for both deep and sequential computation, therefore can address important features such as large-scale, variety, time dependency and real time in IoT data. Our system exploits the advantages of Grid LSTM network and extends it with a multiclass support vector machine by rigorous regularization and optimization approaches, which not only has strong prediction capability of time series data, but also achieves fine-grained multiple classification through the predictive error. We deploy the EdgeLSTM into four IoT applications, including data prediction, anomaly detection, network maintenance and mobility management by extensive experiments. Our evaluation results of real-world time series data with different short-term and long-term time dependency from these typical IoT applications show that our EdgeLSTM system can guarantee robust performance in IoT computing.}
}


@article{DBLP:journals/ton/DuongJCA21,
	author = {Huy Quang Duong and
                  Brigitte Jaumard and
                  David Coudert and
                  Romualdas Armolavicius},
	title = {Efficient Make-Before-Break Layer 2 Reoptimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1910--1921},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078581},
	doi = {10.1109/TNET.2021.3078581},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DuongJCA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optical multilayer optimization periodically reorganizes layer 0-1-2 network elements to handle both existing and dynamic traffic requirements in the most efficient manner. This delays the need for adding new resources in order to cope with the evolution of the traffic, thus saving CAPEX. The focus of this paper is on Layer 2, i.e., on capacity reoptimization at the optical transport network (OTN) layer when routes (e.g., LSPs in MPLS networks) are making unnecessarily long detours to evade congestion. Reconfiguration into optimized routes can be achieved by re-defining the routes, one at a time, so that they use the vacant resources generated by the disappearance of services using part of a path that transits the congested section. To maintain the Quality of Service, it is desirable to operate under a Make-Before-Break (MBB) paradigm, with the minimum number of reroutings. The challenge is to determine the best rerouting order while minimizing the bandwidth requirement. We propose an exact and scalable optimization model for computing a minimum bandwidth rerouting scheme subject to MBB in the OTN layer of an optical network. Numerical results show that we can successfully apply it on networks with up to 30 nodes, a very significant improvement with respect to the state of the art. We also provide some reoptimization analysis in terms of the bandwidth requirement vs. the number of reroutings.}
}


@article{DBLP:journals/ton/HanLZZLLZZ21,
	author = {Dianqi Han and
                  Ang Li and
                  Lili Zhang and
                  Yan Zhang and
                  Jiawei Li and
                  Tao Li and
                  Ting Zhu and
                  Yanchao Zhang},
	title = {Deep Learning-Guided Jamming for Cross-Technology Wireless Networks:
                  Attack and Defense},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1922--1932},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3082839},
	doi = {10.1109/TNET.2021.3082839},
	timestamp = {Fri, 24 Nov 2023 12:52:08 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HanLZZLLZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless networks of different technologies may interfere with each other when they are deployed at proximity. Such cross-technology interference (CTI) has become prevalent with the surge of IoT devices. In this paper, we exploit CTI in coexisting WiFi-Zigbee networks and propose DeepJam, a new stealthy jamming strategy, to jam Zigbee traffic. DeepJam relies on deep learning techniques to capture the temporal pattern of the past wireless traffic and predict the future wireless traffic. By only jamming the victim’s transmissions that are not disrupted by CTI, DeepJam can significantly reduce the victim’s throughput with far fewer jamming signals and is thus much more stealthy than conventional jamming strategies. Detailed evaluations show that DeepJam can converge within 10 sec and achieve the jamming-efficiency gains of up to 742% and 285% over conventional random and reactive jamming strategies, respectively, in practical scenarios. We also propose a simple yet effective countermeasure against DeepJam.}
}


@article{DBLP:journals/ton/TariqSNAMVS21,
	author = {Isfar Tariq and
                  Rajat Sen and
                  Thomas David Novlan and
                  Salam Akoum and
                  Milap Majmundar and
                  Gustavo de Veciana and
                  Sanjay Shakkottai},
	title = {Auto-Tuning for Cellular Scheduling Through Bandit-Learning and Low-Dimensional
                  Clustering},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1933--1947},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3077455},
	doi = {10.1109/TNET.2021.3077455},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TariqSNAMVS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose an online algorithm for clustering channel-states and learning the associated achievable multiuser rates. Our motivation stems from the complexity of multiuser scheduling. For instance, MU-MIMO scheduling involves the selection of a user subset and associated rate selection each time-slot for varying channel states (the vector of quantized channels matrices for each of the users) — a complex integer optimization problem that is different for each channel state. Instead, our algorithm clusters the collection of channel states to a much lower dimension, and for each cluster provides achievable multiuser capacity trade-offs, which can be used for user and rate selection. Our algorithm uses a bandit approach, where it learns both the unknown partitions of the channel-state space (channel-state clustering) as well as the rate region for each cluster along a pre-specified set of directions, by observing the success/failure of the scheduling decisions (e.g. through packet loss). We propose an epoch-greedy learning algorithm that achieves a sub-linear regret, given access to a class of classifying functions over the channel-state space. We empirically validate our approach on a high-fidelity 5G New Radio (NR) wireless simulator developed within AT&T Labs. We show that our epoch-greedy bandit algorithm learns the channel-state clusters and the associated rate regions. Further, adaptive scheduling using this learned rate-region model (map from channel-state to the set of feasible rates) outperforms the corresponding hand-tuned static maps in multiple settings. Thus, we believe that auto-tuning cellular systems through learning-assisted scheduling algorithms can significantly improve performance in real deployments.}
}


@article{DBLP:journals/ton/SahaA21,
	author = {Gourav Saha and
                  Alhussein A. Abouzeid},
	title = {Optimal Spectrum Partitioning and Licensing in Tiered Access Under
                  Stochastic Market Models},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1948--1961},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3077643},
	doi = {10.1109/TNET.2021.3077643},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SahaA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of partitioning a spectrum band into\nM\nchannels of equal bandwidth, and then further assigning these\nM\nchannels into\nP\nlicensed channels and\nM−P\nunlicensed channels. Licensed channels can be accessed both for licensed and opportunistic use following a tiered structure that has a higher priority for licensed use. Unlicensed channels can be accessed only for opportunistic use. We address the following question in this paper. Given a market setup, what values of\nM\nand\nP\nmaximize the net spectrum utilization of the spectrum band? While this problem is fundamental, it is highly relevant practically, e.g., in the context of partitioning the recently proposed Citizens Broadband Radio Service band. If\nM\nis too high or too low, it may decrease spectrum utilization due to limited channel capacity or due to wastage of channel capacity, respectively. If\nP\nis too high (low), it will not incentivize the wireless operators who are primarily interested in unlicensed channels (licensed channels) to join the market. These tradeoffs are captured in our optimization problem which manifests itself as a two-stage Stackelberg game. We design an algorithm to solve the Stackelberg game and hence find the optimal\nM\nand\nP\n. The algorithm design also involves an efficient Monte Carlo integrator to evaluate the expected value of the involved random variables like spectrum utilization and operators’ revenue. We also benchmark our algorithms using numerical simulations.}
}


@article{DBLP:journals/ton/OrneeS21,
	author = {Tasmeen Zaman Ornee and
                  Yin Sun},
	title = {Sampling and Remote Estimation for the Ornstein-Uhlenbeck Process
                  Through Queues: Age of Information and Beyond},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1962--1975},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078137},
	doi = {10.1109/TNET.2021.3078137},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/OrneeS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, a connection between the age of information and remote estimation error was found in a sampling problem of Wiener processes: If the sampler has no knowledge of the signal being sampled, the optimal sampling strategy is to minimize the age of information; however, by exploiting causal knowledge of the signal values, it is possible to achieve a smaller estimation error. In this paper, we generalize the previous study by investigating a problem of sampling a stationary Gauss-Markov process named the Ornstein-Uhlenbeck (OU) process, where we aim to find useful insights for solving the problems of sampling more general signals. The optimal sampling problem is formulated as a constrained continuous-time Markov decision process (MDP) with an uncountable state space. We provide an exact solution to this MDP: The optimal sampling policy is a threshold policy on instantaneous estimation error and the threshold is found. Further, if the sampler has no knowledge of the OU process, the optimal sampling problem reduces to an MDP for minimizing a nonlinear age of information metric. The age-optimal sampling policy is a threshold policy on expected estimation error and the threshold is found. In both problems, the optimal sampling policies can be computed by low-complexity algorithms (e.g., bisection search and Newton’s method), and the curse of dimensionality is circumvented. These results hold for (i) general service time distributions of the queueing server and (ii) sampling problems both with and without a sampling rate constraint. Numerical results are provided to compare different sampling policies.}
}


@article{DBLP:journals/ton/PanLSXZLLHLZ21,
	author = {Tian Pan and
                  Xingchen Lin and
                  Enge Song and
                  Cheng Xu and
                  Jiao Zhang and
                  Hao Li and
                  Jianhui Lv and
                  Tao Huang and
                  Bin Liu and
                  Beichuan Zhang},
	title = {NB-Cache: Non-Blocking In-Network Caching for High-Performance Content
                  Routers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1976--1989},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3083599},
	doi = {10.1109/TNET.2021.3083599},
	timestamp = {Sun, 29 Jan 2023 15:40:40 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PanLSXZLLHLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information-Centric Networking (ICN) provides scalable and efficient content distribution at the Internet scale due to in-network caching and native multicast. To support these features, a content router needs high performance at its data plane, which consists of three forwarding steps: checking the Content Store (CS), then the Pending Interest Table (PIT), and finally the Forwarding Information Base (FIB). In this work, we build an analytical model of the router and identify that CS is the actual bottleneck. Then, we propose a novel mechanism called “NB-Cache” to address CS’s performance issue from a network-wide point of view. In NB-Cache, when packets arrive at a router whose CS is fully loaded, instead of being blocked and waiting for the CS, these packets are forwarded to the next-hop router, whose CS may not be fully loaded. This approach essentially utilizes Content Stores of all the routers along the forwarding path in parallel rather than checking each CS sequentially. NB-Cache follows a design pattern of on-demand load balancing and can be formulated into a non-trivial N-queue bypass model. We use the Markov chain to establish its theoretical base and find an algorithm for automated transition rate matrix generation. Experiments show significant improvement of data plane performance: 70% reduction in round-trip time (RTT) and 130% increase in throughput. NB-Cache decouples the fast packet forwarding from the slower content retrieval thus substantially reducing CS’s heavy dependency on fast but expensive memory.}
}


@article{DBLP:journals/ton/SunXEZ21,
	author = {Wei Sun and
                  Lisong Xu and
                  Sebastian G. Elbaum and
                  Di Zhao},
	title = {Model-Agnostic and Efficient Exploration of Numerical Congestion Control
                  State Space of Real-World {TCP} Implementations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {1990--2004},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078161},
	doi = {10.1109/TNET.2021.3078161},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunXEZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The significant impact of TCP congestion control on the Internet highlights the importance of testing congestion control algorithm implementations (CCAIs) in various network environments. Many CCAI testing problems can be solved by exploring the numerical state space of CCAIs, which is defined by a group of numerical (and nonnumerical) state variables of the CCAIs. However, the current practices for automated numerical state space exploration are either limited by the approximate abstract CCAI models or inefficient due to the large space of network environment parameters and the complicated relation between the CCAI states and network environment parameters. In this paper, we propose an automated numerical state space exploration method, called ACT, which leverages the model-agnostic feature of random testing and greatly improves its efficiency by guiding random testing under the feedback iteratively obtained in a test. Our experiments on five representative Linux TCP CCAIs show that ACT can more efficiently explore a large numerical state space than manual testing, undirected random testing, and symbolic execution based testing, while without requiring an abstract CCAI model. ACT detects multiple design and implementation bugs of these Linux TCP CCAIs, including some new bugs not reported before.}
}


@article{DBLP:journals/ton/ZhangZXWXW21,
	author = {Dai Zhang and
                  Yu Zhou and
                  Zhaowei Xi and
                  Yangyang Wang and
                  Mingwei Xu and
                  Jianping Wu},
	title = {HyperTester: High-Performance Network Testing Driven by Programmable
                  Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2005--2018},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3077652},
	doi = {10.1109/TNET.2021.3077652},
	timestamp = {Sat, 05 Mar 2022 16:02:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZXWXW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern network devices and systems are raising higher requirements on network testers that are regularly used to evaluate performance and assess correctness. These requirements include high scale, high accuracy, flexibility and low cost, which existing testers cannot fulfill at the same time. In this paper, we propose HyperTester, a network tester leveraging new-generation programmable switches and achieving all of the above goals simultaneously. Programmable switches are born with features like high throughput and linerate, deterministic processing pipelines and nanosecond-level hardware timestamps, the P4 programming model as well as comparable pricing with commodity servers, but they come with limited programmability and memory resources. HyperTester uses template-based packet generation to overcome the limitations of the switch ASIC in programmability and designs a stateless connection mechanism as well as counter-based state compression algorithms to overcome the memory resource constraints in the data plane. We have implemented HyperTester on Tofino, and the evaluations on the hardware testbed show that HyperTester supports high-scale packet generation (more than 1.6Tbps) and achieves highly accurate rate control and timestamping. We demonstrate that programmable switches can be potential and attractive targets for realizing network testers.}
}


@article{DBLP:journals/ton/WangRTWBJ21,
	author = {Su Wang and
                  Yichen Ruan and
                  Yuwei Tu and
                  Satyavrat Wagle and
                  Christopher G. Brinton and
                  Carlee Joe{-}Wong},
	title = {Network-Aware Optimization of Distributed Learning for Fog Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2019--2032},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075432},
	doi = {10.1109/TNET.2021.3075432},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangRTWBJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fog computing promises to enable machine learning tasks to scale to large amounts of data by distributing processing across connected devices. Two key challenges to achieving this goal are (i) heterogeneity in devices’ compute resources and (ii) topology constraints on which devices communicate with each other. We address these challenges by developing a novel network-aware distributed learning methodology where devices optimally share local data processing and send their learnt parameters to a server for periodic aggregation. Unlike traditional federated learning, our method enables devices to offload their data processing tasks to each other, with these decisions optimized to trade off costs associated with data processing, offloading, and discarding. We analytically characterize the optimal data transfer solution under different assumptions on the fog network scenario, showing for example that the value of offloading is approximately linear in the range of computing costs in the network when the cost of discarding is modeled as decreasing linearly in the amount of data processed at each node. Our experiments on real-world data traces from our testbed confirm that our algorithms improve network resource utilization substantially without sacrificing the accuracy of the learned model, for varying distributions of data across devices. We also investigate the effect of network dynamics on model learning and resource costs.}
}


@article{DBLP:journals/ton/LinYL21,
	author = {I{-}Chieh Lin and
                  Yu{-}Hsuan Yeh and
                  Kate Ching{-}Ju Lin},
	title = {Toward Optimal Partial Parallelization for Service Function Chaining},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2033--2044},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075709},
	doi = {10.1109/TNET.2021.3075709},
	timestamp = {Sun, 02 Oct 2022 15:51:56 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LinYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of Network Function Virtualization (NFV) and Service Function Chaining (SFC) together enable flexible and agile network management and traffic engineering. Due to the sequential execution nature of SFC, the latency would grow linearly with the number of functions. To resolve this issue, function parallelization has recently been proposed to enable independent functions to work simultaneously. Existing solutions, however, assume all the function instances are installed in the same physical machine and, thus, can be parallelized with only a little overhead. Nowadays, most of the networks deploy function instances in distributed servers for load balancing, parallelization across different servers would, in fact, introduce a non-negligible cost of duplicating or merging packets. Hence, in this work, we propose PPC ( Partial Parallel Chaining ), which only parallelizes functions if parallelization can indeed reduce the latency after considering function placement and the required additional parallelization cost. To this end, we design two schemes, partial parallelism enumeration and instance assignment to identify the optimal partial parallelism that minimizes the latency. Our simulation results show that PPC effectively adapts the degree of parallelism and, hence, outperforms both sequential chaining and full parallelism in any general scenario. Overall, the latency reduction can be up to 47.2% and 35.2%, respectively, as compared to sequential chaining and full parallelism.}
}


@article{DBLP:journals/ton/ShiE21,
	author = {Zai Shi and
                  Atilla Eryilmaz},
	title = {A Flexible Distributed Stochastic Optimization Framework for Concurrent
                  Tasks in Processing Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2045--2058},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078054},
	doi = {10.1109/TNET.2021.3078054},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed stochastic optimization has important applications in the practical implementation of machine learning and signal processing setup by providing means to allow interconnected network of processors to work towards the optimization of a global objective with intermittent communication. Existing works on distributed stochastic optimization predominantly assume all the processors storing related data to perform updates for the optimization task in each iteration. However, such optimization processes are typically executed at shared computing/data centers along with other concurrent tasks. Therefore, it is necessary to develop efficient optimization methods that possess the flexibility to share the computing resources with other ongoing tasks. In this work, we propose a new first-order framework that allows this flexibility through a probabilistic computing resource allocation strategy while guaranteeing the satisfactory performance of distributed stochastic optimization. Our results, both analytical and numerical, show that by controlling a flexibility parameter, our suite of algorithms (designed for various scenarios) can achieve the lower computation and communication costs of distributed stochastic optimization than their inflexible counterparts. This framework also enables the fair sharing of the common resources with other concurrent tasks being processed by the processing network.}
}


@article{DBLP:journals/ton/YangXCH21,
	author = {Xuwei Yang and
                  Hongli Xu and
                  Shigang Chen and
                  He Huang},
	title = {Indirect Multi-Mapping for Burstiness Management in Software Defined
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2059--2072},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078132},
	doi = {10.1109/TNET.2021.3078132},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangXCH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large software defined networks use a cluster of distributed controllers to process flow requests from a massive number of switches. To cope with traffic dynamics, this paper studies a new problem of how to improve the residual capacity available at the controllers to handle request bursts experienced at the switches. While the total residual capacity is a constant under a given total capacity of all controllers and a given total workload from all switches, this paper considers the residual capacity available to each individual switch, which depends on how the switches are mapped to the controllers for management. We focus on how to maximize the minimum residual capacity available to any switch . The prior work either provides poor residual capacity or incurs heavy synchronization overhead by simulation results. This paper proposes a new method called indirect multi-mapping that achieves both high residual capacity and low synchronization cost. We formally define a non-linear integer optimization problem for max-min residual capacity under indirect multi-mapping. We then approximate the problem as two sub-problems: switch-controller mapping selection and weight assignment for each switch-controller mapping. We solve these sub-problems and formally analyze their approximate factor. We implement the proposed solution on an SDN testbed for experimental studies and use simulations for large-scale investigation. Our evaluation shows that indirect multi-mapping improves the minimum residual capacity by 49.8% on average and reduces the synchronization cost by 41.9-60.3% on average when compared with the alternatives.}
}


@article{DBLP:journals/ton/HuangSMCDWX21,
	author = {He Huang and
                  Yu{-}E Sun and
                  Chaoyi Ma and
                  Shigang Chen and
                  Yang Du and
                  Haibo Wang and
                  Qingjun Xiao},
	title = {Spread Estimation With Non-Duplicate Sampling in High-Speed Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2073--2086},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078725},
	doi = {10.1109/TNET.2021.3078725},
	timestamp = {Tue, 01 Mar 2022 17:47:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangSMCDWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Per-flow spread measurement in high-speed networks has many practical applications. It is a more difficult problem than the traditional per-flow size measurement. Most prior work is based on sketches, focusing on reducing their space requirements in order to fit in on-chip cache memory. This design allows the measurement to be performed at the line rate, but it suffers from expensive computation for spread queries (unsuitable for online operations) and large errors in spread estimation for small flows. This paper complements the prior art with a new spread estimator design based on an on-chip/off-chip model. By storing traffic statistics in off-chip memory, our new design faces a key technical challenge to design an efficient online module of non-duplicate sampling that cuts down the off-chip memory access. We first propose a two-stage solution for non-duplicate sampling, which is efficient but cannot handle well a sampling probability that is either too small or too big. We then address this limitation through a three-stage solution that is more space-efficient. Our analysis shows that the proposed spread estimator is highly configurable for a variety of probabilistic performance guarantees. We implement our spread estimator in hardware using FPGA. The experiment results based on real Internet traffic traces show that our estimator produces spread estimation with much better accuracy than the prior art, reducing the mean relative (absolute) error by about one order of magnitude. Moreover, it increases the query throughput by around three orders of magnitude, making it suitable for supporting online queries in real time.}
}


@article{DBLP:journals/ton/ThimmarajuSS21,
	author = {Kashyap Thimmaraju and
                  Liron Schiff and
                  Stefan Schmid},
	title = {Preacher: Network Policy Checker for Adversarial Environments},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2087--2100},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3078143},
	doi = {10.1109/TNET.2021.3078143},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ThimmarajuSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Private networks are typically assumed to be trusted as security mechanisms are usually deployed on hosts and the data plane is managed in-house. The increasing number of attacks on network devices, and recent reports on backdoors, forces us to revisit existing security assumptions and demands new approaches to detect malicious activity. This paper presents Preacher, a runtime network policy checker, which leverages a secure, redundant and adaptive sample distribution scheme that allows us to provably detect and localize adversarial switches or routers trying to reroute, mirror, drop, inject, or modify packets (i.e., header and/or payload) even under collusion. The analysis performed by Preacher is highly parallelizable. We show that emerging programmable networks provide an ideal vehicle to detect suspicious network activity. Furthermore, we analytically and empirically evaluate the effectiveness of our approach in different adversarial settings, report on a proof-of-concept implementation using ONOS, and provide insights into the resource and performance overheads of Preacher.}
}


@article{DBLP:journals/ton/Barrachina-Munoz21,
	author = {Sergio Barrachina{-}Mu{\~{n}}oz and
                  Boris Bellalta and
                  Edward W. Knightly},
	title = {Wi-Fi Channel Bonding: An All-Channel System and Experimental Study
                  From Urban Hotspots to a Sold-Out Stadium},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2101--2114},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3077770},
	doi = {10.1109/TNET.2021.3077770},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Barrachina-Munoz21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present WACA, the first system to simultaneously measure all 24 Wi-Fi channels that allow channel bonding at 5 GHz with microsecond scale granularity. With WACA, we perform a first-of-its-kind measurement study in areas including urban hotspots, residential neighborhoods, universities, and even a game in Futbol Club Barcelona’s Camp Nou, a sold-out stadium with 98,000 fans and 12,000 simultaneous Wi-Fi connections. We study channel bonding in this environment, and our experimental findings reveal the underpinning factors controlling throughput gain, including channel bonding policy and spectrum occupancy statistics. We then show the significance of the gathered dataset for finding insights, which would not be possible otherwise, given that simple channel occupancy models severely underestimate the available gains. Likewise, we characterize the risks of channel bonding due to other BSS’s, including their missed transmission opportunities and potential collisions due to imperfect sensing of bonded transmissions. We explore 802.11ax which imposes constraints on bonded channels to avoid fragmentation and defines different modes that can trade implementation complexity for throughput. Lastly, we show that the stadium, while seemingly too highly occupied for channel bonding gains, has transient gaps yielding impressive gains.}
}


@article{DBLP:journals/ton/ZhangLZ21,
	author = {Qixia Zhang and
                  Fangming Liu and
                  Chaobing Zeng},
	title = {Online Adaptive Interference-Aware {VNF} Deployment and Migration
                  for 5G Network Slice},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2115--2128},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3080197},
	doi = {10.1109/TNET.2021.3080197},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Based on network function virtualization (NFV) and software defined network (SDN), network slicing is proposed as a new paradigm for building service-customized 5G network. In each network slice, service-required virtual network functions (VNFs) can be flexibly deployed in an on-demand manner, which will support a variety of 5G use cases. However, due to the real-time network variations and diverse performance requirements among different 5G scenarios, online adaptive VNF deployment and migration are needed to dynamically accommodate to service-specific requirements. In this paper, we first propose a time-slot based 5G network slice model, which jointly includes both edge cloud servers and core cloud servers. Since VNF consolidation may cause severe performance degradation, we adopt a demand-supply model to quantify the VNF interference. To achieve our objective—maximizing the total reward of accepted requests (i.e., the total throughput minus the weighted total VNF migration cost), we propose an Online Lazy-migration Adaptive Interference-aware Algorithm (OLAIA) for real-time VNF deployment and cost-efficient VNF migration in a 5G network slice, where an Adaptive Interference-aware Algorithm (AIA) is proposed as OLAIA’s core function for placing a given set of requests’ VNFs with maximized total throughput. Through trace-driven evaluations on two typical 5G network slices, we demonstrate that OLAIA can efficiently handle the real-time network variations and the VNF interference when deploying VNFs for real-time arriving requests. In particular, OLAIA improves the total reward by 22.18% in the autonomous driving scenario and by 51.10% in the 4K/8K HD video scenario, as compared with other state-of-the-art solutions.}
}


@article{DBLP:journals/ton/BedewySSS21,
	author = {Ahmed M. Bedewy and
                  Yin Sun and
                  Rahul Singh and
                  Ness B. Shroff},
	title = {Low-Power Status Updates via Sleep-Wake Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2129--2141},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3081102},
	doi = {10.1109/TNET.2021.3081102},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BedewySSS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of optimizing the freshness of status updates that are sent from a large number of low-power sources to a common access point. The source nodes utilize carrier sensing to reduce collisions and adopt an asynchronized sleep-wake scheduling strategy to achieve a target network lifetime (e.g., 10 years). We use age of information (AoI) to measure the freshness of status updates, and design sleep-wake parameters for minimizing the weighted-sum peak AoI of the sources, subject to per-source battery lifetime constraints. When the sensing time (i.e., the time duration of carrier sensing) is zero, this sleep-wake design problem can be solved by resorting to a two-layer nested convex optimization procedure; however, for positive sensing times, the problem is non-convex. We devise a low-complexity solution to solve this problem and prove that, for practical sensing times that are short, the solution is within a small gap from the optimum AoI performance. When the mean transmission time of status-update packets is unknown, we devise a reinforcement learning algorithm that adaptively performs the following two tasks in an “efficient way”: a) it learns the unknown parameter, b) it also generates efficient controls that make channel access decisions. We analyze its performance by quantifying its “regret”, i.e., the sub-optimality gap between its average performance and the average performance of a controller that knows the mean transmission time. Our numerical and NS-3 simulation results show that our solution can indeed elongate the batteries lifetime of information sources, while providing a competitive AoI performance.}
}


@article{DBLP:journals/ton/YuZYWLCDL21,
	author = {Dongxiao Yu and
                  Yifei Zou and
                  Jiguo Yu and
                  Yu Wu and
                  Weifeng Lv and
                  Xiuzhen Cheng and
                  Falko Dressler and
                  Francis C. M. Lau},
	title = {Distributed Broadcasting in Dynamic Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2142--2155},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3087818},
	doi = {10.1109/TNET.2021.3087818},
	timestamp = {Thu, 10 Mar 2022 09:31:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuZYWLCDL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate distributed broadcasting in dynamic networks, where the topology changes continually over time. We propose a network model that captures the dynamicity caused by both churn and mobility of nodes. In contrast to existing work on dynamic networks, our model defines the dynamicity in terms of localized topological changes in the vicinity of each node, rather than a global view of the whole network. Obviously, a local dynamic model suits distributed algorithms better than a global one. The proposed dynamic model uses the more realistic SINR model to depict wireless interference, instead of oversimplified graph-based models adopted in most existing work. We consider the fundamental communication primitive of global broadcast, which is to disseminate a message from a source node to the whole network. Specifically, we present a randomized distributed algorithm that can accomplish dynamic broadcasting in an asymptotically optimal running time of\nO(\nD\nT\n)\nwith a high probability guarantee, under the assumption of reasonably constant dynamicity rate , where\nD\nT\nis the dynamic diameter , a parameter proposed to depict the complexity of dynamic broadcasting. We believe our local dynamic model can greatly facilitate distributed algorithm studies in mobile and dynamic wireless networks.}
}


@article{DBLP:journals/ton/LiuYWWWW21,
	author = {Wenbin Liu and
                  Yongjian Yang and
                  En Wang and
                  Hengzhi Wang and
                  Zihe Wang and
                  Jie Wu},
	title = {Dynamic Online User Recruitment With (Non-) Submodular Utility in
                  Mobile CrowdSensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2156--2169},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3083955},
	doi = {10.1109/TNET.2021.3083955},
	timestamp = {Thu, 04 Aug 2022 11:33:22 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuYWWWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile CrowdSensing (MCS) has recently become a powerful paradigm that recruits users to cooperatively perform various tasks. In many realistic settings, users participate in real time and we have to recruit them in an online manner. The existing works usually formulate the online recruitment problem as a budgeted optimal stopping problem with submodular user utility, while we first argue that not only the budget but also the time constraints can jointly influence the recruitment performance. For example, if we have less budget but plenty of time, we should recruit users with more patience. Second, considering the user’s cooperative willingness, its contribution may be diminishing or even irregular. Hence, we also need to address not only submodular cases but also their non-submodular utility. In this paper, we study the online user recruitment problem with (non-)submodular utility under the budget and time constraints. To deal with the two constraints, we first estimate the number of users to be recruited and then recruit them in segments. Moreover, we extend the segmented strategy with a non-submodular utility, which has the submodularity ratio\nγ\nand the competitive ratio\nγ\n2\n(1−\ne\n−1\n)/7\n. Furthermore, to correct estimation errors and utilize newly obtained information, we dynamically re-adjust the segmented strategy and also prove that the dynamic strategy achieves a competitive ratio of\nγ\n2\n(1−\ne\n−1\n)(1−\ne\n−γ/2\n)/7\n. Finally, a reverse auction-based online pricing mechanism is lightly built into the proposed user recruitment strategy, which achieves truthfulness and individual rationality. Extensive experiments on three real-world data sets validate the proposed online user recruitment strategy under the (non-) submodular utility and two constraints.}
}


@article{DBLP:journals/ton/ZhuZLWCC21,
	author = {Hongzi Zhu and
                  Yuxiao Zhang and
                  Zifan Liu and
                  Xiao Wang and
                  Shan Chang and
                  Yingying Chen},
	title = {Localizing Acoustic Objects on a Single Phone},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2170--2183},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3080820},
	doi = {10.1109/TNET.2021.3080820},
	timestamp = {Wed, 03 Nov 2021 08:24:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuZLWCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding a small object (e.g., earbuds, keys or a wallet) in an indoor environment (e.g., in a house or an office) can be frustrating. In this paper, we propose an innovative system, called HyperEar , to localize such an object using only a single smartphone, based on enhanced time-difference-of-arrival (TDoA) measurements over acoustic signals issued from the object. One major challenge is the hardware limitations of a Commercial-Off-The-Shelf (COTS) phone with a short separation between the two microphones and the low sampling rate of such microphones. HyperEar enhances the accuracy of TDoA measurements by virtually increasing distances between microphones through sliding the phone in the air. HyperEar requires no communication for synchronization between the phone and the object and is a low-cost and easy-to-use system. We evaluate the performance of HyperEar via extensive experiments in various indoor conditions and the results demonstrate that, for an object of 7 m away, HyperEar can achieve a mean localization accuracy of about 15 cm when the object in normal indoor environments.}
}


@article{DBLP:journals/ton/YeLFXFQ21,
	author = {Yuhang Ye and
                  Brian Lee and
                  Ronan Flynn and
                  Jin Xu and
                  Guiming Fang and
                  Yuansong Qiao},
	title = {Delay-Based Network Utility Maximization Modelling for Congestion
                  Control in Named Data Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2184--2197},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3090174},
	doi = {10.1109/TNET.2021.3090174},
	timestamp = {Mon, 31 Jan 2022 11:54:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YeLFXFQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Content replication and name-based routing lead to a natural multi-source and multipath transmission paradigm in NDN. Due to the unique connectionless characteristic of NDN, current end-to-end multipath congestion control schemes (e.g. MPTCP) cannot be used directly on NDN. This paper proposes a Network Utility Maximization (NUM) model to formulate multi-source and multipath transmission in NDN with in-network caches. From this model, a family of receiver-driven transmission solutions can be derived, named as path-specified congestion control. The path-specified congestion control enables content consumers to separate the traffic control on each path, which consequently facilitates fair and efficient bandwidth sharing amongst all consumers. As a specific instance, a Delay-based Path-specified Congestion Control Protocol (DPCCP) is presented, which utilizes queuing delays as signals to measure and control congestion levels of different bottlenecks. In addition, a set of high-performance congestion control laws are designed to accelerate bandwidth and fairness convergence towards the optimum defined by the NUM model. Finally, DPCCP is compared with state-of-the-art solutions. The experimental evaluations show that DPCCP outperforms existing solutions in terms of bandwidth utilization, convergence time and packet loss.}
}


@article{DBLP:journals/ton/GiovanidisBMV21,
	author = {Anastasios Giovanidis and
                  Bruno Baynat and
                  Cl{\'{e}}mence Magnien and
                  Antoine Vendeville},
	title = {Ranking Online Social Users by Their Influence},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2198--2214},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3085201},
	doi = {10.1109/TNET.2021.3085201},
	timestamp = {Wed, 15 Dec 2021 10:26:06 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GiovanidisBMV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce an original mathematical model to analyze the diffusion of posts within a generic online social platform. The main novelty is that each user is not simply considered as a node on the social graph, but is further equipped with his/her own Wall and Newsfeed, and has his/her own individual self-posting and re-posting activity. As a main result using our developed model, we derive in closed form the probabilities that posts originating from a given user are found on the Wall and Newsfeed of any other. These are the solution of a linear system of equations, which can be resolved iteratively. In fact, our model is very flexible with respect to the modeling assumptions. Using the probabilities derived from the solution, we define a new measure of per-user influence over the entire network, the \\Psi\n-score, which combines the user position on the graph with user (re-)posting activity. In the homogeneous case where all users have the same activity rates, it is shown that a variant of the \\Psi\n-score is equal to PageRank. Furthermore, we compare the new model and its \\Psi\n-score against the empirical influence measured from very large data traces (Twitter, Weibo). The results illustrate that these new tools can accurately rank influencers with asymmetric (re-)posting activity for such real world applications.}
}


@article{DBLP:journals/ton/PoupkoSST21,
	author = {Ouri Poupko and
                  Gal Shahaf and
                  Ehud Shapiro and
                  Nimrod Talmon},
	title = {Building a Sybil-Resilient Digital Community Utilizing Trust-Graph
                  Connectivity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2215--2227},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3084303},
	doi = {10.1109/TNET.2021.3084303},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PoupkoSST21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Preventing fake or duplicate digital identities (aka sybils ) from joining a digital community may be crucial to its survival, especially if it utilizes a consensus protocol among its members or employs democratic governance, where sybils can undermine consensus, tilt decisions, or even take over. Here, we explore the use of a trust-graph of identities, with edges representing trust among identity owners, to allow a community to grow indefinitely without increasing its sybil penetration. Since identities are admitted to the digital community based on their trust by existing digital community members, corrupt identities, which may trust sybils, also pose a threat to the digital community. Sybils and their corrupt perpetrators are together referred to as byzantines , and the overarching aim is to limit their penetration into a digital community. We propose two alternative tools to achieve this goal. One is graph conductance, which works under the assumption that honest people are averse to corrupt ones and tend to distrust them. The second is vertex expansion, which relies on the assumption that there are not too many corrupt identities in the community. Of particular interest is keeping the fraction of byzantines below one third, as it would allow the use of Byzantine Agreement (Lamport et al. , 1982) for consensus as well as for sybil-resilient social choice (Shahaf et al. , 2019). This paper considers incrementally growing a trust graph and shows that, under its key assumptions and additional requirements, including keeping the conductance or vertex expansion of the community trust graph sufficiently high, a community may grow safely, indefinitely.}
}


@article{DBLP:journals/ton/JiangWZXD21,
	author = {Hongbo Jiang and
                  Mengyuan Wang and
                  Ping Zhao and
                  Zhu Xiao and
                  Schahram Dustdar},
	title = {A Utility-Aware General Framework With Quantifiable Privacy Preservation
                  for Destination Prediction in LBSs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2228--2241},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3084251},
	doi = {10.1109/TNET.2021.3084251},
	timestamp = {Wed, 09 Feb 2022 08:50:00 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JiangWZXD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Destination prediction plays an important role as the basis for a variety of location-based services (LBSs). However, it poses many threats to users’ location privacy. Most related work ignores privacy preservation in destination prediction. Few studies focus on specific kinds of privacy-preserving destination prediction algorithms and thus are not applicable to other prediction methods. Furthermore, the third party involved in these studies is a potential privacy threat. Additionally, another line of related work regarding LBSs neither guarantees the utility of the predicted results nor provides quantifiable privacy preservation. To this end, in this paper, we propose a general framework that can provide quantifiable privacy preservation and obtain a trade-off between the privacy and the utility of the predicted results by utilizing differential privacy and a neural network model. Specifically, it first adopts a specially designed differential privacy to construct a data-driven privacy-preserving model that formulates the relationship between injected noise and privacy preservation. Then, it combines a Recurrent Neural Network and Multi-hill Climbing to add fine-grained noise to obtain the trade-off between the privacy preservation and the utility of the predicted results. Our extensive experiments on real-world datasets validate that the proposed framework can be applied to different prediction methods, provide quantifiable location privacy preservation, and guarantee the utility of the predicted results simultaneously.}
}


@article{DBLP:journals/ton/WeiCMHR21,
	author = {Jianghong Wei and
                  Xiaofeng Chen and
                  Jianfeng Ma and
                  Xuexian Hu and
                  Kui Ren},
	title = {Communication-Efficient and Fine-Grained Forward-Secure Asynchronous
                  Messaging},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2242--2253},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3084692},
	doi = {10.1109/TNET.2021.3084692},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WeiCMHR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, motivated by the revelation of long-term and widespread surveillance of personal communications, extensive efforts have been putting into store-and-forward asynchronous messaging systems (e.g., email and SMS) for providing critical security guarantees. Of particular interest among them is forward security, which makes past messages remain secure in the event that the secret key gets exposed. Traditional forward-secure public key encryption can provide forward security for asynchronous scenarios, but it is not flexible enough for instant messaging systems. This is mainly because that, after updating his/her secret key, the user totally loses the decryption capacity of ciphertexts that have not been received. In this paper, to achieve practical forward-security of asynchronous messaging systems, we investigate the construction of a new primitive named forward-secure puncturable encryption (FSPE) that captures fine-grained forward security. Namely, the user can maintain the decryption capacity of those encrypted messages that have not been received yet. Meanwhile, even if the secret key is disclosed, those received messages can still remain secure. Specifically, we propose a communication-efficient FSPE scheme for achieving fine-grained forward-secure asynchronous messaging. Moreover, to improve the efficiency of asynchronous messaging built upon FSPE, we extend it to support outsourced decryption. We also implement the proposed scheme and evaluate a proof-of-concept of main algorithms, so as to increase confidence on its correctness and practicability.}
}


@article{DBLP:journals/ton/YuWH21,
	author = {Tianqi Yu and
                  Xianbin Wang and
                  Jianling Hu},
	title = {A Fast Hierarchical Physical Topology Update Scheme for Edge-Cloud
                  Collaborative IoT Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2254--2266},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3085031},
	doi = {10.1109/TNET.2021.3085031},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The awareness of physical network topology in a large-scale Internet of Things (IoT) system is critical to enable location-based service provisioning and performance optimization. However, due to the dynamics and complexity of IoT networks, it is usually very difficult to discover and update the physical topology of the large-scale IoT systems in real-time. Considering the stringent latency requirements in IoT systems, while the initial processing time for topology discovery can be tolerated, latency due to real-time topology update constitutes an even higher level of challenge. In this paper, a novel fast hierarchical topology update scheme is proposed for the large-scale IoT systems enabled by using the edge-cloud collaborative architecture. Specifically, an event-driven neighbor update algorithm, termed as TriggerOn, is firstly developed to update the local neighbor table of the end devices when device association or disassociation occurs. Based on the updated neighbor tables, the physical topology update of the subnet is conducted at the coordinated edge device, where a hybrid multidimensional scaling (MDS) based 3D localization algorithm is developed to locate the newly associated devices. Simulation results have indicated that as compared to the benchmark methods, the neighbor discovery latency has been reduced dramatically, and the 3D localization accuracy has been improved. Furthermore, the overall latency incurred by the proposed hierarchical physical topology update scheme is significantly lower than the distributed consensus-based update scheme, especially for the large-scale IoT subnets.}
}


@article{DBLP:journals/ton/HanTJCFZL21,
	author = {Zhenhua Han and
                  Haisheng Tan and
                  Shaofeng H.{-}C. Jiang and
                  Wanli Cao and
                  Xiaoming Fu and
                  Lan Zhang and
                  Francis C. M. Lau},
	title = {{SPIN:} {BSP} Job Scheduling With Placement-Sensitive Execution},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2267--2280},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3087221},
	doi = {10.1109/TNET.2021.3087221},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HanTJCFZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Bulk Synchronous Parallel (BSP) paradigm is gaining tremendous importance recently due to the popularity of computations as distributed machine learning and graph computation. In a typical BSP job, multiple workers concurrently conduct iterative computations, where frequent synchronization is required. Therefore, the workers should be scheduled simultaneously and their placement on different computing devices could significantly affect the performance. Simply retrofitting a traditional scheduling discipline will likely not yield the desired performance due to the unique characteristics of BSP jobs. In this work, we derive SPIN , a novel scheduling designed for BSP jobs with placement-sensitive execution to minimize the makespan of all jobs. We first prove the problem approximation hardness and then present how SPIN solves it with a rounding-based randomized approximation approach. Our analysis indicates SPIN achieves a good performance guarantee efficiently. Moreover, SPIN is robust against misestimation of job execution time by theoretically bounding its negative impact. We implement SPIN on a production-trace driven testbed with 40 GPUs. Our extensive experiments show that SPIN can reduce the job makespan and the average job completion time by up to 3\\times\nand 4.68\\times\n, respectively. SPIN also demonstrates better robustness to execution time misestimation compared with state-of-the-art heuristic baselines.}
}


@article{DBLP:journals/ton/ChiariottiZKFC21,
	author = {Federico Chiariotti and
                  Andrea Zanella and
                  Step{\'{a}}n Kucera and
                  Kariem Fahmi and
                  Holger Claussen},
	title = {The {HOP} Protocol: Reliable Latency-Bounded End-to-End Multipath
                  Communication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2281--2295},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3084450},
	doi = {10.1109/TNET.2021.3084450},
	timestamp = {Tue, 21 Mar 2023 14:59:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChiariottiZKFC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-generation wireless networks are expected to enable new applications with strict latency constraints. However, existing transport layer protocols are unable to meet the stringent Quality of Service (QoS) requirements on throughput and maximum latency: excessive queuing due to capacity-oriented congestion control inflates end-to-end latency well beyond interactivity deadlines. In this work, we propose a novel framework that evolves best-effort communications into reliability- and latency-aware communications for QoS-sensitive applications. The new protocol, named High-reliability latency-bounded Overlay Protocol (HOP), provides a novel combination of packet-level Forward Error Correction (FEC) and multipath scheduling to compensate for capacity drops and meet pre-defined QoS requirements. More specifically, the sender splits the data and the associated redundancy between the paths by using a stochastic forecast of their future capacity and decides the amount of redundancy necessary to meet the application’s requirements without clogging the connections. We compare HOP’s performance with state-of-the-art multipath protocols in ns-3 simulations using both synthetic and live network traces, and confirm that our scheme can reliably deliver high-throughput data, reducing the number of late blocks by 2 to 5 times with respect to optimized Multipath TCP (MPTCP).}
}


@article{DBLP:journals/ton/FuXQXWC21,
	author = {Luoyi Fu and
                  Jiasheng Xu and
                  Shan Qu and
                  Zhiying Xu and
                  Xinbing Wang and
                  Guihai Chen},
	title = {Seeking the Truth in a Decentralized Manner},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2296--2312},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3085000},
	doi = {10.1109/TNET.2021.3085000},
	timestamp = {Sat, 09 Apr 2022 12:21:23 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuXQXWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In networks where massive sources make observations of same entities, we intend to seek the truth – the most trustworthy value of each entity from conflicting information claimed by multiple sources. Various methods are proposed for accurately inferring both source reliability and truths, yet relying heavily on centralized settings that incur tremendous overhead to source side. In this paper, we offer a decentralized design of truth discovery task that can fit favorably to the environments with limited resources. Considering that sources forming the connected network and making individual observations, we undertake the joint maximum likelihood estimation (MLE) of truth and source reliability. Our decentralization framework simply allows each source to maintain local information exchange at a time, and computes very basic functions of data observations. To this end, we facilitate the decentralization by simplifying the MLE problem into optimizing an objective function. Upon the proof of NP-hardness, two proposed decentralized algorithms (exact and approximation) are decentralized and randomized via a combination of algorithms from their centralized counterparts that ensure performance guarantee. The derived time complexity features explicit data/network dependent terms, which leads to further acceleration in truth finding. Remarkably, in two well connected networks like random geometric and preferential attachment graphs, the accelerated approximation method enjoys logarithmic time complexity while preserving comparable accuracy to the centralized counterparts. The effectiveness of the proposed decentralizations are further empirically confirmed.}
}


@article{DBLP:journals/ton/YaoZHZ21,
	author = {Xin Yao and
                  Rui Zhang and
                  Dingquan Huang and
                  Yanchao Zhang},
	title = {Verifiable Query Processing Over Outsourced Social Graph},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2313--2326},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3085574},
	doi = {10.1109/TNET.2021.3085574},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YaoZHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social data outsourcing is an emerging paradigm for effective and efficient access to the social data. In such a system, a third-party Social Data Provider (SDP) purchases social network datasets from Online Social Network (OSN) operators and then resells them to data consumers who can be any individuals or entities desiring social data through query interfaces. The SDP cannot be fully trusted and may return forged or incomplete query results to data consumers for various reasons, e.g., in favor of the businesses willing to pay. In this paper, we initiate the study on verifiable query processing over outsourced social graph whereby a data consumer can verify both the integrity and completeness of any query result returned by an untrusted SDP. We propose three schemes for single-attribute queries and another scheme for multi-attribute queries over outsourced social data. The four schemes all require the OSN provider to generate some cryptographic auxiliary information, based on which the SDP can construct a verification object to allow the data consumer to verify the integrity and completeness of the query result. They, however, differ in how the auxiliary information is generated and how the verification object is constructed and verified. Detailed analysis and extensive experiments using a real Twitter dataset confirm the efficacy and efficiency of the proposed schemes.}
}


@article{DBLP:journals/ton/SunZWLW21,
	author = {Liyang Sun and
                  Tongyu Zong and
                  Siquan Wang and
                  Yong Liu and
                  Yao Wang},
	title = {Towards Optimal Low-Latency Live Video Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2327--2338},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3087625},
	doi = {10.1109/TNET.2021.3087625},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunZWLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-latency is a critical user Quality-of-Experience (QoE) metric for live video streaming. It poses significant challenges for streaming over the Internet. In this paper, we explore the design space of low-latency live streaming by developing dynamic models and optimal adaptation strategies to establish QoE upper bounds as a function of the allowable end-to-end latency. We further develop practical live streaming algorithms within the iterative Linear Quadratic Regulator (iLQR) based Model Predictive Control and Deep Reinforcement Learning frameworks, namely MPC-Live and DRL-Live, to maximize user live streaming QoE by adapting the video bitrate while maintaining low end-to-end video latency in dynamic network environment. Through extensive experiments driven by real network traces, we demonstrate that our live streaming algorithms can achieve close-to-optimal performance within the latency range of two to five seconds.}
}


@article{DBLP:journals/ton/ShamsiCL21,
	author = {Zain Shamsi and
                  Daren B. H. Cline and
                  Dmitri Loguinov},
	title = {Faulds: {A} Non-Parametric Iterative Classifier for Internet-Wide
                  {OS} Fingerprinting},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2339--2352},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3088333},
	doi = {10.1109/TNET.2021.3088333},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShamsiCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent work in OS fingerprinting has focused on overcoming random distortion in network and user features during Internet-scale SYN scans. These classification techniques work under an assumption that all parameters of the profiled network are known a-priori – the likelihood of packet loss, the popularity of each OS, the distribution of network delay, and the probability of user modification to each default TCP/IP header value. However, it is currently unclear how to obtain realistic versions of these parameters for the public Internet and/or customize them to a particular network being analyzed. To address this issue, we derive a non-parametric Expectation-Maximization (EM) estimator, which we call Faulds , for the unknown distributions involved in single-probe OS fingerprinting and demonstrate its significantly higher robustness to noise compared to methods in prior work. We apply Faulds to a new scan of 67M webservers and discuss its findings.}
}


@article{DBLP:journals/ton/CordeschiRT21,
	author = {Nicola Cordeschi and
                  Floriano De Rango and
                  Mauro Tropea},
	title = {Exploiting an Optimal Delay-Collision Tradeoff in CSMA-Based High-Dense
                  Wireless Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2353--2366},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3089825},
	doi = {10.1109/TNET.2021.3089825},
	timestamp = {Wed, 03 Nov 2021 08:24:59 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CordeschiRT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A novel carrier sense multiple access strategy with collision avoidance (CSMA/CA) balancing contention probability and channel access time is proposed. The approach can be applied to any context where the computational simplicity of the MAC must be preferred to the complexity of the channel access strategy. Our MAC, called Delay-Collision CSMA (DC-CSMA), is a slotted nonpersistent CSMA/CA with nonuniform contention probability distribution, designed to reduce at the same time latency of contenders and preserve a high successful access probability. An utility function aiming at equalizing the effects of these two performance metrics is introduced, and the related theoretical properties and optimal distribution are derived. DC-CSMA is insensitive to the number of contenders and very robust with respect to contention window size, packet length, and impairments such as frame synchronization errors and hidden terminals, and it does not require any adaptive tuning to optimize its performance. Current technologies such as WSN, RFID, IoT devices can benefit from such a simple access technique. The numerical evaluation has been led out considering latency, successful probability and throughput, and DC-CSMA has been compared with other classical strategies such as CSMA with uniformly distributed contention probability, CSMA/ p^{\\ast}\nand Sift distribution.}
}


@article{DBLP:journals/ton/HuHLLLJWH21,
	author = {Jinbin Hu and
                  Jiawei Huang and
                  Wenjun Lyu and
                  Weihe Li and
                  Zhaoyi Li and
                  Wenchao Jiang and
                  Jianxin Wang and
                  Tian He},
	title = {Adjusting Switching Granularity of Load Balancing for Heterogeneous
                  Datacenter Traffic},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {5},
	pages = {2367--2384},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3088276},
	doi = {10.1109/TNET.2021.3088276},
	timestamp = {Thu, 11 Aug 2022 15:50:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuHLLLJWH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The state-of-the-art datacenter load balancing designs commonly optimize bisection bandwidth with homogeneous switching granularity. Their performances surprisingly degrade under mixed traffic containing both short and long flows. Specifically, the short flows suffer from long-tailed delay, while the throughputs of long flows also degrade dramatically due to low link utilization and packet reordering. To solve these problems, we design a traffic-aware load balancing (TLB) scheme to adaptively adjust the switching granularity of long flows according to the load strength of short ones. Under the heavy load of short flows, the long flows use large switching granularity to help short ones obtain more opportunities in choosing short queues to complete quickly. On the contrary, the long flows reroute flexibly with small switching granularity to achieve high throughput. Furthermore, under extremely bursty scenario, we utilize the packet slicing scheme for long flows to release bandwidth for short ones. The experimental results of NS2 simulation and testbed implementation show that TLB significantly reduces the average flow completion time of short flows by 16%-67% over the state-of-the-art load balancers and achieves the high throughput for long flows. Moreover, for extreme bursty case, at the acceptable throughput degradation of long flows, TLB with packet slicing reduces the deadline missing ratio of bursty short flows by up to 80%.}
}


@article{DBLP:journals/ton/Ben-BasatEFMTR21,
	author = {Ran Ben{-}Basat and
                  Gil Einziger and
                  Shir Landau Feibish and
                  Jalil Moraney and
                  Bilal Tayh and
                  Danny Raz},
	title = {Routing-Oblivious Network-Wide Measurements},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2386--2398},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3061737},
	doi = {10.1109/TNET.2021.3061737},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Ben-BasatEFMTR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent introduction of SDN allows deploying new centralized network algorithms that dramatically improve network operations. In such algorithms, the centralized controller obtains a network-wide view by merging measurement data from Network Measurement Points (NMPs). A fundamental challenge is that several NMPs may count the same packet, reducing the accuracy of the measurement. Existing solutions circumvent this problem by assuming that each packet traverses a single NMP or that the routing is fixed and known. This work suggests novel algorithms for three fundamental network-wide measurement problems without making any assumptions on the topology and routing and without modifying the underlying traffic. Specifically, this work introduces two algorithms for estimating the number of (distinct) packets or byte volume in the measurement, estimating per-flow packet and byte counts, and finding the heavy hitter flows. Our work includes formal accuracy guarantees and an extensive evaluation consisting of the realistic fat-tree topology and three real network traces. Our evaluation shows that our algorithms outperform existing works and provide accurate measurements within reasonable space parameters.}
}


@article{DBLP:journals/ton/VallsIT21,
	author = {V{\'{\i}}ctor Valls and
                  George Iosifidis and
                  Leandros Tassiulas},
	title = {Birkhoff's Decomposition Revisited: Sparse Scheduling for High-Speed
                  Circuit Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2399--2412},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3088327},
	doi = {10.1109/TNET.2021.3088327},
	timestamp = {Fri, 21 Jan 2022 21:58:47 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VallsIT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data centers are increasingly using high-speed circuit switches to cope with the growing demand and reduce operational costs. One of the fundamental tasks of circuit switches is to compute a sparse collection of switching configurations to support a traffic demand matrix. Such a problem has been addressed in the literature with variations of the approach proposed by Birkhoff in 1946 to decompose a doubly stochastic matrix exactly. However, the existing methods are heuristic and do not have theoretical guarantees on how well a collection of switching configurations (i.e., permutations) can approximate a traffic matrix (i.e., a scaled doubly stochastic matrix). In this paper, we revisit Birkhoff’s approach and make three contributions. First, we establish the first theoretical bound on the sparsity of Birkhoff’s algorithm (i.e., the number of switching configurations necessary to approximate a traffic matrix). In particular, we show that by using a subset of the admissible permutation matrices, Birkhoff’s algorithm obtains an\nϵ\n-approximate decomposition with at most\nO(log(1/ϵ))\npermutations. Second, we propose a new algorithm, Birkhoff+ , which combines the wealth of Frank-Wolfe with Birkhoff’s approach to obtain sparse decompositions in a fast manner. And third, we evaluate the performance of the proposed algorithm numerically and study how this affects the performance of a circuit switch. Our results show that Birkhoff+ is superior to previous algorithms in terms of throughput, running time, and number of switching configurations.}
}


@article{DBLP:journals/ton/BaditaPA21,
	author = {Ajay Badita and
                  Parimal Parag and
                  Vaneet Aggarwal},
	title = {Single-Forking of Coded Subtasks for Straggler Mitigation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2413--2424},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3075377},
	doi = {10.1109/TNET.2021.3075377},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BaditaPA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the unpredictable nature of the nodes in distributed computing systems, some of the tasks can be significantly delayed. Such delayed tasks are called stragglers. Straggler mitigation can be achieved by redundant computation. In maximum distance separable (MDS) redundancy method, a task is divided into\nk\nsubtasks which are encoded to\nn\ncoded subtasks, such that a task is completed if any\nk\nout of\nn\ncoded subtasks are completed. Two important metrics of interest are task completion time, and server utilization which is the aggregate completed work by all servers in this duration. We consider a proactive straggler mitigation strategy where\nn\n0\nout of\nn\ncoded subtasks are started at time 0 while the remaining\nn−\nn\n0\ncoded subtasks are launched when\nℓ\n0\n≤min{\nn\n0\n,k}\nof the initial ones finish. The coded subtasks are halted when\nk\nof them finish. For this flexible forking strategy with multiple parameters, we analyze the mean of two performance metrics when the random service completion time at each server is independent and distributed identically ( i.i.d. ) to a shifted exponential. From this study, we find a tradeoff between the metrics which provides insights into the parameter choices. Experiments on Intel DevCloud illustrate that the shifted exponential distribution adequately captures the random coded subtask completion times, and our derived insights continue to hold.}
}


@article{DBLP:journals/ton/BastopcuU21,
	author = {Melih Bastopcu and
                  Sennur Ulukus},
	title = {Age of Information for Updates With Distortion: Constant and Age-Dependent
                  Distortion Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2425--2438},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3091493},
	doi = {10.1109/TNET.2021.3091493},
	timestamp = {Mon, 26 Jun 2023 20:53:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BastopcuU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider an information update system where an information receiver requests updates from an information provider in order to minimize its age of information. The updates are generated at the information provider (transmitter) as a result of completing a set of tasks such as collecting data and performing computations on them. We refer to this as the update generation process. We model the quality of an update as an increasing function of the processing time spent while generating the update at the transmitter. In particular, we use distortion as a proxy for quality , and model distortion as a decreasing function of processing time. Processing longer at the transmitter results in a better quality (lower distortion) update, but it causes the update to age in the process. We determine the age-optimal policies for the update request times at the receiver and the update processing times at the transmitter subject to a minimum required quality (maximum allowed distortion) constraint on the updates. For the required quality constraint, we consider the cases of constant maximum allowed distortion constraints, as well as age-dependent maximum allowed distortion constraints.}
}


@article{DBLP:journals/ton/JahanianCR21,
	author = {Mohammad Jahanian and
                  Jiachen Chen and
                  K. K. Ramakrishnan},
	title = {Graph-Based Namespaces and Load Sharing for Efficient Information
                  Dissemination},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2439--2452},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3094839},
	doi = {10.1109/TNET.2021.3094839},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JahanianCR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based namespaces are being increasingly used to represent the organization of complex and ever-growing information eco-systems and individual user roles. Timely and accurate information dissemination requires an architecture with appropriate naming frameworks, adaptable to changing roles, focused on content rather than network addresses. Today’s complex information organization structures make such dissemination very challenging. To address this, we propose POISE, a name-based publish/subscribe architecture for efficient topic-based and recipient-based content dissemination. POISE proposes an information layer, improving on state-of-the-art Information-Centric Networking solutions in two major ways: 1) support for complex graph-based namespaces, and 2) automatic name-based load-splitting. POISE supports in-network graph-based naming, leveraged in a dissemination protocol which exploits information layer rendezvous points (RPs) that perform name expansions. For improved robustness and scalability, POISE supports adaptive load-sharing via multiple RPs, each managing a dynamically chosen subset of the namespace graph. Excessive workload may cause one RP to turn into a “hot spot”, impeding performance and reliability. To eliminate such traffic concentration, we propose an automated load-splitting mechanism, consisting of an enhanced, namespace graph partitioning complemented by a seamless, loss-less core migration procedure. Due to the nature of our graph partitioning and its complex objectives, off-the-shelf graph partitioning, e.g., METIS, is inadequate. We propose a hybrid, iterative bi-partitioning solution, consisting of an initial and a refinement phase. We also implemented POISE on a DPDK-based platform. Using the important application of emergency response, our experimental results show that POISE outperforms state-of-the-art solutions, demonstrating its effectiveness in timely delivery and load-sharing.}
}


@article{DBLP:journals/ton/MaiLB21,
	author = {Van Sy Mai and
                  Richard J. La and
                  Abdella Battou},
	title = {Optimal Cybersecurity Investments in Large Networks Using {SIS} Model:
                  Algorithm Design},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2453--2466},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3091856},
	doi = {10.1109/TNET.2021.3091856},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaiLB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of minimizing the (time) average security costs in large networks/systems comprising many interdependent subsystems, where the state evolution is captured by a susceptible-infected-susceptible (SIS) model. The security costs reflect security investments, economic losses and recovery costs from infections and failures following successful attacks. We show that the resulting optimization problem is nonconvex and propose a suite of algorithms – two based on convex relaxations, and the other two for finding a local minimizer, based on a reduced gradient method and sequential convex programming. Also, we provide a sufficient condition under which the convex relaxations are exact and, hence, an optimal solution of the original problem can be recovered. Numerical results are provided to validate our analytical results and to demonstrate the effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/ton/AhmadiGMY21,
	author = {Mahdieh Ahmadi and
                  Morteza Golkarifard and
                  Ali Movaghar and
                  Hamed Yousefi},
	title = {Processor Sharing Queues With Impatient Customers and State-Dependent
                  Rates},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2467--2477},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3091189},
	doi = {10.1109/TNET.2021.3091189},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AhmadiGMY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study queues with impatient customers and Processor Sharing (PS) discipline as well as other variants of PS discipline, namely, Discriminatory Processor Sharing (DPS) and Generalized Processor Sharing (GPS) disciplines, where customers have deadlines until the end of service (DES). Customers arrive according to a state-dependent Poisson process and have general impatience. Customers have exponential service times with state-dependent service rates. Analytical methods based on simple Markov chains are given for the performance analysis of such queues. The principal measures of performance are the steady-state probability of missing deadline and the steady-state probability of blocking. Similar results are obtained for related queues with Random Order Service (ROS) discipline where customers have deadlines until the beginning of service (DBS). In view of a lack of exact analytical results for First Come First Served (FCFS) queues with state-dependent rates, a highly accurate approximation method is also given for these latter queues. The efficacy and accuracy of the approach are illustrated by some numerical examples and simulation experiments.}
}


@article{DBLP:journals/ton/LinYDCWW21,
	author = {Chi Lin and
                  Ziwei Yang and
                  Haipeng Dai and
                  Liangxian Cui and
                  Lei Wang and
                  Guowei Wu},
	title = {Minimizing Charging Delay for Directional Charging},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2478--2493},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3095280},
	doi = {10.1109/TNET.2021.3095280},
	timestamp = {Fri, 24 Nov 2023 12:41:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinYDCWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a more energy-efficient WPT technology, directional WPT is applied to supply energy for wireless rechargeable sensor networks (WRSNs). Conventional methods that ignore anisotropic energy receiving property of rechargeable sensors cause a waste of energy. To address this issue, in this paper, we focus on minimizing the charging delay with a directional charging scheme. At first, we introduce linear constraints to improve an energy transfer model, which is verified to be practical by experiments. Then, we concern a Minimal chArging Delay with Single charger (S-MAD) problem to promote efficiency, followed by an Optimal Direction Charge with Single charger (S-ODC) solution. Through discretizing charging power and angle, we bound the performance gap of the solution to the optimal one with \\text{a}^{^{^{^{}}}}\\,\\,\\frac {1}{1-\\epsilon ^{2}} approximation ratio, where \\epsilon is the error threshold of discretization. After that, we extend the original S-MAD problem into the large scale WRSN with multiple chargers (i.e., M-MAD) and solve it by proposing M-ODC (i.e., Optimal Direction Charge with Multiple chargers (M-ODC)). Theoretical analyses are presented to exploit the feature of the proposed schemes. Finally, we demonstrate that our methods outperform the baseline methods by an average of 34.2% through simulations and test-bed experiments.}
}


@article{DBLP:journals/ton/YanCCVL21,
	author = {Zun Yan and
                  Peng Cheng and
                  Zhuo Chen and
                  Branka Vucetic and
                  Yonghui Li},
	title = {Two-Dimensional Task Offloading for Mobile Networks: An Imitation
                  Learning Framework},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2494--2507},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3093452},
	doi = {10.1109/TNET.2021.3093452},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YanCCVL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile computing network is envisioned as a powerful framework to support the growing computation-intensive applications in the era of the Internet of Things (IoT). In this paper, we exploit the potential of a multi-layer network via a two-dimensional (2-D) task offloading scheme, which enables horizontal cooperations among the edge nodes. To minimize the average task offloading delay for all the mobile users, we formulate a mixed non-linear programming (MINLP) by jointly optimizing the 2-D offloading decisions and communication/computational resource allocation. To address this very challenging problem, we exploit the unique algorithmic structure of the optimal branch-and-bound (B&B) algorithm, and propose a novel Gaussian process imitation learning (GPIL) method to learn how to discover the shortcut for node searching in the B&B enumeration tree and significantly accelerate the B&B algorithm. When the network key parameters change, we further propose a novel recursive GPIL (RGPIL) method to agilely adapt to the new scenario with a fast policy update, where the new posterior distribution can be recursively updated based on a few new training data. Our simulation results show that the proposed method can achieve a near optimal solution with a significantly reduced complexity (e.g., a reduction of 98.7% in the number of searched nodes for a typical case). On this basis, the advantage of 2-D offloading scheme over the conventional schemes is also verified.}
}


@article{DBLP:journals/ton/YuYCYD21,
	author = {Kan Yu and
                  Jiguo Yu and
                  Xiuzhen Cheng and
                  Dongxiao Yu and
                  Anming Dong},
	title = {Efficient Link Scheduling Solutions for the Internet of Things Under
                  Rayleigh Fading},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2508--2521},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3093306},
	doi = {10.1109/TNET.2021.3093306},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuYCYD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link scheduling is an appealing solution for ensuring the reliability and latency requirements of Internet of Things (IoT). Most existing results on the link scheduling problem were based on the graph or SINR (Signal-to-Interference-plus-Noise-Ratio) models, which ignored the impact of the random fading gain of the signals strength. In this paper, we address the link scheduling problem under the Rayleigh fading model. Both Shortest Link Scheduling (SLS) and Maximum Link Scheduling (MLS) problems are studied. In particular, we show that a set of links can be activated simultaneously under Rayleigh fading model if all link SINR constraints are satisfied. Based on the analysis of previous Link Diversity Partition (LDP) algorithm, we propose an Improved LDP (ILDP) algorithm and a centralized algorithm by localizing the global interference (denoted by CLT), building on which we design a distributed CLT algorithm (denoted by RCRDCLT) that converges to a constant approximation factor of the optimum with the time complexity of\nO(lnn)\n, where\nn\nis the number of links. Furthermore, executing repeatedly RCRDCLT can solve the SLS with an approximation factor of\nΘ(lnn)\n. Extensive simulations indicate that CLT is more effective than previous six popular link scheduling algorithms, and RCRDCLT has the lowest time complexity while only losses a constant fraction of the optimum schedule.}
}


@article{DBLP:journals/ton/LorenzoGGGF21,
	author = {Beatriz Lorenzo and
                  Francisco Javier Gonz{\'{a}}lez{-}Casta{\~{n}}o and
                  Linke Guo and
                  Felipe J. Gil{-}Casti{\~{n}}eira and
                  Yuguang Fang},
	title = {Autonomous Robustness Control for Fog Reinforcement in Dynamic Wireless
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2522--2535},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3091332},
	doi = {10.1109/TNET.2021.3091332},
	timestamp = {Fri, 21 Jan 2022 21:58:47 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LorenzoGGGF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sixth-generation (6G) of wireless communications systems will significantly rely on fog/edge network architectures for service provisioning. To realize this vision, AI-based fog/edge enabled reinforcement solutions are needed to serve highly stringent applications using dynamically varying resources. In this paper, we propose a cognitive dynamic fog/edge network where primary nodes (PNs) temporarily share their resources and act as fog nodes (FNs) for secondary nodes (SNs). Under this architecture, that unleashes multiple access opportunities, we design distributed fog probing schemes for SNs to search for available connections to access neighbouring FNs. Since the availability of these connections varies in time, we develop strategies to enhance the robustness to the uncertain availability of channels and fog nodes, and reinforce the connections with the FNs. A robustness control optimization is formulated with the aim to maximize the expected total long-term reliability of SNs’ transmissions. The problem is solved by an online robustness control (ORC) algorithm that involves online fog probing and an index-based connectivity activation policy derived from restless multi-armed bandits (RMABs) model. Simulation results show that our ORC scheme significantly improves the network robustness, the connectivity reliability and the number of completed transmissions. In addition, by activating the connections with higher indexes, the total long-term reliability optimization problem is solved with low complexity.}
}


@article{DBLP:journals/ton/NegliaLRS21,
	author = {Giovanni Neglia and
                  Emilio Leonardi and
                  Guilherme Iecker Ricardo and
                  Thrasyvoulos Spyropoulos},
	title = {A Swiss Army Knife for Online Caching in Small Cell Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2536--2547},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3100757},
	doi = {10.1109/TNET.2021.3100757},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NegliaLRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a dense cellular network, in which a limited-size cache is available at every base station (BS). Coordinating content allocation across the different caches can lead to significant performance gains, but is a difficult problem even when full information about the network and the request process is available. In this paper we present\nq\nLRU-\nΔ\n, a general-purpose online caching policy that can be tailored to optimize different performance metrics also in presence of coordinated multipoint transmission techniques. The policy requires neither direct communication among BSs, nor a priori knowledge of content popularity and, under stationary request processes, has provable performance guarantees.}
}


@article{DBLP:journals/ton/ReddyvariBPS21,
	author = {Vamseedhar R. Reddyvari and
                  Sarat Chandra Bobbili and
                  Parimal Parag and
                  Srinivas Shakkottai},
	title = {Mode-Suppression: {A} Simple, Stable and Scalable Chunk-Sharing Algorithm
                  for {P2P} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2548--2559},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3092008},
	doi = {10.1109/TNET.2021.3092008},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ReddyvariBPS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ability of a P2P network to scale its throughput up in proportion to the arrival rate of peers has recently been shown to be crucially dependent on the chunk sharing policy employed. Some policies can result in low frequencies of a particular chunk, known as the missing chunk syndrome, which can dramatically reduce throughput and lead to instability of the system. For instance, commonly used policies that nominally “boost” the sharing of infrequent chunks such as the well-known rarest-first algorithm have been shown to be unstable. We take a complementary viewpoint, and instead consider a policy that simply prevents the sharing of the most frequent chunk(s), that we call mode-suppression. We also consider a more general version that suppresses the mode only if the mode frequency is larger than the lowest frequency by a fixed threshold. We prove the stability of mode-suppression using Lyapunov techniques, and use a Kingman bound argument to show that the total download time does not increase with peer arrival rate. We then design versions of mode-suppression that sample a small number of peers at each time, and construct noisy mode estimates by aggregating these samples over time. We show numerically that mode suppression stabilizes and outperforms all other recently proposed chunk sharing algorithms, and via integration into BitTorrent implementation operating over the ns-3 that it ensures stable, low sojourn time operation in a real-world setting.}
}


@article{DBLP:journals/ton/BarbetteSM21,
	author = {Tom Barbette and
                  Cyril Soldani and
                  Laurent Mathy},
	title = {Combined Stateful Classification and Session Splicing for High-Speed
                  {NFV} Service Chaining},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2560--2573},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3099240},
	doi = {10.1109/TNET.2021.3099240},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BarbetteSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network functions such as firewalls, NAT, DPI, content-aware optimizers, and load-balancers are increasingly realized as software to reduce costs and enable outsourcing. To meet performance requirements these virtual network functions (VNFs) often bypass the kernel and use their own user-space networking stack. A naïve realization of a chain of VNFs will exchange raw packets, leading to many redundant operations, wasting resources. In this work, we design a system to execute a pipeline of VNFs. We provide the user facilities to define (i) a traffic class of interest for the VNF, (ii) a session to group the packets (such as the TCP 4-tuple), and (iii) the amount of space per session. The system synthesizes a classifier and builds an efficient flow table that when possible will automatically be partially offloaded and accelerated by the network interface. We utilize an abstract view of flows to support seamless inspection and modification of the content of any flow (such as TCP or HTTP). By applying only surgical modifications to the protocol headers, we avoid the need for a complex, hard-to-maintain user-space TCP stack and can chain multiple VNFs without re-constructing the stream multiple times , allowing up to 5x improvement over standard approaches.}
}


@article{DBLP:journals/ton/PromponasPTP21,
	author = {Panagiotis Promponas and
                  Christos Pelekis and
                  Eirini{-}Eleni Tsiropoulou and
                  Symeon Papavassiliou},
	title = {Games in Normal and Satisfaction Form for Efficient Transmission Power
                  Allocation Under Dual 5G Wireless Multiple Access Paradigm},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2574--2587},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3095351},
	doi = {10.1109/TNET.2021.3095351},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PromponasPTP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, to exploit the challenges and potential offered by the simultaneous use of non-orthogonal multiple access (NOMA) and orthogonal frequency division multiple access (OFDMA) transmission options in future 5G wireless systems, we aim at the proper modeling and transformation of the uplink power allocation problem. In particular, in this setting, each user has two degrees of freedom in the decision making process, namely its overall transmission power level, and the corresponding power investment to the OFDMA and/or NOMA based transmissions. The resulting multi-variable power allocation problem is treated and solved under three different perspectives, namely: 1) Games in Normal Form and Nash Equilibrium (NE); 2) Optimization techniques targeting system social welfare through a centralized optimal solution; and 3) Games in Satisfaction Form and Efficient Satisfaction Equilibrium (ESE). Based on these approaches, different solutions and stable operation points are identified and their properties are analyzed. An in depth evaluation and comparison of the various obtained outcomes is achieved, via modeling and simulations. The focus is placed on the impact and the interplay of the NOMA specific features, including the potential over-exploitation of the available bandwidth, the fairness in accessing it, and the interference treatment. It is also shown that, using the satisfaction form games for the users to converge to the ESE, provides an efficient and promising user-centric modeling approach to the power allocation problem, as the system adapts to the users’ application needs, while at the same time eliminates a significant amount of interference.}
}


@article{DBLP:journals/ton/RamanV21,
	author = {Ravi Kiran Raman and
                  Lav R. Varshney},
	title = {Coding for Scalable Blockchains via Dynamic Distributed Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2588--2601},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3098613},
	doi = {10.1109/TNET.2021.3098613},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RamanV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchains store transaction data in the form of a distributed ledger where each node in the network stores a current copy of the sequence of transactions as a hash chain. This requirement of storing the entire ledger incurs a high storage cost that grows undesirably large for high transaction rates and large networks. In this work we use secret key sharing, private key encryption, and distributed storage to design a coding scheme such that each node stores only a part of each transaction, thereby reducing the cold storage cost to a fraction of its original cost. In addition, the storage code ensures the security of the storage from active adversaries that may aim to corrupt prior transactions by altering copies of the ledger. We further employ a dynamic zone allocation algorithm that spreads the node allocation and data distribution across transactions. Under this coding scheme we show that we can also improve the integrity of the transaction data in the network over current schemes.}
}


@article{DBLP:journals/ton/MengWWYZ21,
	author = {Xuying Meng and
                  Yequan Wang and
                  Suhang Wang and
                  Di Yao and
                  Yujun Zhang},
	title = {Interactive Anomaly Detection in Dynamic Communication Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2602--2615},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3097137},
	doi = {10.1109/TNET.2021.3097137},
	timestamp = {Thu, 01 Jun 2023 10:09:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MengWWYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network flows are the basic components of the Internet. Considering the serious consequences of abnormal flows, it is crucial to provide timely anomaly detection in dynamic communication networks. To obtain accurate anomaly detection results in dynamic networks, supervision from experts is highly demanded. However, to obtain high-quality ground truth of abnormal flows, we suffer from two major problems: (1) limited labor resources : experts with the latest domain knowledge are much fewer than the large number of flows; and (2) dynamic environment : considering the new abnormal patterns (i.e., new attacks) and continuously changing network structures, it requires timely supervision to adaptively update the parameters. To tackle these problems, we propose HADDN, a novel bandit framework for periodic-updated anomaly detection in dynamic communication networks. We formulate the task as a bandit problem, where by interactions, supervision is offered by human experts to provide the ground truth to a fraction of flows. We construct semi-parametric expected rewards to optimize the estimation of flows’ abnormality in limited interactions. Also, we utilize feature-based clusters and structural correlations to make connections between historical flows and new flows to improve both efficiency and accuracy of abnormality estimation. What’s more, we provide two implementations for the semi-parametric expected reward of the proposed HADDN with theoretical proof. Experimental evaluations on public datasets demonstrate the substantial improvement of our proposed approaches compared to state-of-art anomaly detection methods.}
}


@article{DBLP:journals/ton/YangHHCXY21,
	author = {Deliang Yang and
                  Xuan Huang and
                  Jun Huang and
                  Xiangmao Chang and
                  Guoliang Xing and
                  Yang Yang},
	title = {A First Look at Energy Consumption of NB-IoT in the Wild: Tools and
                  Large-Scale Measurement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2616--2631},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3096656},
	doi = {10.1109/TNET.2021.3096656},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangHHCXY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have seen a widespread deployment of NB-IoT networks for massive machine-to-machine communication in the emerging 5G era. Unfortunately, the key aspects of NB-IoT networks, such as radio access performance and power consumption have not been well-understood due to lack of effective tools and closed nature of operational cellular infrastructure. In this paper, we develop NB-Scope - the first hardware NB-IoT diagnostic tool that supports fine-grained fusion of power and protocol traces. We then conduct a large-scale field measurement study consisting of 30 nodes deployed at over 1,200 locations in 4 regions during a period of three months. Our in-depth analysis of the collected 49 GB traces showed that NB-IoT nodes yield significantly imbalanced energy consumption in the wild, up to a ratio of 75:1, which may lead to short battery lifetime and frequent network partition. Such a high performance variance can be attributed to several key factors including diverse network coverage levels, long tail power profile, and excessive control message repetitions. We then explore the optimization of NB-IoT base station settings on a software-defined eNodeB testbed, and suggest several important design aspects that can be considered by future NB-IoT specifications and chipsets.}
}


@article{DBLP:journals/ton/XiaZG21,
	author = {Xianjin Xia and
                  Yuanqing Zheng and
                  Tao Gu},
	title = {LiteNap: Downclocking LoRa Reception},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2632--2645},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3096990},
	doi = {10.1109/TNET.2021.3096990},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaZG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents LiteNap which improves the energy efficiency of LoRa by enabling LoRa nodes to operate in a downclocked ‘light sleep’ mode for packet reception. A fundamental limit that prevents radio downclocking is the Nyquist sampling theorem which demands the clock-rate being at least twice the bandwidth of LoRa chirps. Our study reveals under-sampled LoRa chirps suffer frequency aliasing and cause ambiguity in symbol demodulation. LiteNap addresses the problem by leveraging an empirical observation that the hardware of LoRa radio can cause phase jitters on modulated chirps, which result in frequency leakage in the time domain. The timing information of phase jitters and frequency leakages can serve as physical fingerprints to uniquely identify modulated chirps. We propose a scheme to reliably extract the fingerprints from under-sampled chirps and resolve ambiguities in symbol demodulation. We update the reception pipeline of LoRa radio to enable reliable packet detection and decoding when operating in downclocked mode. We implement LiteNap on a software defined radio platform and conduct trace-driven evaluation to validate the proposed strategies. Experiment results show that LiteNap can downclock LoRa receiver to sub-Nyquist rates for energy savings ( e.g ., 1/8 of Nyquist rate), without substantially affecting packet reception performance ( e.g ., >95% packet reception rate).}
}


@article{DBLP:journals/ton/DasPSA21,
	author = {Shrayan Das and
                  Kirtan Gopal Panda and
                  Debarati Sen and
                  Wasim Arif},
	title = {Maximizing Last-Minute Backup in Endangered Time-Varying Inter-Datacenter
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2646--2663},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3098766},
	doi = {10.1109/TNET.2021.3098766},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DasPSA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Natural disasters are time-varying in nature. They adversely affect backbone datacenter (DC) networks, thereby resulting in huge loss of data within a short span of time. Maximizing last-minute data backup in an endangered DC network hit by a progressive disaster is, therefore, of utmost importance to ensure data protection and service continuation. Thus, in this article, we propose a novel Mixed-Integer Linear Program (MILP) and a heuristic (D-RADDAR) with reduced time-complexity to address risk-aware emergency backup in endangered DC networks hit by dynamic disasters. The mathematical framework for maximizing the overall last-minute backup percentage is developed. A quantitative analysis of the risk associated with such last-minute backups is carried out. The proposed algorithms are evaluated in terms of the DC backup success percentage and compared to existing dynamic backup algorithms in the literature. Extensive simulations indicate that among all existing heuristics, the D-RADDAR achieves maximum data evacuation while maintaining minimal risk and having a polynomial time-complexity.}
}


@article{DBLP:journals/ton/FanXHY21,
	author = {Xingpeng Fan and
                  Hongli Xu and
                  He Huang and
                  Xuwei Yang},
	title = {Real-Time Update of Joint {SFC} and Routing in Software Defined Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2664--2677},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3095935},
	doi = {10.1109/TNET.2021.3095935},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FanXHY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the ever-increasing demands for high-quality network services, a software defined network (SDN) can support various virtual network functions (VNFs) using virtualization technology. Due to network dynamics, an SDN needs to be updated frequently to optimize various performance objectives, such as load balancing. Most previous solutions first determine a new network configuration ( e.g. , target VNF placement and flow routing) based on the current workload, and then update the VNF placement and routing paths of the existing flows. However, due to massive VNF’s state migration and slow update of the flow table, unacceptable update delay may occur, especially in large or frequently changed networks. In this paper, we address the real-time network update, which jointly considers the optimization of the service function chain (SFC) update and the routing update. We propose the delay-satisfied NFV-enabled network update (DSNU) problem, and prove its NP-Hardness. We design an algorithm with bounded approximation factor to solve this problem. To further reduce the delay, we also design an efficient algorithm for the update scheduling. The experimental results show that our method can reduce the network update delay by about 86% compared with the previous network update methods while preserving a similar network performance, i.e. , the VNF instance load ratio increases by less than 5%.}
}


@article{DBLP:journals/ton/LiZXJXWT21,
	author = {Tong Li and
                  Kai Zheng and
                  Ke Xu and
                  Rahul Arvind Jadhav and
                  Tao Xiong and
                  Keith Winstein and
                  Kun Tan},
	title = {Revisiting Acknowledgment Mechanism for Transport Control: Modeling,
                  Analysis, and Implementation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2678--2692},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3101011},
	doi = {10.1109/TNET.2021.3101011},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZXJXWT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The shared nature of the wireless medium induces contention between data transport and backward signaling, such as acknowledgment. The current way of TCP acknowledgment induces control overhead which is counter-productive for TCP performance especially in wireless local area network (WLAN) scenarios. In this paper, we present a new acknowledgment called TACK (“Tame ACK”), as well as its TCP implementation TCP-TACK. TACK seeks to minimize ACK frequency, which is exactly what is required by transport. TCP-TACK works on top of commodity WLAN, delivering high wireless transport goodput with minimal control overhead in the form of ACKs, without any hardware modification. Evaluation results show that TCP-TACK achieves significant advantages over legacy TCP in WLAN scenarios due to less contention between data packets and ACKs. Specifically, TCP-TACK reduces over 90% of ACKs and also obtains an improvement of up to 28% on goodput. A TACK-based protocol is a good replacement of the legacy TCP to compensate for scenarios where the acknowledgment overhead is non-negligible.}
}


@article{DBLP:journals/ton/WanSXWPZWL21,
	author = {Ying Wan and
                  Haoyu Song and
                  Yang Xu and
                  Yilun Wang and
                  Tian Pan and
                  Chuwen Zhang and
                  Yi Wang and
                  Bin Liu},
	title = {T-Cache: Efficient Policy-Based Forwarding Using Small {TCAM}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2693--2708},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3098320},
	doi = {10.1109/TNET.2021.3098320},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WanSXWPZWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ternary Content Addressable Memory (TCAM) is widely used by modern routers and switches to support policy-based forwarding due to its incomparable lookup speed and flexible matching patterns. However, the limited TCAM capacity does not scale with the ever-increasing rule table size due to the high hardware cost and high power consumption. At present, using TCAM just as a rule cache is an appealing solution, but one must resolve several tricky issues including the rule dependency and the associated TCAM updates. In this paper, we propose a new approach which can generate dependency-free rules to cache. By removing the rule dependency, the complex TCAM update problem also disappears. We provide the complete T-cache system design including slow path processing and cache replacement, and implement a T-cache prototype on Barefoot Tofino switches. We conduct comprehensive software simulations and hardware experiments based on real-world and synthesized rule tables and packet traces to show that T-cache is efficient and robust for network traffic in various scenarios.}
}


@article{DBLP:journals/ton/SivaramanS21,
	author = {Vignesh Sivaraman and
                  Biplab Sikdar},
	title = {A Defense Mechanism Against Timing Attacks on User Privacy in {ICN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2709--2722},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3097536},
	doi = {10.1109/TNET.2021.3097536},
	timestamp = {Tue, 18 Oct 2022 08:35:42 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SivaramanS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While in-network caching is an essential feature of Information Centric Networks (ICN) for improved content dissemination and reducing the bandwidth consumption at the core of the network, it is prone to many privacy threats. For example, an adversary can passively breach the privacy of a consumer by simply analyzing the different retrieval times for the same content. This paper aims to address this problem of timing analysis attacks by developing privacy-enhancing caching strategies. The proposed caching strategies use two privacy metrics, namely mutual information from information theory and differential privacy, and formulates a privacy enhancing distributed optimization problem with the objective of optimizing the network cost incurred. We efficiently solve the optimization problem by considering it as a n\n-player, non-cooperative game. We show that Nash equilibrium exists for this game and compute it using an iterative best response algorithm. We compare and validate the performance of our approach on realistic network topologies by comparing it with the existing approaches in literature and the global optimal solutions.}
}


@article{DBLP:journals/ton/AlasmarPC21,
	author = {Mohammed Alasmar and
                  George Parisis and
                  Jon Crowcroft},
	title = {{SCDP:} Systematic Rateless Coding for Efficient Data Transport in
                  Data Centers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2723--2736},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3098386},
	doi = {10.1109/TNET.2021.3098386},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AlasmarPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we propose SCDP, a general-purpose data transport protocol for data centres that, in contrast to all other protocols proposed to date, supports efficient one-to-many and many-to-one communication, which is extremely common in modern data centres. SCDP does so without compromising on efficiency for short and long unicast flows. SCDP achieves this by integrating RaptorQ codes with receiver-driven data transport, packet trimming and Multi-Level Feedback Queuing (MLFQ); (1) RaptorQ codes enable efficient one-to-many and many-to-one data transport; (2) on top of RaptorQ codes, receiver-driven flow control, in combination with in-network packet trimming, enable efficient usage of network resources as well as multi-path transport and packet spraying for all transport modes. Incast and Outcast are eliminated; (3) the systematic nature of RaptorQ codes, in combination with MLFQ, enable fast, decoding-free completion of short flows. We extensively evaluate SCDP in a wide range of simulated scenarios with realistic data centre workloads. For one-to-many and many-to-one transport sessions, SCDP performs significantly better compared to NDP and PIAS. For short and long unicast flows, SCDP performs equally well or better compared to NDP and PIAS.}
}


@article{DBLP:journals/ton/TianXWXLWZC21,
	author = {Jiazheng Tian and
                  Kun Xie and
                  Xin Wang and
                  Gaogang Xie and
                  Kenli Li and
                  Jigang Wen and
                  Dafang Zhang and
                  Jiannong Cao},
	title = {Efficiently Inferring Top-k Largest Monitoring Data Entries Based
                  on Discrete Tensor Completion},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2737--2750},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3103527},
	doi = {10.1109/TNET.2021.3103527},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TianXWXLWZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network-wide monitoring is important for many network functions. Due to the need of sampling to reduce high measurement cost, system failure, and unavoidable data transmission loss, network monitoring systems suffer from the incompleteness of network monitoring data. Different from the traditional network monitoring data estimation problem which aims to infer all missing monitoring data entries with incomplete measurement data, we study a challenging problem of inferring the top- $k$ largest monitoring data entries. The recent study shows it is promising to more accurately interpolate the missing data with a 3-D tensor compared to that based on a 2-D matrix. Taking full advantage of the multilinear structures, we apply tensor completion to first recover the missing data and then find the top- $k$ data entries. To reduce the computational overhead, we propose a novel discrete tensor completion model which uses binary codes to represent the factor matrices. Based on the model, we further propose three novel techniques to speed up the whole top- $k$ entry inference process: a discrete optimization algorithm to train the binary factor matrices, bit operations to facilitate quick missing data inference, and simplifying the finding of top- $k$ largest entries with binary code partition. In our discrete tensor completion model, only one bit is needed to represent the entry in the factor matrices instead of a real value (32 bits) needed in traditional tensor completion model, thus the storage cost is reduced significantly. To quickly infer the top- $k$ largest data entries when measurement data arrive sequentially, we also propose a sliding window based online algorithm using the discrete tensor completion model. Extensive experiments using five real data sets and one synthetic data set demonstrate that compared with the state of art tensor completion algorithms, our discrete tensor completion algorithm can achieve similar top- $k$ entry inference accuracy using significantly smaller time and storage space.}
}


@article{DBLP:journals/ton/LiI21,
	author = {Yuanyuan Li and
                  Stratis Ioannidis},
	title = {Cache Networks of Counting Queues},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2751--2764},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3102518},
	doi = {10.1109/TNET.2021.3102518},
	timestamp = {Sat, 08 Jan 2022 02:21:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiI21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a cache network in which intermediate nodes equipped with caches can serve content requests. We model this network as a universally stable queuing system, in which packets carrying identical responses are consolidated before being forwarded downstream. We refer to resulting queues as \\mathtt {M/M/1c}\nor counting queues , as consolidated packets carry a counter indicating the packet’s multiplicity. Cache networks comprising such queues are hard to analyze; we propose two approximations: one via \\mathtt {M/M/\\infty }\nqueues, and one based on \\mathtt {M/M/1c}\nqueues under the assumption of Poisson arrivals. We show that, in both cases, the problem of jointly determining (a) content placements and (b) service rates admits a poly-time, 1-1/e\napproximation algorithm. We also show that our analysis, with respect to both algorithms and associated guarantees, extends to (a) counting queues over items, rather than responses, as well as to (b) queuing at nodes and edges, as opposed to just edges. Numerical evaluations indicate that our proposed approximation algorithms yield good solutions in practice, significantly outperforming competitors.}
}


@article{DBLP:journals/ton/ZhangXWZLS21,
	author = {Xinyi Zhang and
                  Gaogang Xie and
                  Xin Wang and
                  Penghao Zhang and
                  Yanbiao Li and
                  Kav{\'{e}} Salamatian},
	title = {Fast Online Packet Classification With Convolutional Neural Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2765--2778},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3100114},
	doi = {10.1109/TNET.2021.3100114},
	timestamp = {Wed, 07 Dec 2022 23:05:02 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangXWZLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet classification is a critical component in network appliances. Software Defined Networking and cloud computing update the rulesets frequently for flexible policy configuration. Tuple Space Search (TSS), implemented in Open vSwitch (OVS), achieves fast rule updating at the sacrifice of the classification rate. In TSS, each tuple is managed by a hash table and classifying a packet needs to go through all hash tables. Merging tuples can reduce the number of hash tables, but inevitably increases the hash conflicts that may even worsen the classification performance in some cases. No existing algorithm meets the need of both fast packet classification and online rule updating. In this paper, we propose Convolutional Neural Network (CNN)-based Range Partition (CRP) to achieve fast packet classification and online update simultaneously. CRP exploits CNN-based image recognition to quickly partition tuples into range spaces upon the change of ruleset distribution, which reduces hash operations while avoiding rule overlapping caused by hashing many rules to the same location of the hash table. Experimental results demonstrate that CRP achieves\n3.2×\nclassification speed and\n4.2×\nupdate speed on average compared with state-of-the-art algorithms. We also implement CRP in OVS. The throughput of CRP-OVS is\n10×\nthat of native OVS.}
}


@article{DBLP:journals/ton/HouWLL21,
	author = {Tao Hou and
                  Tao Wang and
                  Zhuo Lu and
                  Yao Liu},
	title = {Combating Adversarial Network Topology Inference by Proactive Topology
                  Obfuscation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2779--2792},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3101692},
	doi = {10.1109/TNET.2021.3101692},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HouWLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The topology of a network is fundamental for building network infrastructure functionalities. In many scenarios, enterprise networks may have no desire to disclose their topology information. In this paper, we aim at preventing attacks that use adversarial, active end-to-end topology inference to obtain the topology information of a target network. To this end, we propose a Proactive Topology Obfuscation (ProTO) system that adopts a detect-then-obfuscate framework: (i) a lightweight probing behavior identification mechanism based on machine learning is designed to detect any probing behavior, and then (ii) a topology obfuscation design is developed to proactively delay all identified probe packets in a way such that the attacker will obtain a structurally accurate yet fake network topology based on the measurements of these delayed probe packets, therefore deceiving the attacker and decreasing its appetency for future inference. We evaluate ProTO under different evaluation scenarios. Experimental results show that ProTO is able to (i) achieve a detection rate of 99.9% with a false alarm of 3%, (ii) effectively disrupt adversarial topology inference and lead to the topology inferred by the attacker close to a fake topology, and (iii) result in an overall network delay performance degradation of 1.3% - 2.0%.}
}


@article{DBLP:journals/ton/YuXHMB21,
	author = {Mingli Yu and
                  Tian Xie and
                  Ting He and
                  Patrick D. McDaniel and
                  Quinn K. Burke},
	title = {Flow Table Security in {SDN:} Adversarial Reconnaissance and Intelligent
                  Attacks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2793--2806},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3099717},
	doi = {10.1109/TNET.2021.3099717},
	timestamp = {Wed, 19 Oct 2022 10:49:19 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuXHMB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance-driven design of SDN architectures leaves many security vulnerabilities, a notable one being the communication bottleneck between the controller and the switches. Functioning as a cache between the controller and the switches, the flow table mitigates this bottleneck by caching flow rules received from the controller at each switch, but is very limited in size due to the high cost and power consumption of the underlying storage medium. It thus presents an easy target for attacks. Observing that many existing defenses are based on simplistic attack models, we develop a model of intelligent attacks that exploit specific cache-like behaviors of the flow table to infer its internal configuration and state, and then design attack parameters accordingly. Our evaluations show that such attacks can accurately expose the internal parameters of the target flow table and cause measurable damage with the minimum effort.}
}


@article{DBLP:journals/ton/WangLYCPZ21,
	author = {Wei Wang and
                  Xin Liu and
                  Yao Yao and
                  Zicheng Chi and
                  Yan Pan and
                  Ting Zhu},
	title = {Coexistent Routing and Flooding Using WiFi Packets in Heterogeneous
                  IoT Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2807--2819},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3101949},
	doi = {10.1109/TNET.2021.3101949},
	timestamp = {Mon, 28 Nov 2022 16:50:42 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLYCPZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Routing and flooding are important functions in wireless networks. However, until now routing and flooding protocols are investigated separately within the same network (i.e., a WiFi network or a ZigBee network). Moreover, further performance improvement has been hampered by the assumption of the harmful cross technology interference. In this paper, we present coexistent routing and flooding (CRF), which leverages the unique feature of physical layer cross-technology communication technique for concurrently conducting routing within the WiFi network and flooding among ZigBee nodes using a single stream of WiFi packets. We extensively evaluate our design under different network settings and scenarios. The evaluation results show that CRF i) improves the throughput of WiFi network by 1.12 times than the state-of-the-art routing protocols; and ii) significantly reduces the flooding delay in ZigBee network (i.e., 31 times faster than the state-of-the-art flooding protocol).}
}


@article{DBLP:journals/ton/LiangSWZ21,
	author = {Teng Liang and
                  Junxiao Shi and
                  Yi Wang and
                  Beichuan Zhang},
	title = {On the Prefix Granularity Problem in {NDN} Adaptive Forwarding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2820--2833},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3103187},
	doi = {10.1109/TNET.2021.3103187},
	timestamp = {Sun, 29 Jan 2023 15:39:21 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiangSWZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One unique architectural benefit of Named Data Networking (NDN) is adaptive forwarding, i.e., the forwarding plane is able to observe past data retrieval performance and use it to adjust forwarding decisions for future Interests. To be effective, adaptive forwarding assumes that Interest Routing Locality is related to Interests’ common name prefix, meaning that Interests sharing the same prefix are likely to follow a similar forwarding path within a short period of time. Since Interests can have multiple common prefixes with different lengths, the real challenge is determining which prefix length should be used in adaptive forwarding to record path performance measurements - we refer to this as the Prefix Granularity Problem . The longer the common prefix is, the better the Interest Routing Locality, and the larger the forwarding table. Given the limited FIB size, route names are designed to be considerably shorter than Interest names. Existing adaptive forwarding designs use route names to record path performance measurements, which looses forwarding adaptability as it promises in the event of partial network failures. In this work, we propose to dynamically aggregate and de-aggregate name prefixes in the forwarding table in order to use the prefixes that are the most appropriate given current network situation. In addition, to reduce the overhead of adaptive forwarding, we propose mechanisms to minimize the use of the longest prefix matching in Data packet processing. Simulations demonstrate that the proposed techniques can result in better forwarding decisions in the event of partial network failures with significantly reduced overhead.}
}


@article{DBLP:journals/ton/FantacciPPP21,
	author = {Romano Fantacci and
                  Tommaso Pecorella and
                  Benedetta Picano and
                  Laura Pierucci},
	title = {Martingale Theory Application to the Delay Analysis of a Multi-Hop
                  Aloha {NOMA} Scheme in Edge Computing Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2834--2842},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3103424},
	doi = {10.1109/TNET.2021.3103424},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FantacciPPP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper analyzes the end-to-end delay performance in an edge-computing scenario where a set of Internet of Things devices (IoTDs) access the computation facilities of an Edge Node by means of a 5G based network. In particular, the paper deals with a two power levels slotted Aloha non-orthogonal-multiple-access (NOMA) scheme and formulates a stochastic end-to-end delay bound, in terms of complementary cumulative probability distribution, by resorting to the application of the martingale theory. In order to validate the proposed analysis, the paper proposes comparisons between the achieved analytical predictions and actual values derived by resorting to extensive computer simulations. Furthermore, the well known Boole bound has been formulated and compared with the proposed Martingale approach to highlight the better behavior of the proposed solution.}
}


@article{DBLP:journals/ton/Ma21,
	author = {Richard T. B. Ma},
	title = {Internet Transport Economics: Model and Analysis},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {29},
	number = {6},
	pages = {2843--2854},
	year = {2021},
	url = {https://doi.org/10.1109/TNET.2021.3103796},
	doi = {10.1109/TNET.2021.3103796},
	timestamp = {Sat, 08 Jan 2022 02:21:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Ma21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of video streaming and cloud services, the Internet has evolved into a content-centric service platform. Due to the best-effort service model of the Internet, the quality of service (QoS) of Internet services however cannot be guaranteed. Furthermore, characterizing QoS is challenging since it depends on the autonomous business decisions such as capacity planning, routing strategies and peering agreements of network providers. To quantify the QoS for Internet-based services, we regard the Internet infrastructure as a transport system for data packets and study the Internet ecosystem and the economics of transport services collectively provided by the autonomous network providers. In contrast to the traditional transport economics that studies the movement of people and goods over space and time, our focus in the Internet transport economics is the movement of streams of data packets that create information services. In particular, we model the supply of network capacities and demands of throughput driven by network protocols and establish a macroscopic network equilibrium under which both the end-to-end delays and drop rates of Internet routes can be derived. We show that this equilibrium solution always exists and its uniqueness can be guaranteed under various realistic scenarios. We analyze the impacts of user demands and resource capacities on the network equilibrium and provide implications of Netflix-Comcast type of peering on the QoS of users. We demonstrate that our framework can be used as a building block to understand the routing strategies under a Wardrop equilibrium and to enable further studies such as Internet peering and in-network caching.}
}
