@article{DBLP:journals/ton/KimIL24,
	author = {Junseon Kim and
                  Youngbin Im and
                  Kyunghan Lee},
	title = {Enabling Delay-Guaranteed Congestion Control With One-Bit Feedback
                  in Cellular Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {3--16},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3268721},
	doi = {10.1109/TNET.2023.3268721},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KimIL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unexpected large packet delays are often observed in cellular networks due to huge network queuing caused by excessive traffic coming into the network. To deal with the large queue problem, many congestion control algorithms try to find out how much traffic the network can accommodate, either by measuring network performance or by directly providing explicit information. However, due to the nature of the control in which queue growth should be observed or the necessity to modify the overall network architecture, existing algorithms are experiencing difficulties in keeping queues within a strict bound. In this paper, we propose a novel congestion control algorithm based on simple feedback, ECLAT which can provide bounded queuing delay using only one-bit signaling already available in traditional network architecture. To do so, a base station or a router running ECLAT 1) calculates how many packets each flow should transmit and 2) analyzes when congestion feedback needs to be forwarded to adjust the flow’s packet transmission to the desired rate. Our extensive experiments in our testbed demonstrate that ECLAT achieves strict queuing delay bounds, even in the dynamic cellular network environment.}
}


@article{DBLP:journals/ton/KhochareSSD24,
	author = {Aakash Khochare and
                  Francesco Betti Sorbelli and
                  Yogesh Simmhan and
                  Sajal K. Das},
	title = {Improved Algorithms for Co-Scheduling of Edge Analytics and Routes
                  for {UAV} Fleet Missions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {17--33},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3277810},
	doi = {10.1109/TNET.2023.3277810},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KhochareSSD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicles (UAVs) or drones are increasingly used for urban applications like traffic monitoring and construction surveys. Autonomous navigation allows drones to visit waypoints and accomplish activities as part of their mission. A common activity is to hover and observe a location using on-board cameras. Advances in Deep Neural Networks (DNNs) allow such videos to be analyzed for automated decision making. UAVs also host edge computing capability for on-board inferencing by such DNNs. To this end, for a fleet of drones, we propose a novel Mission Scheduling Problem (MSP) that co-schedules the flight routes to visit and record video at waypoints, and their subsequent on-board edge analytics. The proposed schedule maximizes the data capture and computing utilities from the activities while meeting the activity deadlines, and the energy and computing constraints. We first prove that MSP is NP-hard and then optimally solve it by formulating a mixed integer linear programming (MILP) problem. Next, we design five time-efficient heuristic algorithms that provide sub-optimal but fast solutions that are empirically competitive with the optimal solution. Evaluation of these five schedulers using real drone traces demonstrate utility–runtime trade-offs under diverse workloads.}
}


@article{DBLP:journals/ton/ShanJZJLTR24,
	author = {Danfeng Shan and
                  Linbing Jiang and
                  Peng Zhang and
                  Wanchun Jiang and
                  Hao Li and
                  Yazhe Tang and
                  Fengyuan Ren},
	title = {Enforcing Fairness in the Traffic Policer Among Heterogeneous Congestion
                  Control Algorithms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {34--49},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3276410},
	doi = {10.1109/TNET.2023.3276410},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShanJZJLTR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic policing is widely used by ISPs to limit their customers’ traffic rates. It has long been believed that a well-tuned traffic policer offers a satisfactory performance for TCP. However, we find this belief breaks with the emergence of new congestion control (CC) algorithms: flows using new CC algorithms can easily occupy the majority of bandwidth, starving traditional TCP flows. We confirm this problem with experiments and reveal its root cause as follows. Without a buffer in traffic policers, congestion only causes packet losses, while new CC algorithms are loss-resilient. When being policed, they will not reduce the sending rate until an unacceptable loss ratio for TCP is reached, resulting in low throughput for competing TCP flows. Simply adding a buffer to the traffic policer improves fairness but incurs high latency. To this end, we propose FairPolicer, which can achieve fair bandwidth allocation without sacrificing latency. FairPolicer regards a token as a basic unit of bandwidth and fairly allocates tokens to active flows in a round-robin manner. To avoid bandwidth waste when flows come and go, FairPolicer puts all available tokens in a global bucket and maintains the amount of residual bucket space rather than the number of available tokens. To scale to massive concurrent flows, FairPolicer uses a Count-Min Sketch structure to maintain per-flow data with a small memory footprint. Testbed experiments show that FairPolicer can allocate bandwidth in a max-min fair manner and achieve much lower latency than other kinds of rate limiters.}
}


@article{DBLP:journals/ton/YuanWWARZF24,
	author = {Yali Yuan and
                  Weijun Wang and
                  Yuhan Wang and
                  Sripriya Srikant Adhatarao and
                  Bangbang Ren and
                  Kai Zheng and
                  Xiaoming Fu},
	title = {Joint Optimization of QoE and Fairness for Adaptive Video Streaming
                  in Heterogeneous Mobile Environments},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {50--64},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3277729},
	doi = {10.1109/TNET.2023.3277729},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuanWWARZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of mobile video traffic and user demand poses a more stringent requirement for efficient bandwidth allocation in mobile networks where multiple users may share a bottleneck link. This provides content providers an opportunity to jointly optimize multiple users’ experiences but users often suffer short connection durations and frequent handoffs because of their high mobility. In this paper, we propose an end-to-end scheme, VSiM, for supporting mobile video streaming applications in heterogeneous wireless networks. The key idea is allocating bottleneck bandwidth among multiple users based on their mobility profiles and Quality of Experience (QoE)-related knowledge to achieve max-min QoE fairness. Besides, the QoE of buffer-sensitive clients is further improved by the novel server push strategy based on HTTP/3 protocol without affecting the existing bandwidth allocation approach or sacrificing other clients’ view quality. VSiM is lightweight and easy to deploy in the real world without touching the underlying network infrastructure. We evaluated VSiM experimentally in both simulations and a lab testbed on top of the HTTP/3 protocol. We find that the clients’ QoE fairness of VSiM achieves more than 40% improvement compared with state-of-the-art solutions, i.e., the viewing quality of clients in VSiM can be improved from 720p to 1080p in resolution. Meanwhile, VSiM provides about 20% improvement of average QoE.}
}


@article{DBLP:journals/ton/HeFFWZYY24,
	author = {Qiang He and
                  Zheng Feng and
                  Hui Fang and
                  Xing{-}Wei Wang and
                  Liang Zhao and
                  Yudong Yao and
                  Keping Yu},
	title = {A Blockchain-Based Scheme for Secure Data Offloading in Healthcare
                  With Deep Reinforcement Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {65--80},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3274631},
	doi = {10.1109/TNET.2023.3274631},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeFFWZYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread popularity of the Internet of Things and various intelligent medical devices, the amount of medical data is rising sharply, and thus medical data processing has become increasingly challenging. Mobile edge computing technology allows computing power to be allocated at the edge closer to users, which enables efficient data offloading for healthcare systems. However, existing studies on medical data offloading seldom guarantee effective data privacy and security. Moreover, the research equipping data offloading architectures with Blockchain neglect the delay and energy consumption costs incurred in using Blockchain technology for medical data offloading. Therefore, in this paper, we propose a data offloading scheme for healthcare based on Blockchain technology, which achieves optimal medical resource allocation and simultaneously minimizes the cost of offloading tasks. Specifically, we design a smart contract to ensure secure data offloading. And, we formulate the cost problem as a Markov Decision Process, solved by a policy search-based deep reinforcement learning (Asynchronous Advantage Actor-Critic) scheme, where we jointly consider offloading decisions, allocation of computing resources and radio transmission bandwidth, and Blockchain data security audits. The security of our smart-contract-based mechanism is theoretically and empirically proved, while extensive experimental results also show that our solution can obtain superior performance gains with lower cost than other baselines.}
}


@article{DBLP:journals/ton/CarvalhoFCVR24,
	author = {Fabr{\'{\i}}cio B. Carvalho and
                  Ronaldo A. Ferreira and
                  {\'{I}}talo Cunha and
                  Marcos A. M. Vieira and
                  Murali Krishna Ramanathan},
	title = {State Disaggregation for Dynamic Scaling of Network Functions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {81--95},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3282562},
	doi = {10.1109/TNET.2023.3282562},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CarvalhoFCVR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Function Virtualization promises better utilization of computational resources by dynamically scaling resources on demand. However, most network functions (NFs) are stateful and require state updates on a per-packet basis. During a scaling operation, cores need to synchronize access to a shared state to avoid race conditions and to guarantee that NFs process packets in arrival order. Unfortunately, the classic approach to control concurrent access to a shared state with locks does not scale to today’s throughput and latency requirements. Moreover, network traffic is highly skewed, leading to load imbalances in systems that use only sharding to partition the NF states. To address these challenges, we present Dyssect, a system that enables dynamic scaling of stateful NFs by disaggregating the states of network functions. By carefully coordinating actions between cores and a central controller, Dyssect migrates shards and flows between cores for load balancing or traffic prioritization without resorting to locks or reordering packets. Also, Dyssect’s state disaggregation allows the offloading of stateful network functions to programmable NICs and makes it easier for exploring hardware-software tradeoffs that better suit specific service chains and traffic loads. Our experimental evaluation shows that Dyssect reduces tail latency up to 32.04% and increases throughput up to 19.36% when compared to state-of-the-art competing solutions.}
}


@article{DBLP:journals/ton/WangZHYGZZMDC24,
	author = {Yizong Wang and
                  Dong Zhao and
                  Chenghao Huang and
                  Fuyu Yang and
                  Teng Gao and
                  Anfu Zhou and
                  Huanhuan Zhang and
                  Huadong Ma and
                  Yang Du and
                  Aiyun Chen},
	title = {TrafAda: Cost-Aware Traffic Adaptation for Maximizing Bitrates in
                  Live Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {96--109},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3285812},
	doi = {10.1109/TNET.2023.3285812},
	timestamp = {Sun, 06 Oct 2024 21:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangZHYGZZMDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The business growth of live streaming causes expensive bandwidth costs from the Content Delivery Network service. It necessitates traffic adaptation, i.e., adapting video bitrates for cost-efficient bandwidth utilization, especially under the 95\nth\npercentile pricing. However, our data-driven investigations indicate the existing methods are hard to achieve bitrate-cost balance in a long month-level billing cycle due to dynamic traffic patterns. We propose TrafAda, a learning-based cost-aware traffic adaptation method consisting of i) an ultra-long-term bandwidth demand forecasting model to learn complex bandwidth usage patterns, and ii) an imitation learning-based bitrate decision mechanism to optimize the ultra-long-term objective. We have implemented and deployed TrafAda on a large-scale live streaming system in China serving over one billion viewers from 388 cities. The results show that TrafAda improves peak-hour bitrate, quality of experience (QoE), and watching time by 34.75%, 44.56%, and 10.68%, respectively, without extra bandwidth cost, which can be converted to a considerable value for a commercial system.}
}


@article{DBLP:journals/ton/ZhaoWXZYH24,
	author = {Gongming Zhao and
                  Jingzhou Wang and
                  Hongli Xu and
                  Yangming Zhao and
                  Xuwei Yang and
                  He Huang},
	title = {Joint Request Updating and Elastic Resource Provisioning With QoS
                  Guarantee in Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {110--126},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3276881},
	doi = {10.1109/TNET.2023.3276881},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoWXZYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a commercial cloud, service providers (e.g., video streaming service provider) rent resources from cloud vendors (e.g., Google Cloud Platform) and provide services to cloud users, making a profit from the price gap. Cloud users acquire services by forwarding their requests to corresponding servers. In practice, as a common scenario, traffic dynamics will cause server overload or load-unbalancing. Existing works mainly deal with the problem by two methods: elastic resource provisioning and request updating. Elastic resource provisioning is a fast and agile solution but may cost too much since service providers need to buy extra resources from cloud vendors. Though request updating is a free solution, it will cause a significant delay, resulting in a bad users’ QoS. In this paper, we present a new scheme, called real-time request updating with elastic resource provisioning (TRUST), to help service providers pay less cost with users’ QoS guarantee in clouds. In addition, we propose an efficient algorithm for TRUST with a bounded approximation factor based on progressive-rounding. Both small-scale experiment results and large-scale simulation results show the superior performance of our proposed algorithm compared with state-of-the-art benchmarks.}
}


@article{DBLP:journals/ton/JiangLWLHSW24,
	author = {Wanchun Jiang and
                  Haoyang Li and
                  Jia Wu and
                  Zheyuan Liu and
                  Jiawei Huang and
                  Danfeng Shan and
                  Jianxin Wang},
	title = {Improvement of Copa: Behaviors and Friendliness of Delay-Based Congestion
                  Control Algorithm},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {127--142},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3278677},
	doi = {10.1109/TNET.2023.3278677},
	timestamp = {Sun, 06 Oct 2024 21:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JiangLWLHSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Delay-based congestion control has drawn a lot of attention in both academics and industry recently. Specifically, the Copa algorithm proposed in NSDI can achieve consistent high performance under various network environments and has already been deployed on Facebook. In this paper, we theoretically analyze Copa and reveal its large queuing delay and poor fairness issue under certain conditions. The root cause is that Copa fails to achieve its expected behaviors, i.e., clear the bottleneck buffer occupancy periodically. Moreover, we also reveal that the pathological competitive mode of Copa fails to guarantee friendliness. To address these issues, we propose Copa+, which enhances Copa with a parameter adaptation mechanism and an optimized competitive mode. Designed based on our theoretical analysis, Copa+ can adaptively clear the bottleneck buffer occupancy and become friendly to Cubic in the competitive mode. As a result, Copa+ inherits the advantages of Copa but achieves lower queuing delay and better fairness under different environments, as confirmed by real-world experiments and simulations. Specifically, Copa+ has the highest average throughput over different Internet links among different cloud nodes, compared to Cubic, BBR, PCC Vivace, Remy, and Indigo. Meanwhile, Copa+ has an 8.1% increase in throughput and similar low queuing delay compared to Copa. Moreover, Copa+ achieves 14.6% lower queuing delay and 2.4% higher throughput compared to Sprout over emulated cellular links.}
}


@article{DBLP:journals/ton/GuoDJX24,
	author = {Zehua Guo and
                  Songshi Dou and
                  Wenchao Jiang and
                  Yuanqing Xia},
	title = {Toward Improved Path Programmability Recovery for Software-Defined
                  WANs Under Multiple Controller Failures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {143--158},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286456},
	doi = {10.1109/TNET.2023.3286456},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoDJX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enabling path programmability is an essential feature of Software-Defined Networking (SDN). During controller failures in Software-Defined Wide Area Networks (SD-WANs), a resilient design should maintain path programmability for offline flows, which were controlled by the failed controllers. Existing solutions can only partially recover the path programmability rooted in two problems: 1) the implicit preferable recovering flows with long paths and 2) the sub-optimal remapping strategy in the coarse-grained switch level. In this paper, we propose ProgrammabilityGuardian to recover the path programmability of offline flows while maintaining low communication overhead. These goals are achieved through the fine-grained flow-level mappings enabled by existing SDN techniques. ProgrammabilityGuardian configures the flow-controller mappings to recover offline flows with a similar path programmability, maximize the total programmability of the offline flows, and minimize the total communication overhead for controlling these recovered flows. Simulation results of different controller failure scenarios under two different topologies show that ProgrammabilityGuardian recovers offline flows with a balanced path programmability, improves the total programmability of the recovered flows up to 68% and 70%, and reduces the communication overhead by 96% and 99%, compared with the baseline algorithm.}
}


@article{DBLP:journals/ton/ZhangGSCXL24,
	author = {Xiaoli Zhang and
                  Wei Geng and
                  Yiqiao Song and
                  Hongbing Cheng and
                  Ke Xu and
                  Qi Li},
	title = {Privacy-Preserving and Lightweight Verification of Deep Packet Inspection
                  in Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {159--174},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3282100},
	doi = {10.1109/TNET.2023.3282100},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangGSCXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the trend of network middleboxes as a service, enterprise customers adopt in-the-cloud deep packet inspection (DPI) services to protect networks. As network misconfigurations and hardware failures notoriously exist, recent efforts envision to ensure the execution integrity of DPI services in untrusted clouds. However, they either require enterprise customers to know proprietary DPI rulesets of cloud providers or introduce forbidden overhead in the network context. In the paper, we propose a privacy-preserving and lightweight verification scheme that efficiently checks whether in-the-cloud DPI services run correctly without leaking private DPI rulesets. Particularly, our design introduces one trusted third party to perform privacy-preserving and trustworthy ruleset evaluation and DPI execution verification. Meanwhile, it devises a novel DPI ruleset authentication method that enables tamper-proof DPI operations and facilitates fast proof generation. The proofs can be verified without requiring the verifier to always maintain all rulesets. To further reduce the verification costs while resisting cloud cheating behaviors like bias treatments of packets, it employs a commitment-based delayed sampling mechanism which requires the DPI services to first demonstrate that all packets have been processed before receiving sampling decisions. Moreover, extensive experiments are conducted based on Click modules. The results show that the proposed scheme is practical and only incurs the real-time overhead of 10–20 microseconds.}
}


@article{DBLP:journals/ton/DengD24,
	author = {Yong Deng and
                  Min Dong},
	title = {Decentralized Caching Under Nonuniform File Popularity and Size: Memory-Rate
                  Tradeoff Characterization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {175--190},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3284347},
	doi = {10.1109/TNET.2023.3284347},
	timestamp = {Sun, 19 Jan 2025 13:55:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DengD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper aims to characterize the memory-rate tradeoff for decentralized caching under nonuniform file popularity and size. We consider a recently proposed decentralized modified coded caching scheme (D-MCCS) and formulate the cache placement optimization problem to minimize the average rate for the D-MCCS. To solve this challenging non-convex optimization problem, we first propose a successive Geometric Programming (GP) approximation algorithm, which guarantees convergence to a stationary point but has high computational complexity. Next, we develop a low-complexity file-group-based approach, where we propose a popularity-first and size-aware (PF-SA) cache placement strategy to partition files into two groups, taking into account the nonuniformity in file popularity and size. Both algorithms do not require the knowledge of active users beforehand for cache placement. Numerical results show that they perform very closely to each other. We further develop a lower bound for decentralized caching under nonuniform file popularity and size as a non-convex optimization problem and solved it using a similar successive GP approximation algorithm. We show that the D-MCCS with the optimized cache placement attains this lower bound when no more than two active users request files at a time. The same is true for files with uniform size but nonuniform popularity and the optimal cache placement being symmetric among files. In these cases, the optimized D-MCCS characterizes the exact memory-rate tradeoff for decentralized caching. For general cases, our numerical results show that the average rate achieved by the optimized D-MCCS is very close to the lower bound.}
}


@article{DBLP:journals/ton/ChenLHZZWL24,
	author = {Xiang Chen and
                  Hongyan Liu and
                  Qun Huang and
                  Dong Zhang and
                  Haifeng Zhou and
                  Chunming Wu and
                  Xuan Liu},
	title = {Toward Scalable and Low-Cost Traffic Testing for Evaluating DDoS Defense
                  Solutions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {191--206},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3281449},
	doi = {10.1109/TNET.2023.3281449},
	timestamp = {Fri, 21 Feb 2025 13:04:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLHZZWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To date, security researchers evaluate their solutions of mitigating distributed denial-of-service (DDoS) attacks via kernel-based or kernel-bypassing testing tools. However, kernel-based tools exhibit poor scalability in attack traffic generation while kernel-bypassing tools incur unacceptable monetary cost. We propose Excalibur, a scalable and low-cost testing framework for evaluating DDoS defense solutions. The key idea is to leverage the emerging programmable switch to empower testing tasks with Tbps-level scalability and low cost. Specifically, Excalibur offers intent-based primitives to enable academic researchers to customize testing tasks on demand. Moreover, in view of switch resource limitations, Excalibur coordinates both a server and a programmable switch to jointly perform testing tasks. It realizes flexible attack traffic generation, which requires a large number of resources, in the server while using the switch to increase the sending rate of attack traffic to Tbps-level. We have implemented Excalibur on a 64\\times 100 Gbps Tofino switch. Our experiments on a 64\\times 100 Gbps Tofino switch show that Excalibur achieves orders-of-magnitude higher scalability and lower cost than existing tools.}
}


@article{DBLP:journals/ton/JinHLW24,
	author = {Meng Jin and
                  Yuan He and
                  Yunhao Liu and
                  Xinbing Wang},
	title = {Covert Communication With Acoustic Noise},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {207--221},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286692},
	doi = {10.1109/TNET.2023.3286692},
	timestamp = {Thu, 14 Mar 2024 16:59:09 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JinHLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Along with the proliferation of IoT devices, people have a lot of concerns on the privacy issues brought by them. Existing solutions, employing encryption or trying to hide the communication in PHY layer, often suffer from the limited capability of IoT devices. To address this issue, we propose Rustle, an acoustic communication design which builds covert connection among IoT devices using random noise. Noise signal can be easily generated and exchanged by the widely used speakers and microphones on IoT devices. Based on a fine-grained control of signal’s wave shape, Rustle generates a series of mutually uncorrelated random signals that contain “hidden patterns” to embed information. Extensive evaluations demonstrate that Rustle can achieve a lower than 1% BER while the eavesdropper’s error rate on detecting the signal is higher than 80%.}
}


@article{DBLP:journals/ton/HosseinalipourWMABLC24,
	author = {Seyyedali Hosseinalipour and
                  Su Wang and
                  Nicol{\`{o}} Michelusi and
                  Vaneet Aggarwal and
                  Christopher G. Brinton and
                  David J. Love and
                  Mung Chiang},
	title = {Parallel Successive Learning for Dynamic Distributed Model Training
                  Over Heterogeneous Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {222--237},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286987},
	doi = {10.1109/TNET.2023.3286987},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HosseinalipourWMABLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the access point. PSL considers the realistic scenario where global aggregations are conducted with idle times in-between them for resource efficiency improvements, and incorporates data dispersion and model dispersion with local model condensation into FedL. Our analysis sheds light on the notion of cold vs. warmed up models, and model inertia in distributed machine learning. We then propose network-aware dynamic model tracking to optimize the model learning vs. resource efficiency tradeoff, which we show is an NP-hard signomial programming problem. We finally solve this problem through proposing a general optimization solver. Our numerical results reveal new findings on the interdependencies between the idle times in-between the global aggregations, model/concept drift, and D2D cooperation configuration.}
}


@article{DBLP:journals/ton/YuYFC24,
	author = {Kan Yu and
                  Jiguo Yu and
                  Zhiyong Feng and
                  Honglong Chen},
	title = {A Reassessment on Applying Protocol Interference Model Under Rayleigh
                  Fading: From Perspective of Link Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {238--252},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3284433},
	doi = {10.1109/TNET.2023.3284433},
	timestamp = {Mon, 18 Nov 2024 20:29:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuYFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link scheduling plays a pivotal role in accommodating stringent reliability and latency requirements. In this paper, we focus on the availability and effectiveness of applying protocol interference model (PIM) under Rayleigh fading model to solve the problem. The motivation is that PIM caters to distributed link scheduling algorithm design, but usually lead to irrationality due to its localization behavior. While Rayleigh fading model can accurately describe the inherent characteristic of wireless signal propagation, but the features of global interference and channel fading make algorithm design more challenging. To be specific, we first remove the effect of channel fading on algorithmic design by establishing the relationship between Rayleigh fading model and non-fading model. We then propose a centralized once link elimination (OLE) algorithm by utilizing local nature of PIM, and achieve its distributed implementation based on the message delivery with time complexity of O(\\Delta _{\\max }\\ln \\Delta _{\\max }) , where \\Delta _{\\max } is the maximum number of nodes around a given node inside some range. Furthermore, based on random contention resolution, we design another distributed algorithm to schedule all the links within O(\\Delta ^{3}_{\\max }\\ln \\Delta _{\\max }) rounds. Simulations show that the PIM is of great confidence as same as Rayleigh fading model, and the proposed algorithms outperform three popular link scheduling algorithms.}
}


@article{DBLP:journals/ton/HayashiK24,
	author = {Masahito Hayashi and
                  Takeshi Koshiba},
	title = {Universal Adaptive Construction of Verifiable Secret Sharing and Its
                  Application to Verifiable Secure Distributed Data Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {253--267},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3283577},
	doi = {10.1109/TNET.2023.3283577},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HayashiK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secret sharing is a useful method for secure distributed data storage. Such a distributed data storage can avoid the information leakage under an attack to a limited number of distributed servers. While such distributed servers send their shares to an end user according to the request, there is a risk that malicious distributed servers send incorrect shares. To detect or identify such malicious servers, we need verifiable secure distributed data storage, which can be constructed from verifiable secret sharing. However, many of previous protocols for verifiable secret sharing are constructed in a specific form. This paper proposes an adaptive construction of verifiable secret sharing, which uses an existing secret sharing protocol as a subprotocol. Also, our method can be applied to any existing secret sharing protocol. This type construction realizes an economical construction.}
}


@article{DBLP:journals/ton/ZhaoWZXHQ24,
	author = {Gongming Zhao and
                  Jingzhou Wang and
                  Yangming Zhao and
                  Hongli Xu and
                  Liusheng Huang and
                  Chunming Qiao},
	title = {Segmented Entanglement Establishment With All-Optical Switching in
                  Quantum Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {268--282},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3281901},
	doi = {10.1109/TNET.2023.3281901},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoWZXHQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There are two conventional methods to establish an entanglement connection in a Quantum Data Networks (QDN). One is to create single-hop entanglement links first and then connect them with quantum swapping, and the other is forwarding one of the entangled photons from one end to the other via all-optical switching at intermediate nodes to directly establish an entanglement connection. The two methods both have pros and cons. Respectively, the former method has a higher success probability of constructing entanglement link, but it would consume more quantum resources. The latter method, however, has a lower success probability to deliver a photon across multiple quantum links with fewer quantum resources. Accordingly, we are expecting to establish significantly more entanglement connections with limited quantum resources by first creating entanglement segments, each spanning multiple quantum link, using all-optical switching, and then connecting them with quantum swapping. In this paper, we design SEE, a Segmented Entanglement Establishment approach that seamlessly integrates quantum swapping and all-optical switching to maximize quantum network throughput. SEE first creates entanglement segments over one or multiple quantum links with all-optical switching, and then connect them with quantum swapping. Accordingly, SEE can theoretically outperform conventional entanglement link-based approaches. Large scale simulations show that SEE can achieve up to 100.00% larger throughput compared with the state-of-the-art entanglement link-based approaches, e.g., Redundant Entanglement Provisioning and Selection (REPS).}
}


@article{DBLP:journals/ton/LenzenMSS24,
	author = {Christoph Lenzen and
                  Moti Medina and
                  Mehrdad Saberi and
                  Stefan Schmid},
	title = {Robust Routing Made Easy: Reinforcing Networks Against Non-Benign
                  Faults},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {283--297},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3283184},
	doi = {10.1109/TNET.2023.3283184},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LenzenMSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing scale of communication networks, the likelihood of failures grows as well. Since these networks form a critical backbone of our digital society, it is important that they rely on robust routing algorithms which ensure connectivity despite such failures. While most modern communication networks feature robust routing mechanisms, these mechanisms are often fairly complex to design and verify, as they need to account for the effects of failures and rerouting on communication. This paper conceptualizes the design of robust routing mechanisms, with the aim to avoid such complexity. In particular, we showcase simple and generic blackbox transformations that increase resilience of routing against independently distributed failures, which allows to simulate the routing scheme on the original network, even in the presence of non-benign node failures (henceforth called faults). This is attractive as the system specification and routing policy can simply be preserved. We present a scheme for constructing such a reinforced network, given an existing (synchronous) network and a routing scheme. We prove that this algorithm comes with small constant overheads, and only requires a minimal amount of additional node and edge resources; in fact, if the failure probability is smaller than 1/n\n, the algorithm can come without any overhead at all. At the same time, it allows to tolerate a large number of independent random (node) faults, asymptotically almost surely. We complement our analytical results with simulations on different real-world topologies.}
}


@article{DBLP:journals/ton/MurakamiKOY24,
	author = {Masaki Murakami and
                  Takashi Kurimoto and
                  Satoru Okamoto and
                  Naoaki Yamanaka},
	title = {Experimental Evaluation on Priority-Aware Guaranteed Resource Allocation
                  for Resource Pool Based Reconfigurable Hardware},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {298--307},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3288021},
	doi = {10.1109/TNET.2023.3288021},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MurakamiKOY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a priority-aware guaranteed hardware resource allocation in virtual packet optical nodes (VPONs) and describes experimental evidence of service provisioning with the proposed method on testbed. A network based on the VPON brings solution of service diversification and traffic explosion because it provides multiple services by hardware level separated virtual networks such as IP/Ethernet/ Multi-Protocol Label Switching. The VPON has reconfigurable hardware which is dedicatedly allocated for each service and service-specific logical functions are implemented respectively. However, a previous hardware resource allocation method is not efficient. In previous work, the hardware resources once allocated to a service are not reallocated until the service terminated whenever the hardware is not optimally used. In recent years, hardware reconfiguration time has become shorter and it has enabled to apply for service demand change by releasing hardware from services with low demand and reallocating it to other services with high demand. Thus, this paper proposes preemptive dynamic hardware resource reallocation algorithm for VPON and a priority-aware guaranteed hardware resource reallocation method to realize efficient resource usage and high service capacity. Results of the computer simulation show improving service capability. Furthermore, to demonstrate the feasibility of service provisioning with the proposed method, emulators of the virtual packet optical node were constructed and performed resource reallocation. Results of the experiment show that the virtual packet optical node can configure function chains for providing services based on calculation results of the proposed resource reallocation method.}
}


@article{DBLP:journals/ton/CarrascosaZamacoisGKB24,
	author = {Marc Carrascosa{-}Zamacois and
                  Giovanni Geraci and
                  Edward W. Knightly and
                  Boris Bellalta},
	title = {Wi-Fi Multi-Link Operation: An Experimental Study of Latency and Throughput},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {308--322},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3283154},
	doi = {10.1109/TNET.2023.3283154},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CarrascosaZamacoisGKB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we investigate the real-world capability of the multi-link operation (MLO) framework—one of the key MAC-layer features included in the IEEE 802.11be amendment—by using a large dataset containing 5 GHz spectrum occupancy measurements on multiple channels. Our results show that when both available links are often busy, as is the case in ultra-dense and crowded scenarios, MLO attains the highest throughput gains over single-link operation (SLO) since it is able to leverage multiple intermittent transmission opportunities. As for latency, if the two links exhibit statistically the same level of occupancy, MLO can outperform SLO by one order of magnitude. In contrast, in asymmetrically occupied links, MLO can sometimes be detrimental and even increase latency. We study this somewhat unexpected phenomenon, and find its origins to be packets suboptimally mapped to either link before carrying out the backoff, with the latter likely to be interrupted on the busier link. We cross validate our study with real-time traffic generated by a cloud gaming application and quantify MLO’s benefits for latency-sensitive applications.}
}


@article{DBLP:journals/ton/ZhangLL24,
	author = {Weiguang Zhang and
                  Jiarong Liang and
                  Xinyu Liang},
	title = {Approximation Algorithms for Computing Virtual Backbones Considering
                  Routing Costs in Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {323--337},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3284051},
	doi = {10.1109/TNET.2023.3284051},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The strategy of constructing a virtual backbone (VB) to perform routing tasks is considered a mature method for addressing the broadcast storm problem in wireless sensor networks (WSNs). A WSN can be regarded as a unit disk graph (UDG), and its VBs can be regarded as connected dominating sets (CDSs) in the UDG. The smaller the VB is, the less overhead and wireless signal collision and interference there are in the process of communication via the VB. Therefore, when a VB is designed in a WSN, its size is naturally an important factor to be considered. In addition, other factors that are often ignored by researchers, such as the routing cost of the VB, should be considered. In this article, we focus on how to construct a VB with a small size and routing cost in a WSN. We propose two centralized approximation algorithms: one is an algorithm for constructing a VB with a guaranteed routing cost, called RCC-CDS, and the other is an algorithm for constructing a VB with a bounded diameter, called BD-CDS. The performance ratio (PR) of RCC-CDS is 142.758, and that of BD-CDS is 13.596. We compare these two algorithms with previous works though theoretical analysis and simulation experiments on the basis of algorithm performance. The results obtained show that our algorithms have better performance.}
}


@article{DBLP:journals/ton/LiuCWYHZ24,
	author = {Xin Liu and
                  Zicheng Chi and
                  Wei Wang and
                  Yao Yao and
                  Pei Hao and
                  Ting Zhu},
	title = {High-Granularity Modulation for {OFDM} Backscatter},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {338--351},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286880},
	doi = {10.1109/TNET.2023.3286880},
	timestamp = {Wed, 17 Jul 2024 16:21:23 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuCWYHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Orthogonal frequency-division multiplexing (OFDM) has been widely used in WiFi, LTE, and adopted in 5G. Recently, researchers have proposed multiple OFDM-based WiFi backscatter systems that use the same underlying design principle (i.e., codeword translation) at the OFDM symbol-level to transmit the tag data. However, since the phase error correction in WiFi receivers can eliminate the phase offset created by a tag, the codeword translation requires specific WiFi receivers that can disable the phase error correction. As a result, phase error is introduced into the decoding procedure of the codeword translation, which significantly increases the tag data decoding error. To address this issue, we designed a novel OFDM backscatter called TScatter, which uses high-granularity sample-level modulation to avoid the phase offset created by a tag being eliminated by phase error correction. Moreover, by taking advantage of the phase error correction, our system is able to work in more dynamic environments. Our design also has two advantages: much lower bit error rate (BER) and higher throughput. We conducted extensive evaluations under different scenarios. The experimental results show that TScatter has i) three to four orders of magnitude lower BER when its throughput is similar to the latest OFDM backscatter system MOXcatter; or ii) more than 212 times higher throughput when its BER is similar to MOXcatter. Our design is generic and has the potential to be applied to backscatter other OFDM signals (e.g., LTE and 5G).}
}


@article{DBLP:journals/ton/ZengZLLY24,
	author = {Yiming Zeng and
                  Jiarui Zhang and
                  Ji Liu and
                  Zhenhua Liu and
                  Yuanyuan Yang},
	title = {Entanglement Routing Design Over Quantum Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {352--367},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3282560},
	doi = {10.1109/TNET.2023.3282560},
	timestamp = {Wed, 22 Jan 2025 12:19:52 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZengZLLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum networks have emerged as a future platform for quantum information exchange and applications, with promising capabilities far beyond traditional communication networks. Remote quantum entanglement is an essential component of a quantum network. How to efficiently design a multi-routing entanglement protocol is a fundamental yet challenging problem. In this paper, we study a quantum entanglement routing problem to simultaneously maximize the number of quantum-user pairs and their expected throughput. Our approach is to formulate the problem as two sequential integer programming problems. We propose efficient entanglement routing algorithms for these two optimization problems and analyze their time complexity and performance bounds. Evaluation results highlight that our approach outperforms existing solutions in both the number of quantum-user pairs served and network throughput.}
}


@article{DBLP:journals/ton/ChenXLLYSL24,
	author = {Lutong Chen and
                  Kaiping Xue and
                  Jian Li and
                  Ruidong Li and
                  Nenghai Yu and
                  Qibin Sun and
                  Jun Lu},
	title = {{Q-DDCA:} Decentralized Dynamic Congestion Avoid Routing in Large-Scale
                  Quantum Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {368--381},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3285093},
	doi = {10.1109/TNET.2023.3285093},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenXLLYSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quantum network that allows users to communicate in a quantum way will be available in the foreseeable future. The network capable of distributing Bell state entangled pairs faces many challenges due to entanglement decoherence and limited network performance, especially when the network scale is enormous. Many entanglement distribution protocols have been proposed so far, and most of them are in a centralized and synchronized manner, which may be infeasible in large-scale networks. As such, in this paper, we propose a full spontaneous version of quantum networks in which the quantum nodes autonomously manage multiple entanglement distribution requests. However, one major issue is that quantum nodes have little knowledge about the network, especially the congestion (e.g., some nodes may have no usable quantum memories). We present a routing algorithm to adaptive evaluate the congestion on the neighbor nodes to avoid potential congestion. We use SimQN, the new network layer simulation platform built by our research team, to evaluate our proposed design. The result demonstrates that it can adapt to changes in network resources and reduce the drop rate that eventually leads to a higher entanglement distribution rate but remains fair for multiple requests to use the network resources fairly and achieve a more balanced throughput.}
}


@article{DBLP:journals/ton/XieLDLDJZY24,
	author = {Guorui Xie and
                  Qing Li and
                  Guanglin Duan and
                  Jiaye Lin and
                  Yutao Dong and
                  Yong Jiang and
                  Dan Zhao and
                  Yuan Yang},
	title = {Empowering In-Network Classification in Programmable Switches by Binary
                  Decision Tree and Knowledge Distillation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {382--395},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3287091},
	doi = {10.1109/TNET.2023.3287091},
	timestamp = {Tue, 13 Aug 2024 14:11:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieLDLDJZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the high packet processing efficiency of programmable switches (e.g., P4 switches of Tbps), several works are proposed to offload the decision tree (DT) to P4 switches for in-network classification. Although the DT is suitable for the match-action paradigm in P4 switches, the range match rules used in the DT may not be supported across devices of different P4 standards. Additionally, emerging models including neural networks (NNs) and ensemble models, have shown their superior performance in networking tasks. But their sophisticated operations pose new challenges to the deployment of these models in switches. In this paper, we propose Mousikav2 to address these drawbacks successfully. First, we design a new tree model, i.e., the binary decision tree (BDT). Unlike the DT, our BDT consists of classification rules in the form of bits, which is a good fit for the standard ternary match supported by different hardware/software switches. Second, we introduce a teacher-student knowledge distillation architecture in Mousikav2, which enables the general transfer from other sophisticated models to the BDT. Through this transfer, sophisticated models are indirectly deployed in switches to avoid switch constraints. Finally, a lightweight P4 program is developed to perform classification tasks in switches with the BDT after knowledge distillation. Experiments on three networking tasks and three commodity switches show that Mousikav2 not only improves the classification accuracy by 3.27%, but also reduces the switch stage and memory usage by 2.00\\times\nand 28.67%, respectively. Code is available at https://github.com/xgr19/Mousika.}
}


@article{DBLP:journals/ton/CaiLKCLQM24,
	author = {Mingxin Cai and
                  Yutong Liu and
                  Linghe Kong and
                  Guihai Chen and
                  Liang Liu and
                  Meikang Qiu and
                  Shahid Mumtaz},
	title = {Resource Critical Flow Monitoring in Software-Defined Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {396--410},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286691},
	doi = {10.1109/TNET.2023.3286691},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CaiLKCLQM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flow monitoring is widely applied in software-defined networks (SDNs) for monitoring network performance. Especially, detecting heavy hitters can prevent the Distributed Denial of Service (DDoS) attack. However, many existing approaches fall into one of two undesirable extremes: (i) inefficient collection where only accuracy is concerned in the method; (ii) sacrifice of accuracy due to fast detection. One practical problem with this is that it does not have the flexibility to adjust the monitoring strategy to the monitoring needs, making it difficult to meet different applications. To alleviate this problem, we propose our design of a novel flow monitoring framework that keeps the balance between accuracy and efficiency. It provides customized monitoring services for applications, where network resources can be saved, and the error rate can also be confined. In this paper, we present cReFeR, a three-step “compression Report-Feedback-Report” framework to monitor SDNs. The IP and the value compressor are specially designed to reduce the volume of flow statistics collection. This framework thus can achieve accuracy-ensured and resource-saving flow monitoring in SDNs. Theoretical analysis and simulated evaluation have proved the effectiveness of our solution. cReFeR keeps the error rate under 3% and reduces the amount of monitoring data more than 40%, which guarantees high efficiency compared with existing methods.}
}


@article{DBLP:journals/ton/GuTCWWZ24,
	author = {Liyuan Gu and
                  Ye Tian and
                  Wei Chen and
                  Zhongxiang Wei and
                  Cenman Wang and
                  Xin{-}Ming Zhang},
	title = {Per-Flow Network Measurement With Distributed Sketch},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {411--426},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286879},
	doi = {10.1109/TNET.2023.3286879},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuTCWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketch-based method has emerged as a promising direction for per-flow measurement in data center networks. Usually in such a measurement system, a sketch data structure is placed as a whole at one switch for counting all passing packets, but when summarizing measurement results from multiple switches, the overall accuracy is generally constrained by a few individual switches with small-sized sketches due to their limited memory resources. To address this problem, in this paper, we present Distributed Sketch, a new method for per-flow network measurement in data center networks. In Distributed Sketch, each network path is associated with a logical sketch, whose data structure is collectively maintained by all the switches along the path; meanwhile, each switch multiplexes its physical sketch to the constructions of the logical sketches of all the paths it belongs to. With Distributed Sketch, switches collaborate to measure network flows, and the network-wide measurement workload is fairly distributed among all the switches in the network. We implement Distributed Sketch with P4 on commodity hardware programmable switch, and in particular, to overcome the limitation that hardware switches do not support float-point computation, we present an optimal approximation method that involves only integer operations. We also propose an In-band Network Telemetry (INT) based method for addressing the challenges in deploying Distributed Sketch in large-scale data centers. Experiment results and theoretical analysis show that our proposed method is lightweight regarding measurement overhead, and by aggregating and making fair uses of resources from all the switches in the network, Distributed Sketch achieves a higher measurement accuracy compared with the state-of-the-art solutions.}
}


@article{DBLP:journals/ton/ZhuZWWGZT24,
	author = {Fengyuan Zhu and
                  Renjie Zhao and
                  Bingbing Wang and
                  Xinbing Wang and
                  Xinping Guan and
                  Chenghu Zhou and
                  Xiaohua Tian},
	title = {Enabling {OFDMA} in Wi-Fi Backscatter},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {427--444},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3290370},
	doi = {10.1109/TNET.2023.3290370},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuZWWGZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper for the first time demonstrates how to enable OFDMA in Wi-Fi backscatter for capacity and concurrency enhancement. With our design, the excitation signal is reflected, modulated and shifted to lie in the frequency band of the OFDM subcarrier by the tag; OFDMA is realized by coordinating tags to convey information to the receiver with orthogonal subcarriers concurrently through backscatter. The crux of the design is to achieve strict synchronization among communication components, which is more challenging than in regular OFDMA systems due to the more prominent hardware diversity and uncertainty for backscattering. We reveal how the subtle synchronization scenarios particularly for backscattering can incur system offsets, and present a series of novel designs for the excitation signal transmitter, tag, and receiver to address the issue. We build a prototype in 802.11g OFDM framework to validate our design. Experimental results show that our system can achieve 5.2- 16Mbps aggregate throughput by allowing 48 tags to transmit concurrently, which is 1.45- 5\\times capacity and 48\\times concurrency compared with the existing design respectively. We also design an OFDMA tag IC, with the corresponding simulation and numerical analysis results show that the tag’s power consumption is in tens of \\mu W .}
}


@article{DBLP:journals/ton/ChenZMCJWQLL24,
	author = {Ning Chen and
                  Sheng Zhang and
                  Zhi Ma and
                  Yu Chen and
                  Yibo Jin and
                  Jie Wu and
                  Zhuzhong Qian and
                  Yu Liang and
                  Sanglu Lu},
	title = {ViChaser: Chase Your Viewpoint for Live Video Streaming With Block-Oriented
                  Super-Resolution},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {445--459},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286108},
	doi = {10.1109/TNET.2023.3286108},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenZMCJWQLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The usage of live streaming services has led to a substantial increase in live video traffic. However, the perceived quality of experience of users is frequently limited by variations in the upstream bandwidth of streamers. To address this issue, several adaptive bitrate (ABR) algorithms have been developed to mitigate bandwidth variations. Nevertheless, the ability of users to enjoy high-quality live streams remains limited. While neural-enhanced approaches, such as super-resolution, offer significant quality improvements, frame-oriented super-resolution leads to excessive inference delay that violates the real-time feature of live streaming. In response, we propose ViChaser, which examines block-oriented super-resolution for live streaming. ViChaser performs neural super-resolution on potential blocks of interest in the media server, corresponding to the user’s viewpoint, and uses online learning to adapt to the dynamic content of the video. Additionally, ViChaser utilizes the Lyapunov framework to efficiently allocate uplink bandwidth for original low-quality live video and high-quality labels. The experimental results demonstrate that ViChaser achieves 1.2–1.5 dB higher video quality in Peak-Signal-to-Noise-Ratio than WebRTC and increases processing speed by 11–16 fps relative to LiveNAS.}
}


@article{DBLP:journals/ton/WeiWL24,
	author = {Qinhan Wei and
                  Yongcai Wang and
                  Deying Li},
	title = {{EMI:} An Efficient Algorithm for Identifying Maximal Rigid Clusters
                  in 3D Generic Graphs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {460--474},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3287822},
	doi = {10.1109/TNET.2023.3287822},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WeiWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying the Maximal Rigid subGraphs (MRGs) whose relative formations cannot deform continuously in \\Re ^{d}\n, is a fundamental problem in network formation control and network localization. When d=3\n, it becomes extremely challenging and has been open for decades because the fundamental Laman condition doesn’t hold in \\Re ^{3}\n. This paper presents a new understanding of this problem. Because of the existence of “implicit hinges” in 3D, its essence should be to detect the Maximal Rigid Clusters (MRCs). An MRC is a maximal set of vertices in which each vertex is mutually rigid to the others, but the vertices are not necessarily connected. We show that the MRGs in the original graph can be easily deduced from the connected components generated by the MRCs. For efficiently identifying the MRCs, at first, a randomized algorithm to detect mutually rigid vertex pairs is exploited. Based on this, a Basic MRC Identification algorithm (BMI) is proposed, which is an exact algorithm that can detect all MRCs based on the extracted rigid vertex pairs, but it has O(|V|^{4})\ntime complexity. To further pursue an efficient algorithm, we observe the “hinge MRCs” appear rarely. So an Efficient framework for MRC Identification (EMI) is proposed. It consists of two steps: 1) a Trimmed-BMI algorithm that guarantees to detect all simple MRCs and may miss only hinge MRCs; 2) a Trim-FIX algorithm that can find all hinge MRCs. We prove EMI can guarantee to detect all the MRCs as accurately as BMI, using O(|V|^{3})\ntimes. Further, we show EMI achieves magnitudes of times faster than BMI in experiments. Extensive evaluations verify the effectiveness and high efficiency of EMI in various 3D networks. We have uploaded the code of the related program to https://github.com/fdwqh/EMI-algorithm.}
}


@article{DBLP:journals/ton/AbdisarabshaliLRAH24,
	author = {Payam Abdisarabshali and
                  Minghui Liwang and
                  Amir Rajabzadeh and
                  Mahmood Ahmadi and
                  Seyyedali Hosseinalipour},
	title = {Decomposition Theory Meets Reliability Analysis: Processing of Computation-Intensive
                  Dependent Tasks Over Vehicular Clouds With Dynamic Resources},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {475--490},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3286709},
	doi = {10.1109/TNET.2023.3286709},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AbdisarabshaliLRAH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular cloud (VC) is a promising technology for processing computation-intensive applications (CI-Apps) on smart vehicles. Implementing VCs over the network edge faces two key challenges: (C1) On-board computing resources of a single vehicle are often insufficient to process a CI-App; (C2) The dynamics of available resources, caused by vehicles’ mobility, hinder reliable CI-App processing. This work is among the first to jointly address (C1) and (C2), while considering two common CI-App graph representations, directed acyclic graph (DAG) and undirected graph (UG). To address (C1), we consider partitioning a CI-App with m dependent (sub-)tasks into k\\le m groups, which are dispersed across vehicles. To address (C2), we introduce a generalized reliability metric called conditional mean time to failure (C-MTTF). Subsequently, we increase the C-MTTF of dependent sub-tasks processing via introducing a general framework of redundancy-based processing of dependent sub-tasks over semi-dynamic VCs (RP-VC). We demonstrate that RP-VC can be modeled as a non-trivial semi-Markov process (SMP). To analyze this SMP model and its reliability, we develop a novel mathematical framework, called event stochastic algebra ( \\langle e\\rangle -algebra). Based on \\langle e\\rangle -algebra, we propose decomposition theorem (DT) to transform the presented SMP to a decomposed SMP (D-SMP). We subsequently calculate the C-MTTF of our methodology. We demonstrate that \\langle e\\rangle -algebra and DT are general mathematical tools that can be used to analyze other cloud-based networks. Simulation results reveal the exactness of our analytical results and the efficiency of our methodology in terms of acceptance and success rates of CI-App processing.}
}


@article{DBLP:journals/ton/WangYL24,
	author = {Xiong Wang and
                  Jiancheng Ye and
                  John C. S. Lui},
	title = {Mean Field Graph Based {D2D} Collaboration and Offloading Pricing
                  in Mobile Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {491--505},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3288558},
	doi = {10.1109/TNET.2023.3288558},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) facilitates computation offloading to edge server and task processing via device-to-device (D2D) collaboration. Existing works mainly focus on centralized network-assisted offloading solutions, which are unscalable to collaborations among massive users. In this paper, we propose a joint framework of decentralized D2D collaboration and task offloading for MEC systems with large populations. Specifically, we utilize the power of two choices for D2D collaboration, which enables users to assist each other in a decentralized manner. Due to short-range D2D communication and user movements, we formulate a mean field model on a finite-degree and dynamic graph to analyze the collaboration state evolution. We derive the existence, uniqueness and convergence of the state stationary point to provide a tractable collaboration performance. Complementing this D2D collaboration, we further build a Stackelberg game to model users’ task offloading, where the provider, managing many servers, is the leader to determine service prices, while users are followers to make offloading decisions. By embedding Stackelberg game into Lyapunov optimization, we develop an online offloading and pricing scheme, which can optimize servers’ service utility or fairness, and users’ system cost simultaneously. Extensive evaluations show that D2D collaboration can mitigate users’ workloads by 73.8% and fair pricing can promote servers’ utility fairness by 15.87%.}
}


@article{DBLP:journals/ton/LiaoQZZCXL24,
	author = {Zhengyu Liao and
                  Shiyou Qian and
                  Zhonglong Zheng and
                  Jiange Zhang and
                  Jian Cao and
                  Guangtao Xue and
                  Minglu Li},
	title = {PT-Tree: {A} Cascading Prefix Tuple Tree for Packet Classification
                  in Dynamic Scenarios},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {506--519},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3289029},
	doi = {10.1109/TNET.2023.3289029},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoQZZCXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For software-defined networking (SDN), multi-field packet classification plays a key role in the processing of flows, mainly involving fast packet classification and dynamic rule updates. Due to the increasing complexity and size of rulesets, it is becoming more difficult to design a packet classification algorithm which achieves fast lookup and update. In this paper, we propose a novel structure, PT-Tree, for packet classification with high overall performance. PT-Tree cascades the prefixes of multiple discriminatory bytes to achieve efficient partitioning of the ruleset, thereby reducing the search space and ensuring the performance of both lookup and update. Meanwhile, a multi-granularity priority-aware pruning mechanism (MPPM) based on PT-Tree filters out most of the candidate subsets, which further improves the lookup speed. In addition, we propose an auxiliary tree-based optimization method (ATOM) to cope with severely overlapping rules in the search space. Therefore, PT-Tree can better handle the case where the rules in certain fields are skewed. We conduct comprehensive experiments to evaluate the performance of PT-Tree. The results show that compared with the state-of-the-art, the lookup time of PT-Tree is reduced by at least 49.95% on average. Moreover, PT-Tree is also at least 7.13x and 33x faster than the baselines in terms of the update and construction speed on average, respectively. Meanwhile, the performance stability of PT-Tree on multiple rulesets improves by up to 13.68 times.}
}


@article{DBLP:journals/ton/ZhengDZYGC24,
	author = {Jiaqi Zheng and
                  Zhuoxuan Du and
                  Zhenqing Zha and
                  Zixuan Yang and
                  Xiaofeng Gao and
                  Guihai Chen},
	title = {Learning to Configure Converters in Hybrid Switching Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {520--534},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3294803},
	doi = {10.1109/TNET.2023.3294803},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengDZYGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data centers heavily rely on scale-out architectures like fat-tree, BCube and VL2 to accommodate a large number of commodity servers. Since the traditional electrical network is demand-oblivious and cannot perfectly respond to the bursty traffic generated by big data applications, a growing trend is to design demand-aware topologies via introducing the converters with adjustable optical links, instead of adding more wiring links. However, little is known today about how to fully exploit the potential of the flexibility from the converters: the joint optimization on adjusting the optical links inside the converters and the routing in the whole network remains algorithmically challenging. In this paper, we design a set of customized converters for Diamond, VL2 and BCube topologies and initiate the optimization study in hybrid switching data center networks. As a case study, we introduce demand-aware load balancing problem (DLBP), i.e., a joint optimization on the physical layer (how the optical links interconnect inside the converter) and the network layer (how to determine the route especially for elephant flows in the whole network). We prove that DLBP is not only NP-hard, but also \\rho\n-inapproximation. Accordingly, we design two algorithms: the first one is an intuitive greedy algorithm and the second one uses reinforcement learning to improve upon the solution of the first one. Trace-driven evaluations show that our algorithms can reduce the traffic congestion by 12% on average.}
}


@article{DBLP:journals/ton/TianZLOFTW24,
	author = {Xiaohua Tian and
                  Fengyuan Zhu and
                  Hao Li and
                  Mingwei Ouyang and
                  Luwei Feng and
                  Xinyu Tong and
                  Xinbing Wang},
	title = {MobiScatter: Enhancing Capacity in Drone-Assisted High-Concurrency
                  Backscatter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {535--549},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3290168},
	doi = {10.1109/TNET.2023.3290168},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TianZLOFTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents MobiScatter, which enhances capacity of CSS based backscatter networks for accommodating drone-carried access points (APs). CSS based backscatter design has favorable features including long range and high concurrency. However, the concurrency of the network can be reduced by 34% when the drone-carried AP is introduced due to the mobility and fast fading. In order to maintain high concurrency, MobiScatter presents a series of new designs. In particular, we propose to enhance concurrency of the CSS based backscatter network with symmetric upchirps and downchirps, which neutralizes the impact of imperfect frequency orthogonality. Then, to mitigate the impact of fast fading on decoding, we present a novel half-period chirp modulation scheme for crossed chirps. Finally, we provide a power management method for tags by controlling transmitting time of chirps. We construct a MobiScatter prototype, which contains a drone-carried AP implemented with a mobile USRP and 200 tags. We deploy those tags in an area of 200 m\\times 180 m on a meadow. Experimental results show that MobiScatter can support 160 concurrent backscatter transmissions when the AP moves at 15 m/s .}
}


@article{DBLP:journals/ton/ZengCHLZZ24,
	author = {Liekang Zeng and
                  Xu Chen and
                  Peng Huang and
                  Ke Luo and
                  Xiaoxi Zhang and
                  Zhi Zhou},
	title = {Serving Graph Neural Networks With Distributed Fog Servers for Smart
                  IoT Services},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {550--565},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3293052},
	doi = {10.1109/TNET.2023.3293052},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZengCHLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environments. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and fog deployment by up to\n5.39×\nexecution speedup and\n6.84×\nthroughput improvement.}
}


@article{DBLP:journals/ton/WangLYWL24,
	author = {Hao Wang and
                  Chi Harold Liu and
                  Haoming Yang and
                  Guoren Wang and
                  Kin K. Leung},
	title = {Ensuring Threshold AoI for UAV-Assisted Mobile Crowdsensing by Multi-Agent
                  Deep Reinforcement Learning With Transformer},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {566--581},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3289172},
	doi = {10.1109/TNET.2023.3289172},
	timestamp = {Tue, 16 Apr 2024 15:32:13 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangLYWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) crowdsensing (UCS) is an emerging data collection paradigm to provide reliable and high quality urban sensing services, with age-of-information (AoI) requirement to measure data freshness in real-time applications. In this paper, we explicitly consider the case to ensure that the attained AoI always stay within a specific threshold. The goal is to maximize the total amount of collected data from diverse Point-of-Interests (PoIs) while minimizing AoI and AoI threshold violation ratio under limited energy supplement. To this end, we propose a decentralized multi-agent deep reinforcement learning framework called “DRL-UCS( \\text {AoI}_{th} )” for multi-UAV trajectory planning, which consists of a novel transformer-enhanced distributed architecture and an adaptive intrinsic reward mechanism for spatial cooperation and exploration. Extensive results and trajectory visualization on two real-world datasets in Beijing and San Francisco show that, DRL-UCS( \\text {AoI}_{th} ) consistently outperforms all nine baselines when varying the number of UAVs, AoI threshold and generated data amount in a timeslot.}
}


@article{DBLP:journals/ton/QiSW24,
	author = {Jianpeng Qi and
                  Xiao Su and
                  Rui Wang},
	title = {Toward Distributively Build Time-Sensitive-Service Coverage in Compute
                  First Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {582--597},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3289830},
	doi = {10.1109/TNET.2023.3289830},
	timestamp = {Thu, 31 Oct 2024 16:54:19 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite placing services and computing resources at the edge of the network for ultra-low latency, we still face the challenge of centralized scheduling costs, including delays from additional request forwarding and resource selection. To address this challenge, we propose SmartBuoy, a new computing paradigm. Our approach starts with a service coverage concept that assumes users within the coverage have high access availability. To enable users to perceive service status, we design a distributed metric table that synchronizes service status periodically and distributively. We propose coverage indicator updating principles to make the updating process more effective. We then implement two distributed methods, SmartBuoy-Time and SmartBuoy-Reliability, that enable users to perceive service capability directly and immediately. To determine the metric table update window size, we provide an analysis method based on user access patterns and offer a theoretical upper bound in a dynamic environment, making SmartBuoy easy to use. Finally, we implement the proposed methods distributively on an open-source edge computing simulator. Experiments on a real-world network topology dataset demonstrate the efficiency of SmartBuoy in reducing delays and improving the success rate.}
}


@article{DBLP:journals/ton/GuidolinPinaBB24,
	author = {Damien Guidolin{-}Pina and
                  Marc Boyer and
                  Jean{-}Yves Le Boudec},
	title = {Configuration of Guard Band and Offsets in Cyclic Queuing and Forwarding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {598--612},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3293050},
	doi = {10.1109/TNET.2023.3293050},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuidolinPinaBB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyclic Queuing and Forwarding (CQF) is a mechanism defined by IEEE TSN for providing low jitter in a deterministic network. CQF uses a common time cycle and two buffers per node output port: during one cycle incoming packets are stored in one buffer while packets in the other buffer are being transmitted; at the end of a cycle, the roles of the two buffers are exchanged. The cycle start times are determined by a time offset that may be different for every output buffer. A guard band at both cycle ends is devised in order to compensate for misalignment and timing inaccuracies. The proper operation of CQF requires that the guard band and the offsets are computed such that nodes are sufficiently time-aligned. First, we give necessary and sufficient conditions for this to be guaranteed. The sufficient conditions lend themselves to tractable computations and we show that they are close to optimal. Our conditions account for nonideal clocks and non-zero propagation times; we show that accounting for these two elements does matter. Second, we give a method for computing the minimal duration of the guard band, given prior choices of time offsets. Third, a judicious choice of time offsets can considerably decrease the required duration of the guard band: we give a practical algorithm, based on a Mixed Integer Linear Program, for computing offsets that minimize the guard band. We illustrate our results on several CQF network topologies with or without cyclic dependencies.}
}


@article{DBLP:journals/ton/ZhangWCLSZZL24,
	author = {Xiaohan Zhang and
                  Jinwen Wang and
                  Yueqiang Cheng and
                  Qi Li and
                  Kun Sun and
                  Yao Zheng and
                  Ning Zhang and
                  Xinghua Li},
	title = {Interface-Based Side Channel in TEE-Assisted Networked Services},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {613--626},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3294019},
	doi = {10.1109/TNET.2023.3294019},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWCLSZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the accelerating adaption of Cloud and Edge computing, cloud-based networked deployment emerges to enable providers to deliver services in a cost-effective and elastic manner. However, security concern remains one of the major obstacles to its wider adaption. Trusted Execution Environment (TEE) has been advocated to protect cloud services in an isolated execution environment. In this paper, we present a new genre of side-channel attack called interface-based side-channel attack and demonstrate its effectiveness on the TEE-assisted networked service system. The root cause of this attack is the input-dependent interface invocation (e.g., interface information and invocation patterns) that can be observed by untrusted software to reveal the control flows inside the enclave. Our evaluation demonstrates that the attack can effectively re-identify encrypted web pages processed in the SGX enclave with an accuracy of 87.6% and a recall of 76.6%, and can reduce the search domain of the 1024 bits RSA private keys to 1.69 \\times 10^{-6} of the original search domain. As countermeasures, we propose, implement and evaluate a set of static analysis tools to mitigate the newly discovered threats. The key idea is to use inter-procedural dataflow analysis to identify potential leakage via the interface, and then mitigate them during compilation using techniques including branch obfuscation, loop obfuscation, and constant size wrapper.}
}


@article{DBLP:journals/ton/ZhangZZHC24,
	author = {Junxue Zhang and
                  Chaoliang Zeng and
                  Hong Zhang and
                  Shuihai Hu and
                  Kai Chen},
	title = {LiteFlow: Toward High-Performance Adaptive Neural Networks for Kernel
                  Datapath},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {627--642},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3293152},
	doi = {10.1109/TNET.2023.3293152},
	timestamp = {Mon, 22 Apr 2024 09:27:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZZHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive neural networks (NN) have been used to optimize OS kernel datapath functions because they can achieve superior performance under changing environments. However, how to deploy these NNs remains a challenge. One approach is to deploy these adaptive NNs in the userspace. However, such userspace deployments suffer from either high cross-space communication overhead or low responsiveness, significantly compromising the function performance. On the other hand, pure kernel-space deployments also incur a large performance degradation because the computation logic of model tuning algorithm is typically complex, interfering with the performance of normal datapath execution. This paper presents LiteFlow, a hybrid solution to build high-performance adaptive NNs for kernel datapath. At its core, LiteFlow decouples the control path of adaptive NNs into: 1) a kernel-space fast path for efficient model inference; and 2) a userspace slow path for effective model tuning. We have implemented LiteFlow with Linux kernel datapath and evaluated it with three popular datapath functions including congestion control, flow scheduling, and load balancing. Compared to prior works, LiteFlow achieves 44.4% better goodput for congestion control, and improves the completion time for long flows by 33.7% and 56.7% for flow scheduling and load balancing, respectively.}
}


@article{DBLP:journals/ton/GangulyA24,
	author = {Bhargav Ganguly and
                  Vaneet Aggarwal},
	title = {Online Federated Learning via Non-Stationary Detection and Adaptation
                  Amidst Concept Drift},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {643--653},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3294366},
	doi = {10.1109/TNET.2023.3294366},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GangulyA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is an emerging domain in the broader context of artificial intelligence research. Methodologies pertaining to FL assume distributed model training, consisting of a collection of clients and a server, with the main goal of achieving optimal global model with restrictions on data sharing due to privacy concerns. It is worth highlighting that the diverse existing literature in FL mostly assume stationary data generation processes; such an assumption is unrealistic in real-world conditions where concept drift occurs due to, for instance, seasonal or period observations, faults in sensor measurements. In this paper, we introduce a multiscale algorithmic framework which combines theoretical guarantees of FedAvg and FedOMD algorithms in near stationary settings with a non-stationary detection and adaptation technique to ameliorate FL generalization performance in the presence of concept drifts. We present a multi-scale algorithmic framework leading to \\tilde {\\mathcal {O}} (\\min \\{ \\sqrt {LT}, \\Delta ^{({1}/{3})}T^{({2}/{3})} + \\sqrt {T} \\})\ndynamic regret for T\nrounds with an underlying general convex loss function, where L\nis the number of times non-stationary drifts occurred and \\Delta\nis the cumulative magnitude of drift experienced within T\nrounds.}
}


@article{DBLP:journals/ton/AyepahMensahSBAL24,
	author = {Daniel Ayepah{-}Mensah and
                  Guolin Sun and
                  Gordon Owusu Boateng and
                  Stephen Anokye and
                  Guisong Liu},
	title = {Blockchain-Enabled Federated Learning-Based Resource Allocation and
                  Trading for Network Slicing in 5G},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {654--669},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3297390},
	doi = {10.1109/TNET.2023.3297390},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AyepahMensahSBAL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Access Network (RAN) slicing enables resource sharing among multiple tenants and is an essential feature for next-generation mobile networks. Usually, a centralized controller aggregates available resource pools from multiple tenants to increase spectrum availability. In dynamic resource allocation, a tenant could behave strategically by adjusting its preferences based on perceived conditions to maximize its utility. Slice tenants may lie about the resources needed to gain greater utility. Such behavior could lead to poor resource utilization due to excess resources acquired by lying tenants and resource shortages because slice tenants choose not to purchase high-priced resources to save costs. Furthermore, in a scenario with many slice tenants, the centralized controller can become overwhelmed by the number of requests. This, in turn, can lead to slower response times and higher latency, resulting in poor resource utilization and QoS performance of slice tenants. Therefore, this paper proposes a peer-to-peer (P2P) approach to resource trading, where slice tenants communicate directly instead of relying on a centralized orchestrator. This design is motivated by the need for slice tenants to collaborate effectively. We model the interaction between tenants in a Stackelberg multi-leader and multi-follower game and solve the game with multi-agent deep reinforcement learning with an incentive-reward model to achieve the Stackelberg equilibrium. Furthermore, we propose a decentralized resource trading framework by integrating blockchain technology and federated deep reinforcement learning, enabling network tenants to perform inter-slice resource sharing securely. The simulation results show that the proposed mechanism has significant performance improvements over existing implementations.}
}


@article{DBLP:journals/ton/WuSLBX24,
	author = {Si Wu and
                  Zhirong Shen and
                  Patrick P. C. Lee and
                  Zhiwei Bai and
                  Yinlong Xu},
	title = {Elastic Reed-Solomon Codes for Efficient Redundancy Transitioning
                  in Distributed Key-Value Stores},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {670--685},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3303865},
	doi = {10.1109/TNET.2023.3303865},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WuSLBX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern distributed key-value (KV) stores increasingly adopt erasure coding to reliably store data. To adapt to the changing demands on access performance and reliability requirements, distributed KV stores perform redundancy transitioning by tuning the redundancy schemes with different coding parameters. However, redundancy transitioning incurs extensive network I/Os, which impair the performance of distributed KV stores. We propose a new family of erasure codes, called Elastic Reed-Solomon (ERS) codes, whose primary goal is to mitigate network I/Os in redundancy transitioning. ERS codes eliminate data block relocation, while limiting network I/Os for parity block updates via the new co-design of encoding matrix construction and data placement. ERS codes achieve such gains in both forward and backward transitioning scenarios. We realize ERS codes in a distributed KV store prototype based on Memcached, and show via testbed experiments in both local and cloud environments that ERS codes significantly reduce the latency of redundancy transitioning compared with state-of-the-arts.}
}


@article{DBLP:journals/ton/VoronovRR24,
	author = {Tomer Voronov and
                  Danny Raz and
                  Ori Rottenstreich},
	title = {A Framework for Anomaly Detection in Blockchain Networks With Sketches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {686--698},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3298253},
	doi = {10.1109/TNET.2023.3298253},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VoronovRR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A blockchain is a distributed ledger composed of immutable blocks of data that often refer to money transfers. As blockchain networks gain popularity, there is a rising concern for security against malicious and hacking users. Detection anomalies and unusual account activities can be based on comparing upcoming activity with recent and historical data. However, the size and rapid growth of the complete blockchain history can result in slow and expensive processing. This paper proposes a solution to this challenge by analyzing summarized block data structures, known as sketches, instead of the entire blockchain. Sketches are commonly used in computer systems and blockchain networks to provide efficient query executions while maintaining a compact data representation. This study explores the use of sketches, such as Bloom Filter and HyperLogLog, to identify suspicious accounts without requiring the examination of the entire blockchain data. We design solutions for anomaly detection of certain goals that may be indications of known attacks. We develop methods to identify accounts with high transaction volume, frequency, and node degree. Furthermore, the innovation of this paper lies in the generalization of sketch-based anomaly detection through a generic solution capable of addressing diverse queries. We conduct experiments based on real Ethereum data and compare the accuracy, time complexity, and memory usage of our algorithms with traditional detection algorithms that rely on the complete blockchain data. Our results indicate that sketch-based anomaly detection methods can provide a practical and scalable solution for detecting anomalies in transactions on blockchain networks. We managed to reduce the amount of memory used by the detection process by 90%-96% and reduce the time complexity by 86% while maintaining high accuracy.}
}


@article{DBLP:journals/ton/WangWLLMRWRD24,
	author = {Zihao Wang and
                  Hang Wang and
                  Zhuowen Li and
                  Xinghua Li and
                  Yinbin Miao and
                  Yanbing Ren and
                  Yunwei Wang and
                  Zhe Ren and
                  Robert H. Deng},
	title = {Robust Permissioned Blockchain Consensus for Unstable Communication
                  in {FANET}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {699--712},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3295378},
	doi = {10.1109/TNET.2023.3295378},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangWLLMRWRD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The utilization of blockchain technology as a distributed information sharing system has gained widespread adoption across various domains. However, its application to Flying Ad-Hoc Network (FANET), characterized by severe packet loss, poses significant challenges. The high packet loss rates in FANETs can result in decreased consensus success rates and negatively impact information sharing consistency and efficiency. In this paper, we proposed RoUBC, a novel consensus scheme for Flying Ad-Hoc Networks (FANET), which is based on the Raft protocol and is designed to address the challenges posed by the severe packet loss network in FANET. The proposed scheme consists of two phases: leader election and block consensus. In the leader election phase, we integrate multi-criteria decision-making and link prediction algorithms to design an efficient stable-leader election method. In the block consensus phase, we propose a dynamic block verification algorithm based on historical verification information to achieve efficient block consensus. Our theoretical analysis demonstrates that the proposed consensus protocol is safe and live, effectively ensuring the consistency of message sharing in FANET. Experiment results show that our scheme outperforms traditional Raft schemes, with 35% increase in consensus success rate and 25% improvement in consensus efficiency.}
}


@article{DBLP:journals/ton/ZengCFZC24,
	author = {Liekang Zeng and
                  Haowei Chen and
                  Daipeng Feng and
                  Xiaoxi Zhang and
                  Xu Chen},
	title = {{A3D:} Adaptive, Accurate, and Autonomous Navigation for Edge-Assisted
                  Drones},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {713--728},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3297876},
	doi = {10.1109/TNET.2023.3297876},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZengCFZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate navigation is of paramount importance to ensure flight safety and efficiency for autonomous drones. Recent research starts to use Deep Neural Networks (DNN) to enhance drone navigation given their remarkable predictive capability for visual perception. However, existing solutions either run DNN inference tasks on drones in situ, impeded by the limited onboard resource, or offload the computation to external servers which may incur large network latency. Few works consider jointly optimizing the offloading decisions along with image transmission configurations and adapting them on the fly. In this paper, we propose A3D, an edge server assisted drone navigation framework that can dynamically adjust task execution location, input resolution, and image compression ratio in order to achieve low inference latency, high prediction accuracy, and long flight distances. Specifically, we first augment state-of-the-art convolutional neural networks for drone navigation and define a novel metric called Quality of Navigation as our optimization objective which can effectively capture the above goals. We then design a deep reinforcement learning (DRL) based neural scheduler at the drone side for which an information encoder is devised to reshape the state features and thus improve its learning ability. To further support simultaneous multi-drone serving, we extend the edge server design by developing a network-aware resource allocation algorithm, which allows provisioning containerized resources aligned with drones’ demand. We finally implement a proof-of-concept prototype with realistic devices and validate its performance in a real-world campus scene, as well as a simulation environment for thorough evaluation upon AirSim. Extensive experimental results show that A3D can reduce end-to-end latency by 28.06% and extend the flight distance by up to 27.28% compared with non-adaptive solutions.}
}


@article{DBLP:journals/ton/LuoLMZLRLD24,
	author = {Bin Luo and
                  Xinghua Li and
                  Yinbin Miao and
                  Man Zhang and
                  Ximeng Liu and
                  Yanbing Ren and
                  Xizhao Luo and
                  Robert H. Deng},
	title = {PAM\({}^{\mbox{3}}\)S: Progressive Two-Stage Auction-Based Multi-Platform
                  Multi-User Mutual Selection Scheme in {MCS}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {729--744},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3297258},
	doi = {10.1109/TNET.2023.3297258},
	timestamp = {Fri, 14 Feb 2025 20:58:22 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoLMZLRLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) has been applied in various fields to realize data sharing, where multiple platforms and multiple Mobile Users (MUs) have appeared recently. However, aiming at mutual selection, the existing works ignore making MUs’ utilities with the limited resources and platforms’ utilities while achieving the desired sensing data quality maximum as far as possible. Thus, they cannot motivate both MUs and platforms to participate. To address this problem, standing on both sides of MUs and platforms with conflicting interests, we propose a Progressive two-stage Auction-based Multi-platform Multi-user Mutual Selection scheme (PAM3S). Specifically, in PAM3S, we treat mutual selection as a two-stage auction and devise the auction models for MU and platform using forward and reverse auction ideas, presenting and maximizing the utilities from their respective perspectives. Then, based on the proposed progressive two-stage auction structure, we adopt 0–1 knapsack and Myerson’s price theory to construct the first stage MU-oriented auction and the second stage platform-oriented auction, achieving devised models. Theoretical analysis shows that PAM3S is economically robust. Extensive experiments on the real dataset demonstrate that PAM3S respectively promotes platforms’ and MUs’ utilities by 76.23% and 10.74 times, compared with the existing works.}
}


@article{DBLP:journals/ton/DaiSYN24,
	author = {Miao Dai and
                  Gang Sun and
                  Hongfang Yu and
                  Dusit Niyato},
	title = {Maximize the Long-Term Average Revenue of Network Slice Provider via
                  Admission Control Among Heterogeneous Slices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {745--760},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3297883},
	doi = {10.1109/TNET.2023.3297883},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DaiSYN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing endows 5G/B5G with differentiated and customized capabilities to cope with the proliferation of diversified services, whereas limited physical network resources may not be able to support all service requests. Slice admission control is regarded as an essential means to ensure service quality and service isolation when the network is under burden. Herein, the scenario where rational tenants coexist with partially competitive network slice providers is adopted. We aim to maximize the long-term average revenue of the network operators through slice admission control, with the feasibility of multidimensional resource requirements, the priority differences among heterogeneous slices, and the admission fairness within each slice taken into account concurrently. We prove the intractability of our problem by a reduction from the Multidimensional Knapsack Problem (MKP), and propose a two-stage algorithm called MPSAC to make a suboptimal solution efficiently. The principle of MPSAC is to split the original problem into two sub-problems; inter-slice decision-making and intra-slice quota allocation, which are solved using a heuristic method and a tailored auction mechanism respectively. Extensive simulations are carried out to demonstrate the efficacy of our algorithm, the results show that the long-term average revenue of ours is at least 9.6% higher than comparisons while maintaining better priority relations and achieving improved fairness performance.}
}


@article{DBLP:journals/ton/TeymooriWH24,
	author = {Peyman Teymoori and
                  Michael Welzl and
                  David A. Hayes},
	title = {{LGCC:} {A} Novel High-Throughput and Low Delay Paradigm Shift in
                  Multi-Hop Congestion Control},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {761--776},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3301291},
	doi = {10.1109/TNET.2023.3301291},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TeymooriWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technological advancements have provided wireless links with very high data rate capacity for 5G/6G mobile networks and WiFi 6, which will be widely deployed by 2025. However, the capacity can have substantial fluctuations, violating the assumption at the transport layer that the capacity is (almost) steady. In this paper, we present a general and efficient, yet deployable solution to this problem through a novel design empowered with a rich theory, allowing a significantly improved experience in using new technologies, especially mobile cellular services. We employ the well-known theory of food-chain models in biology, where a bottleneck link can be modeled as prey, while flows are predators. We extend this model to a chain of predators and preys to form a multi-hop congestion controller, called LGCC. Through simulation evaluation with real-life 5G traces we show the effectiveness of LGCC, compared with the state-of-the-art ABC (Accel-Brake Control). Our results show an order of magnitude bottleneck queuing delay decrease, with only a small decrease in throughput because LGCC tries to never exceed link capacities. LGCC’s design can additionally open a new paradigm in stable multi-hop congestion control and flow aggregation.}
}


@article{DBLP:journals/ton/YanDYYX24,
	author = {Fulong Yan and
                  Xiong Deng and
                  Changshun Yuan and
                  Boyuan Yan and
                  Chongjin Xie},
	title = {On the Performance Investigation of a Recursive Fast Optical Switch-Based
                  High Performance Computing Network Architecture},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {777--790},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3302650},
	doi = {10.1109/TNET.2023.3302650},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YanDYYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel high performance computing (HPC) network architecture \\mathrm {HFOS}_{L}\nbased on L\nparallel levels distributed low radix fast optical switches (FOS). We provide a detailed description of the blade, FOS and the operation of the \\mathrm {HFOS}_{L}\nnetwork. The \\mathrm {HFOS}_{L}\nHPC network is highly scalable, and HFOS4 architecture can support an extremely large HPC network of 65,536 blades under distributed FOS with the same radix of 16 in each level. In principle, the \\mathrm {HFOS}_{L}\nHPC network can be built by FOSes with different radices at each level. To find out the best configuration of FOS at each level and solve the energy and cost optimization problem in \\mathrm {HFOS}_{L}\nnetwork, we break down all the components in the FOS and develop the energy and cost models for the FOS. We verify that the energy and cost per radix functions of FOS are convex functions. Given this foundation, the theoretical investigation of the energy and cost optimization problem shows that the \\mathrm {HFOS}_{L}\nnetwork could achieve the minimum energy and cost only when the FOS radices of all levels in \\mathrm {HFOS}_{L}\nnetwork are the same. Besides, the cost and power consumption of \\mathrm {HFOS}_{L}\nnetworks are compared with a widely used Leaf-Spine network.}
}


@article{DBLP:journals/ton/LiuZXXWH24,
	author = {Jianchun Liu and
                  Qingmin Zeng and
                  Hongli Xu and
                  Yang Xu and
                  Zhiyuan Wang and
                  He Huang},
	title = {Adaptive Block-Wise Regularization and Knowledge Distillation for
                  Enhancing Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {791--805},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3301972},
	doi = {10.1109/TNET.2023.3301972},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuZXXWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a distributed model training framework that allows multiple clients to collaborate on training a global model without disclosing their local data in edge computing (EC) environments. However, FL usually faces statistical heterogeneity (e.g., non-IID data) and system heterogeneity (e.g., computing and communication capabilities), resulting in poor model training performance. To deal with the above two challenges, we propose an efficient FL framework, named FedBR, which integrates the idea of block-wise regularization and knowledge distillation (KD) into the pioneering FL algorithm FedAvg, for resource-constrained edge computing. Specifically, we first divide the model into multiple blocks according to the layer order of deep neural network (DNN). The server only sends some consecutive model blocks instead of an entire model to clients for communication efficiency. Then, the clients make use of knowledge distillation to absorb the knowledge of global model blocks to alleviate statistical heterogeneity during local training. We provide a theoretical convergence guarantee for FedBR and show that the convergence bound will decrease as the increasing number of model blocks sent by the server. Besides, since the increasing number of model blocks brings more computing and communication costs, we design a heuristic algorithm (GMBS) to determine the appropriate number of model blocks for clients according to their varied data distributions, computing, and communication capabilities. Extensive experimental results show that FedBR can reduce the bandwidth consumption by about 31%, and achieve an average accuracy improvement of around 5.6% compared with the baselines under heterogeneous settings.}
}


@article{DBLP:journals/ton/ChenGCLSG24,
	author = {Quan Chen and
                  Song Guo and
                  Zhipeng Cai and
                  Jing Li and
                  Tuo Shi and
                  Hong Gao},
	title = {Peak AoI Minimization at Wireless-Powered Network Edge: From the Perspective
                  of Both Charging and Transmitting},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {806--821},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3303266},
	doi = {10.1109/TNET.2023.3303266},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenGCLSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information, which emerged as a new metric to quantify the freshness of information, has attracted increasing interests recently. To optimize the system AoI, most existing works try to compute an efficient schedule from the point of data transmission. Unfortunately, at wireless-powered network edge, the charging schedule of the source nodes also needs to be decided besides data transmission. Thus, in this paper, we investigate the joint scheduling problem of data transmission and energy replenishment to optimize the maximum peak AoI at network edge with directional chargers. To the best of our knowledge, this is the first work that considers such two problems simultaneously. Firstly, the theoretical bounds of the maximum peak AoI with respect to the charging latency are derived. Secondly, for the minimum peak AoI scheduling problem with a single charger, an optimal scheduling algorithm is proposed to minimize the charging latency, and then a data transmission scheduling strategy is also given to optimize the maximum peak AoI. The proposed algorithm is proved to have a constant approximation ratio of up to 1.5. As for the scenario with multiple chargers, an approximate algorithm is also proposed to minimize the charging latency and the maximum peak AoI. Additionally, when the network bandwidth constraint is considered, the algorithm which considers the parallelism of the charging process and data transmission process is also proposed to reduce the latency and the maximum peak AoI. Finally, the theoretical analysis and simulation results verify that the proposed algorithms have high performance in terms of latency and AoI.}
}


@article{DBLP:journals/ton/ZhangHZZYDG24,
	author = {Yuntian Zhang and
                  Ning Han and
                  Tengteng Zhu and
                  Junjie Zhang and
                  Minghao Ye and
                  Songshi Dou and
                  Zehua Guo},
	title = {Prophet: Traffic Engineering-Centric Traffic Matrix Prediction},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {822--832},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3293098},
	doi = {10.1109/TNET.2023.3293098},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangHZZYDG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic Matrix (TM), which records traffic volumes among network nodes, is important for network operation and management. Due to cost and operation issues, TMs cannot be directly measured and collected in real time. Therefore, many studies work on predicting future TMs based on historical TMs. However, existing works are usually accuracy-centric prediction solutions that mainly focus on improving predicting accuracy of flows’ sizes (i.e., values of elements in TMs) without considering the practical application of TMs. In this paper, we propose a novel TM prediction solution called Prophet for Traffic Engineering (TE), a typical application for TMs which takes TMs as input to optimize routing. We identify that the critical property (i.e., ratio among elements) in a TM plays an important role in TE’s performance. Based on this analysis, we adopt the matrix normalization to maintain the critical property in TMs and customize a TE-centric angle loss function to introduce scale invariance of TMs for capturing the overall relationship error. Different from the element-wise Mean Squared Error (MSE) loss function in accuracy-centric prediction solutions, our proposed TE-centric angle loss function has a clear geometric interpretation, which confines the angle between predicted TM and real TM to zero. Simulation results show that the predicted TMs from Prophet can improve the performance of link-level TE and path-level TE by up to 45.4% and 52.8%, respectively, compared to existing solutions.}
}


@article{DBLP:journals/ton/NguyenT24,
	author = {Truc D. T. Nguyen and
                  My T. Thai},
	title = {Preserving Privacy and Security in Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {833--843},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3302016},
	doi = {10.1109/TNET.2023.3302016},
	timestamp = {Fri, 22 Mar 2024 09:02:02 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NguyenT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning is known to be vulnerable to both security and privacy issues. Existing research has focused either on preventing poisoning attacks from users or on concealing the local model updates from the server, but not both. However, integrating these two lines of research remains a crucial challenge since they often conflict with one another with respect to the threat model. In this work, we develop a principle framework that offers both privacy guarantees for users and detection against poisoning attacks from them. With a new threat model that includes both an honest-but-curious server and malicious users, we first propose a secure aggregation protocol using homomorphic encryption for the server to combine local model updates in a private manner. Then, a zero-knowledge proof protocol is leveraged to shift the task of detecting attacks in the local models from the server to the users. The key observation here is that the server no longer needs access to the local models for attack detection. Therefore, our framework enables the central server to identify poisoned model updates without violating the privacy guarantees of secure aggregation.}
}


@article{DBLP:journals/ton/SaadARM24,
	author = {Muhammad Saad and
                  Afsah Anwar and
                  Srivatsan Ravi and
                  David Mohaisen},
	title = {Revisiting Nakamoto Consensus in Asynchronous Networks: {A} Comprehensive
                  Analysis of Bitcoin Safety and Chain Quality},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {844--858},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3302955},
	doi = {10.1109/TNET.2023.3302955},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SaadARM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Bitcoin blockchain safety relies on strong network synchrony. Therefore, violating the blockchain safety requires strong adversaries that control a mining pool with ≈51% hash rate. In this paper, we show that the network synchrony does not hold in the real world Bitcoin network which can be exploited to feasibly violate the blockchain safety and chain quality. Towards that, first we construct the Bitcoin ideal functionality to formally specify its ideal execution model in a synchronous network. We then develop a large-scale data collection system through which we connect with more than 103K IP addresses of the Bitcoin nodes and identify 871 mining nodes. We contrast the ideal functionality against the real world measurements to expose the network anomalies that can be exploited to optimize the existing attacks. Particularly, we observe a non-uniform block propagation pattern among the mining nodes showing that the Bitcoin network is asynchronous in practice. To realize the threat of an asynchronous network, we present the HashSplit attack that allows an adversary to orchestrate concurrent mining on multiple branches of the blockchain to violate common prefix and chain quality properties. We also propose the attack countermeasures by tweaking Bitcoin Core to model the Bitcoin ideal functionality. Our measurements, theoretical modeling, proposed attack, and countermeasures open new directions in the security evaluation of Bitcoin and similar blockchain systems.}
}


@article{DBLP:journals/ton/SalemCNPA24,
	author = {Tareq Si Salem and
                  Gabriele Castellano and
                  Giovanni Neglia and
                  Fabio Pianese and
                  Andrea Araldo},
	title = {Toward Inference Delivery Networks: Distributing Machine Learning
                  With Optimality Guarantees},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {859--873},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3305922},
	doi = {10.1109/TNET.2023.3305922},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SalemCNPA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An increasing number of applications rely on complex inference tasks that are based on machine learning (ML). Currently, there are two options to run such tasks: either they are served directly by the end device (e.g., smartphones, IoT equipment, smart vehicles), or offloaded to a remote cloud. Both options may be unsatisfactory for many applications: local models may have inadequate accuracy, while the cloud may fail to meet delay constraints. In this paper, we present the novel idea of inference delivery networks (IDNs), networks of computing nodes that coordinate to satisfy ML inference requests achieving the best trade-off between latency and accuracy. IDNs bridge the dichotomy between device and cloud execution by integrating inference delivery at the various tiers of the infrastructure continuum (access, edge, regional data center, cloud). We propose a distributed dynamic policy for ML model allocation in an IDN by which each node dynamically updates its local set of inference models based on requests observed during the recent past plus limited information exchange with its neighboring nodes. Our policy offers strong performance guarantees in an adversarial setting and shows improvements over greedy heuristics with similar complexity in realistic scenarios.}
}


@article{DBLP:journals/ton/WangYZFC24,
	author = {Guijuan Wang and
                  Jiguo Yu and
                  Yifei Zou and
                  Jianxi Fan and
                  Wei Cheng},
	title = {A New Measure of Fault-Tolerance for Network Reliability: Double-Structure
                  Connectivity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {874--889},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3305611},
	doi = {10.1109/TNET.2023.3305611},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangYZFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most data center services are finished by the cooperation among the connected servers. However, the malicious attackers always try to divide the network into disconnected components to start some attacks, such as the address resolution protocol (ARP) attack, the denial of service (DoS) attack, the botnet attack, and so on. The connectivity is an excellent indicator to measure the reliability and fault-tolerant ability of the network. Whereas, the traditional connectivity and current conditional connectivity cannot well reflect the fault-tolerant performance of the network when attackers are a block or have a certain structure and the components of the remaining network still have a certain structure. Based on this fact, we propose a new measure: the double-structure connectivity, which can accurately reflect the fault-tolerant ability of the network when attackers are structured and each component of the network has a certain structure after removing the attacked servers. Meanwhile, a hypercube is a high-performance interconnection network that can also be used to design some data center networks. Therefore, we study the double-structure fault-tolerance of the hypercube and determine the double-structure connectivity of distinct structures of the hypercube. Furthermore, we propose algorithms to construct structures of attackers directly to measure the fault-tolerant ability of the hypercube under this attack. Our results can be applied not only to interconnection networks but also to some data center networks.}
}


@article{DBLP:journals/ton/XuWXLDXWGD24,
	author = {Wenzheng Xu and
                  Chengxi Wang and
                  Hongbin Xie and
                  Weifa Liang and
                  Haipeng Dai and
                  Zichuan Xu and
                  Ziming Wang and
                  Bing Guo and
                  Sajal K. Das},
	title = {Reward Maximization for Disaster Zone Monitoring With Heterogeneous
                  UAVs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {890--903},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3300174},
	doi = {10.1109/TNET.2023.3300174},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuWXLDXWGD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the deployment of K heterogeneous UAVs to monitor Points of Interest (PoIs) in a disaster zone, where a PoI may represent a school building or an office building, in which people are trapped. A UAV can take images/videos of PoIs and send its collected information back to a nearby rescue station for decision-making. Unlike most existing studies that focused on only homogeneous UAVs, we here study the scheduling of K heterogeneous UAVs, where different UAVs have different energy capacities and functionalities that lead to different monitoring qualities (monitoring rewards) of each PoI. For example, one type of UAVs can take only visual images while the other type of UAVs can take both visual and thermal infrared images. In this paper, we investigate a problem of scheduling K heterogeneous UAVs to monitor PoIs so that the sum of monitoring rewards received by all UAVs is maximized, subject to energy capacity on each UAV. We propose the very first \\frac {1}{3} -approximation algorithm for this scheduling problem. We also evaluate the performance of the proposed algorithm, using real parameters of commercial UAVs. Experimental results show that the performance of the proposed algorithm is promising, which is improved by 25%, compared with existing algorithms.}
}


@article{DBLP:journals/ton/LiaoXXYWQ24,
	author = {Yunming Liao and
                  Yang Xu and
                  Hongli Xu and
                  Zhiwei Yao and
                  Lun Wang and
                  Chunming Qiao},
	title = {Accelerating Federated Learning With Data and Model Parallelism in
                  Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {1},
	pages = {904--918},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3299851},
	doi = {10.1109/TNET.2023.3299851},
	timestamp = {Thu, 29 Feb 2024 20:53:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoXXYWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, edge AI has been launched to mine and discover valuable knowledge at network edge. Federated Learning, as an emerging technique for edge AI, has been widely deployed to collaboratively train models on many end devices in data-parallel fashion. To alleviate the computation/communication burden on the resource-constrained workers (e.g., end devices) and protect user privacy, Spilt Federated Learning (SFL), which integrates both data parallelism and model parallelism in Edge Computing (EC), is becoming a practical and popular approach for model training over distributed data. However, apart from the resource limitation, SFL still faces two other critical challenges in EC, i.e., system heterogeneity and context dynamics. To overcome these challenges, we present an efficient SFL method, named AdaSFL, which controls both local updating frequency and batch size to better accelerate model training. We theoretically analyze the model convergence rate and obtain a convergence upper bound regarding local updating frequency given a fixed batch size. Upon this, we develop a control algorithm to determine adaptive local updating frequency and diverse batch sizes for heterogeneous workers to enhance the training efficiency. The experimental results show that AdaSFL can reduce the completion time by about 43% and the network traffic consumption by about 31% for achieving the similar test accuracy, compared to the baselines.}
}


@article{DBLP:journals/ton/AyanHEK24,
	author = {Onur Ayan and
                  Sandra Hirche and
                  Anthony Ephremides and
                  Wolfgang Kellerer},
	title = {Optimal Finite Horizon Scheduling of Wireless Networked Control Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {927--942},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3300949},
	doi = {10.1109/TNET.2023.3300949},
	timestamp = {Sat, 08 Jun 2024 13:14:29 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AyanHEK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Control over networks is envisioned to be one of the driving applications of future mobile networks. Networked control systems contain sensors and controllers exchanging time-sensitive information to fulfill a particular control goal. In this work, we consider N\nheterogeneous feedback control loops closed over a wireless star network. A centralized scheduler located at the central node, i.e., base station (BS), determines the transmission schedule of sensor-to-BS and BS-to-controller communication links. We assume that each link can accommodate a single transmission at a time and is prone to data losses with time-varying probability. Moreover, each controller estimates the system state remotely based on available information. In such a setting, we formulate an optimization problem to minimize the network-induced estimation error at the controller. In particular, we determine the optimal transmission schedule on each link that leads to the minimum normalized mean squared error (nMSE) in a given finite horizon (FH). We compare the performance of our proposed FH scheduler to various schedulers from the existing literature. Our simulation results show that by solving the finite horizon problem optimally, we are able to reduce the nMSE by 10% when compared to the best performing scheduling policy among the selected policies from the state-of-the-art. Moreover, the linear-quadratic Gaussian (LQG) cost is reduced by more than 13% indicating a control performance improvement in the network.}
}


@article{DBLP:journals/ton/PanWLYG24,
	author = {Qianqian Pan and
                  Jun Wu and
                  Jianhua Li and
                  Wu Yang and
                  Mohsen Guizani},
	title = {Blockchain and Multi-Agent Learning Empowered Incentive {IRS} Resource
                  Scheduling for Intelligent Reconfigurable Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {943--958},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3309729},
	doi = {10.1109/TNET.2023.3309729},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PanWLYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising technology, intelligent reflecting surface (IRS) enables future communications and networks to realize programmable data transmissions. Due to the untrustworthiness of the communication environment and the selfishness of wireless devices, secure and intelligent IRS resource management is still an open issue. In this paper, we aim to implement IRS resource scheduling with properties of security, intelligence, efficiency, and fairness. To realize the above goals, we propose the blockchain and multi-agent learning empowered incentive scheduling system for tamper-proof and undeniable IRS resource management. To overcome the low throughout and intensive computation issues of blockchain, we devise a hybrid framework combining traditional Satoshi-style and directed acyclic graph blockchain for IRS resource scheduling. Due to the storage limitation of wireless devices, an intelligent blockchain storage reduction mechanism is proposed, where a multi-dimensional multi-hierarchy feature-based scheme is designed to determine block storage priority. Based on this storage priority and device states, the selection of storage-reduction devices is formulated as a cooperative multi-agent decision problem. Then, a multi-agent deep reinforcement learning-driven scheme is proposed to determine reduction strategies. To facilitate IRS providers/subscribers participating in the proposed system and maintain the efficiency of resource scheduling, an auction-based incentive mechanism is devised. In this mechanism, we propose the IRS resource allocation scheme and the payment scheme to achieve economic robustness and high efficiency. Finally, security analysis and experiment analysis indicate the feasibility and effectiveness of the proposed IRS resource scheduling in intelligent reconfigurable networks.}
}


@article{DBLP:journals/ton/ReviriegoAAED24,
	author = {Pedro Reviriego and
                  Jim Apple and
                  {\'{A}}lvaro Alonso and
                  Otmar Ertl and
                  Niv Dayan},
	title = {Cardinality Estimation Adaptive Cuckoo Filters {(CE-ACF):} Approximate
                  Membership Check and Distinct Query Count for High-Speed Network Monitoring},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {959--970},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3302306},
	doi = {10.1109/TNET.2023.3302306},
	timestamp = {Sun, 19 Jan 2025 13:55:30 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ReviriegoAAED24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In network monitoring applications, it is often beneficial to employ a fast approximate set-membership filter to check if a given packet belongs to a monitored flow. Recent adaptive filter designs, such as the Adaptive Cuckoo Filter, are especially promising for such use cases as they adapt fingerprints to eliminate recurring false positives. In many traffic monitoring applications, it is also of interest to know the number of distinct flows that traverse a link or the number of nodes that are sending traffic. This is commonly done using cardinality estimation sketches. Therefore, on a given switch or network device, the same packets are typically processed using both a filter and a cardinality estimator. Having to process each packet with two independent data structures adds complexity to the implementation and limits performance. This paper shows that adaptive cuckoo filters can also be used to estimate the number of distinct negative elements queried on the filter. In flow monitoring, those distinct queries correspond to distinct flows. This is interesting as we get the cardinality estimation for free as part of the normal adaptive filter’s operation. We provide (1) a theoretical analysis, (2) simulation results, and (3) an evaluation with real packet traces to show that adaptive cuckoo filters can accurately estimate a wide range of cardinalities in practical scenarios.}
}


@article{DBLP:journals/ton/LiDSWZSPH24,
	author = {Hao Li and
                  Yihan Dang and
                  Guangda Sun and
                  Changhao Wu and
                  Peng Zhang and
                  Danfeng Shan and
                  Tian Pan and
                  Chengchen Hu},
	title = {Programming Network Stack for Physical Middleboxes and Virtualized
                  Network Functions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {971--986},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3307641},
	doi = {10.1109/TNET.2023.3307641},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiDSWZSPH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Middleboxes are becoming indispensable in modern networks. However, programming the network stack of middleboxes to support emerging transport protocols and flexible stack hierarchy is still a daunting task. To this end, we propose Rubik, a language that greatly facilitates the task of middlebox stack programming. Different from existing hand-written approaches, Rubik offers various high-level constructs for relieving the operators from dealing with massive native code, so that they can focus on specifying their processing intents. We show that using Rubik one can program the middlebox stack with minor effort, e.g., 250 lines of code for a complete TCP/IP stack, which is a reduction of 2 orders of magnitude compared to the hand-written versions. To maintain a high performance, we conduct extensive optimizations at the middle- and back-end of the compiler. Experiments show that the stacks generated by Rubik outperform the mature hand-written stacks by at least 30% in throughput.}
}


@article{DBLP:journals/ton/LiP24,
	author = {Weihe Li and
                  Paul Patras},
	title = {P-Sketch: {A} Fast and Accurate Sketch for Persistent Item Lookup},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {987--1002},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3306897},
	doi = {10.1109/TNET.2023.3306897},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In large data streams consisting of sequences of data items, those appearing over a long period of time are regarded as persistent. Compared with frequent items, persistent items do not necessarily hold large amounts of data and thus may hamper the effectiveness of vanilla volume-based detectors. Identifying persistent items plays a crucial role in a range of areas such as fraud detection and network management. Fast detection of persistent items in massive streams is however challenging due to the inherently high data rates, while state-of-the-art persistent item lookup solutions routinely require large enough memory to attain high accuracy, which questions the feasibility of deploying them in practice. In this paper, we introduce P-Sketch, a novel approach to persistent item lookup that achieves high accuracy even with small memory (L1 Cache) budgets and maintains high update speed across different settings. Specifically, we introduce the concept of arrival continuity (hotness) that counts the number of consecutive windows in which an item appears, to effectively protect persistent items from being wrongly replaced by non-persistent ones. Through meticulous data analysis, we also reveal that items with higher persistence tend to possess a stronger hotness than non-persistent ones. Thus, we harness the information of persistence and hotness, and employ a probability-based replacement strategy to achieve a good balance between memory efficiency, lookup accuracy, and update speed. We also present a theoretical analysis of the performance of the proposed P-Sketch. Through trace-driven emulations, we demonstrate that our P-Sketch yields average F1 score and update throughput gains of up to 10.32\\times and respectively 2.9\\times , over existing schemes. Lastly, we show how to further boost the P-Sketch’s update speed with Single Instruction Multiple Data (SIMD) instructions.}
}


@article{DBLP:journals/ton/LiuALGYXL24,
	author = {Chenyi Liu and
                  Vaneet Aggarwal and
                  Tian Lan and
                  Nan Geng and
                  Yuan Yang and
                  Mingwei Xu and
                  Qing Li},
	title = {{FERN:} Leveraging Graph Attention Networks for Failure Evaluation
                  and Robust Network Design},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1003--1018},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3311678},
	doi = {10.1109/TNET.2023.3311678},
	timestamp = {Thu, 05 Dec 2024 20:56:40 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuALGYXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Robust network design, which aims to guarantee network availability under various failure scenarios while optimizing performance/cost objectives, has received significant attention. Existing approaches often rely on model-based mixed-integer optimization that is hard to scale or employ deep learning to solve specific engineering problems yet with limited generalizability. In this paper, we show that failure evaluation provides a common kernel to improve the tractability and scalability of existing solutions. By providing a neural network function approximation of this common kernel using graph attention networks, we develop a unified learning-based framework, FERN, for scalable Failure Evaluation and Robust Network design. FERN represents rich problem inputs as a graph and captures both local and global views by attentively performing feature extraction from the graph. It enables a broad range of robust network design problems, including robust network validation, network upgrade optimization, and fault-tolerant traffic engineering that are discussed in this paper, to be recasted with respect to the common kernel and thus computed efficiently using neural networks and over a small set of critical failure scenarios. Extensive experiments on real-world network topologies show that FERN can efficiently and accurately identify key failure scenarios for both OSPF and optimal routing scheme, and generalizes well to different topologies and input traffic patterns. It can speed up multiple robust network design problems by more than 80x, 200x, 10x, respectively with negligible performance gap.}
}


@article{DBLP:journals/ton/QuMLSLXLLFG24,
	author = {Jian Qu and
                  Xiaobo Ma and
                  Wenmao Liu and
                  Hongqing Sang and
                  Jianfeng Li and
                  Lei Xue and
                  Xiapu Luo and
                  Zhenhua Li and
                  Li Feng and
                  Xiaohong Guan},
	title = {On Smartly Scanning of the Internet of Things},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1019--1034},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3312162},
	doi = {10.1109/TNET.2023.3312162},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QuMLSLXLLFG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber search engines, such as Shodan and Censys, have gained popularity due to their strong capability of indexing the Internet of Things (IoT). They actively scan and fingerprint IoT devices for unearthing IP-device mapping. Because of the large address space of the Internet and the mapping’s mutative nature, efficiently tracking the evolution of IP-device mapping with a limited budget of scans is essential for building timely cyber search engines. An intuitive solution is to use reinforcement learning to schedule more scans to networks with high churn rates of IP-device mapping. However, such an intuitive solution has never been systematically studied. In this paper, we take the first step toward demystifying this problem based on our experiences in maintaining a global IoT scanning platform. Inspired by the measurement study of large-scale real-world IoT scan records, we land reinforcement learning onto a system capable of smartly scanning IoT devices in a principled way. We disclose key parameters affecting the effectiveness of different scanning strategies, and real-world experiments demonstrate that our system can scan up to around 40 times as many IP-device mapping mutations as random/sequential scanning.}
}


@article{DBLP:journals/ton/DiamantiPTP24,
	author = {Maria Diamanti and
                  Christos Pelekis and
                  Eirini{-}Eleni Tsiropoulou and
                  Symeon Papavassiliou},
	title = {Delay Minimization for Rate-Splitting Multiple Access-Based Multi-Server
                  {MEC} Offloading},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1035--1047},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3311131},
	doi = {10.1109/TNET.2023.3311131},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DiamantiPTP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate-Splitting Multiple Access (RSMA) has been recently recognized as a more general multiple access technique that overcomes the limiting factors of its predecessors related to the signal decoding complexity and interference management tradeoff. In this paper, we investigate the application of the RSMA technique to facilitate the users’ concurrent offloading to multiple servers in a multi-server Multi-Access Edge Computing (MEC) system. Each user fully offloads different parts of its computation task at the available MEC servers (or a combination of them) using the same frequency band. We aim to minimize the sum of users’ maximum experienced delay among the different MEC servers, stemming from both the offloading and processing, by jointly optimizing their computation task assignment ratios to the servers, their allocated common-message rates, common and private-message transmission powers, and computing resources related to each server. The formulated min-max-sum problem is non-convex, and its optimization variables are highly coupled. By examining its structure, we equivalently transform the problem and further decompose it into two independent sub-problems that separately provide solutions to the radio and computing resource allocation problems. Numerical results show the effectiveness of the proposed solution in terms of the users’ experienced delay and the proposed algorithm’s real execution time.}
}


@article{DBLP:journals/ton/EsmatL24,
	author = {Haitham H. Esmat and
                  Beatriz Lorenzo},
	title = {Self-Learning Multi-Mode Slicing Mechanism for Dynamic Network Architectures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1048--1063},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3305975},
	doi = {10.1109/TNET.2023.3305975},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/EsmatL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic network architectures that utilize communication, computing, and storage resources at the wireless edge are key to delivering emerging services in next-generation networks (e.g., AR/VR, 3D video, intelligent cars, etc). Network slicing can be significantly enhanced by including dynamically available resources throughout the fog/edge/cloud continuum and using mmWave/THz bands. However, network slicing of dynamic multi-tier computing networks remains under-explored. In this paper, we present a self-learning end-to-end network slicing mechanism (SELF-E2E-NS) that facilitates collaboration between the Infrastructure Provider (InP) and tenants to slice their subscribers’ resources (i.e., radio, computing, and storage) as fog resources. To adapt to the uncertain availability of resources at the edge and minimize the risk of non-satisfying service level agreements (SLAs), our slicing mechanism has two operational modes. Operational mode 1 is for joint network slicing (JNS) in which the InP infrastructure is augmented with fog resources and jointly sliced to meet high throughput and delay tolerant requirements. Operational mode 2 is for independent network slicing (INS) in which the InP infrastructure and fog resources are sliced separately to achieve high throughput, low-latency, and high-reliability requirements. Our schemes leverage mmWave/THz, fog/edge/cloud computing, and caching to achieve new service requirements. We design a DQ-E2E-JNS algorithm that uses Deep Dueling network and a MAAC-E2E-INS algorithm based on multi-agent actor-critic, which incorporate service-aware pricing feedback and fog trading matching, respectively. These algorithms find the optimal slice request admission and collaboration policy that maximizes the long-term revenue of the InP and tenants for each mode. The simulation results show that our novel slicing mechanism can serve up to 4 times more requests and effectively exploits different spectrum bands and fog resources to improve revenue and performance.}
}


@article{DBLP:journals/ton/LiuNLM24,
	author = {Bai Liu and
                  Quang Minh Nguyen and
                  Qingkai Liang and
                  Eytan H. Modiano},
	title = {Tracking Drift-Plus-Penalty: Utility Maximization for Partially Observable
                  and Controllable Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1064--1079},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3307684},
	doi = {10.1109/TNET.2023.3307684},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuNLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stochastic network models with all components being observable and controllable have been the focus of classic network optimization theory for decades. However, in modern network systems, it is common that the network controller can only observe and operate on some nodes (i.e., overlay nodes), and the other nodes (i.e., underlay nodes) are neither observable nor controllable. Moreover, the dynamics can be non-stochastic or even adversarial. In this paper, we focus on the network utility maximization (NUM) problem for networks with overlay-underlay structures. The network dynamics, such as packet admissions, external arrivals and control actions of underlay nodes, can be stochastic, non-stochastic or even adversarial. We propose the Tracking Drift-plus-Penalty (TDP*) algorithm that only operates on the overlay nodes and does not require direct observations of the underlay nodes, and analyze the tradeoffs between the average utility and queue backlog. We show that as long as the peak queue backlog of the network is sublinear in time horizon, TDP* can solve the NUM problem, i.e., reaching the maximum utility while preserving stability.}
}


@article{DBLP:journals/ton/DasPGCPS24,
	author = {Soumadeep Das and
                  Aryan Mohammadi Pasikhani and
                  Prosanta Gope and
                  John A. Clark and
                  Chintan Patel and
                  Biplab Sikdar},
	title = {{AIDPS:} Adaptive Intrusion Detection and Prevention System for Underwater
                  Acoustic Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1080--1095},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3313156},
	doi = {10.1109/TNET.2023.3313156},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DasPGCPS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for underwater environments and find applications in many areas. However, a lack of security considerations, the unstable and challenging nature of the underwater environment, and the resource-constrained nature of the sensor nodes used for UW-ASNs (which makes them incapable of adopting security primitives) make the UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The proposed AIDPS can improve the security of the UW-ASNs so that they can efficiently detect underwater-related attacks (e.g., blackhole, grayhole and flooding attacks). To determine the most effective configuration of the proposed construction, we conduct a number of experiments using several state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest (ARF), light gradient-boosting machine, and K-nearest neighbours) and concept drift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our experimental results show that incremental ARF using ADWIN provides optimal performance when implemented with One-class support vector machine (SVM) anomaly-based detectors. Furthermore, our extensive evaluation results also show that the proposed scheme outperforms state-of-the-art bench-marking methods while providing a wider range of desirable features such as scalability and complexity.}
}


@article{DBLP:journals/ton/EtezadiNTWF24,
	author = {Ehsan Etezadi and
                  Carlos Natalino and
                  Christine Tremblay and
                  Lena Wosinska and
                  Marija Furdek},
	title = {Programmable Filterless Optical Networks: Architecture, Design, and
                  Resource Allocation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1096--1109},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3319746},
	doi = {10.1109/TNET.2023.3319746},
	timestamp = {Sun, 19 Jan 2025 13:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/EtezadiNTWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Filterless optical networks (FONs) are a cost-effective optical networking technology that replaces reconfigurable optical add-drop multiplexers, used in conventional, wavelength-switched optical networks (WSONs), by passive optical splitters and couplers. FONs follow the drop-and-waste transmission scheme, i.e., broadcast signals without filtering, which generates spectrum waste. Programmable filterless optical networks (PFONs) reduce this waste by equipping network nodes with programmable optical white box switches that support arbitrary interconnections of passive elements. Cost-efficient PFON solutions require optimal routing, modulation format and spectrum assignment (RMSA) to connection requests, as well as optimal design of the node architecture. This paper presents an optimization framework for PFONs. We formulate the RMSA problem in PFONs as a single-step integer linear program (ILP) that jointly minimizes the total spectrum and optical component usage. As RMSA is an NP-complete problem, we propose a two-step ILP formulation that addresses the RMSA sub-problems separately and seeks sub-optimal solutions to larger problem instances in acceptable time. Simulation results indicate a beneficial trade-off between component usage and spectrum consumption in proposed PFON solutions. They use up to 64% less spectrum than FONs, up to 84% fewer active switching elements than WSONs, and up to 81% fewer optical amplifiers at network nodes than FONs or WSONs.}
}


@article{DBLP:journals/ton/WangCHYDQ24,
	author = {Mei Wang and
                  Jing Chen and
                  Kun He and
                  Ruozhou Yu and
                  Ruiying Du and
                  Zhihao Qian},
	title = {UFinAKA: Fingerprint-Based Authentication and Key Agreement With Updatable
                  Blind Credentials},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1110--1123},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3311130},
	doi = {10.1109/TNET.2023.3311130},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangCHYDQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authentication and key agreement are two basic functionalities to guarantee secure network communications, which are naturally integrated as an Authentication and Key Agreement (AKA) protocol. AKAs usually either need a dedicated device to store a cryptographic key or require the user to remember a password. In recent years, AKAs built on biometrics, e.g., human fingerprints, have gained research attention since they avoid these issues. Unlike keys or passwords that can be updated, biometrics are at greater risk that cannot be reused once disclosed. However, existing mechanisms either explicitly expose the biometrics to the server or consume a massive amount of resources. This paper proposes UFinAKA, a privacy-preserving fingerprint-based authentication and key agreement system with updatable blind credentials. UFinAKA explores a fingerprint-based blind credential authentication scheme as a building block such that the server has no access to the fingerprint data hidden within the credential. Furthermore, UFinAKA provides an updatable fingerprint-based credentials AKA protocol, which allows the server to update the blind credentials and guarantees anonymous fingerprint authentication to mitigate further leakage when the server is corrupted. We perform security analysis and experimental evaluation on UFinAKA. The evaluation results show that UFinAKA requires only linear computation overhead for the client, a single round of interaction, and roughly linear computation and storage cost for the server. The running time of UFinAKA is at least 4 times faster than the state-of-the-art solutions, and the storage cost of these solutions is at least 100 times more than UFinAKA.}
}


@article{DBLP:journals/ton/YangWHW24,
	author = {Xiaoxue Yang and
                  Hao Wang and
                  Bing Hu and
                  Chunming Wu},
	title = {{ABOI:} AWGR-Based Optical Interconnects for Single-Wavelength and
                  Multi-Wavelength},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1124--1139},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3314096},
	doi = {10.1109/TNET.2023.3314096},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangWHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optical interconnect can achieve a substantial increase in the number of nodes and switching capability for data centers, by virtue of their low power consumption and high bandwidth. In this paper, we propose a single-wavelength switch architecture based on two-stage AWGR for data centers. Then the single-wavelength design is extended to support multi-wavelength, which has higher scalability but lower hardware complexity and power consumption. We prove that the single wavelength architecture is internal strictly non-blocking. When the incoming traffic is externally blocked, a buffer allocation algorithm is designed to allocate feedforward and feedback FDL for the blocked packets. Based on the simulation results, the proposed switch can provide higher throughput, lower average latency, and a 0 out-of-order ratio compared to existing solutions.}
}


@article{DBLP:journals/ton/NiZW24,
	author = {Peikun Ni and
                  Jianming Zhu and
                  Guoqing Wang},
	title = {Activity-Oriented Production Promotion Utility Maximization in Metaverse
                  Social Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1140--1154},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3309624},
	doi = {10.1109/TNET.2023.3309624},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NiZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The continuous development of network media technology has driven the unceasing change in the online social environment, from PC social to mobile social, which are currently experiencing a new change: Metaverse social. The iteration of the social environment gives impetus to the uninterrupted upgrading of social patterns, which leads to the ceaseless innovation of the product promotion model. In this article, motivated by the characteristics of the Metaverse social, we devise virtual activity-oriented product promotion tactics. We propose an activity-oriented promotion utility maximization problem and tackle it systematically. We demonstrate the complexity and inapproximability of the problem. A continuity approximate concave relaxation method is devised to optimize the set function with a supermodularity ratio. We develop an approximate projected subgradient procedure to obtain the solution with an approximate factor guarantee. Experiments on two types (traditional social network and Metaverse social network) of real-world datasets verify the feasibility and scalability of our algorithm and model, and the research results have guiding significance for the online promotion of products.}
}


@article{DBLP:journals/ton/JiaoWHXWFC24,
	author = {Wenli Jiao and
                  Ju Wang and
                  Yelu He and
                  Xiangdong Xi and
                  Fuwei Wang and
                  Dingyi Fang and
                  Xiaojiang Chen},
	title = {Eliminating Design Effort: {A} Reconfigurable Sensing Framework for
                  Chipless, Backscatter Tags},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1155--1170},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3320263},
	doi = {10.1109/TNET.2023.3320263},
	timestamp = {Tue, 07 May 2024 20:25:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JiaoWHXWFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter tag based sensing has received a lot of attention recently due to the battery-free, low-cost, and widespread use of backscatter tags, e.g., RFIDs. Despite that, they suffer from an extensive, costly, and time-consuming redesign effort when there are changes in application requirements, such as changes in sensing targets or working frequency bands. This paper introduces a reconfigurable sensing framework, which enables us to easily reconfigure the design parameters of chipless backscatter tags for sensing different targets or working with different frequency bands, without the need for onerous design effort. To realize this vision, we capture the relationship between the application requirements and the sensing tag’s design parameters. This relationship enables us to fast and efficiently reconfigure/change an existing sensing tag design to meet new application requirements. Real-world experiments show that, by using our reconfigurable framework to flexibly redesign a tag’s parameters, the sensing tag achieves more than 92.1% accuracy for sensing four different applications and working on four different frequency bands.}
}


@article{DBLP:journals/ton/LiZCCXL24,
	author = {Yu Li and
                  Jiaheng Zhang and
                  Junjie Chen and
                  Yicong Chen and
                  Ning Xie and
                  Hongbin Li},
	title = {Privacy-Preserving Physical-Layer Authentication Under Cooperative
                  Attacks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1171--1186},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3311470},
	doi = {10.1109/TNET.2023.3311470},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiZCCXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we are concerned about the problem of guaranteeing both privacy and security in a Location-Based Service (LBS) system, where a challenging scenario involving cooperative attack is considered. Since prior Physical-Layer Authentication (PLA) schemes do not consider cooperative attack, their security significantly declines under such attacks. We propose two privacy-preserving PLA schemes: the Privacy-Preserving Physical-Layer Authentication using Noise Variance (PPPLA-NV) scheme and the Privacy-Preserving Physical-Layer Authentication using Multiple Channel Responses (PPPLA-MCR) scheme, which significantly improve the privacy-preserving performance under a cooperative attack. Note that the proposed schemes protect not only user’s identity information but also data message. We theoretically analyze the performance of the proposed schemes, derive their closed-form expressions, and provide a theoretical comparison between both proposed schemes. We implement the proposed schemes and conduct extensive performance comparisons through simulations. Experimental results show a perfect match between the theoretical and simulation results. From the experimental results, we observe that if the overhead is not the priority, the PPPLA-MCR scheme is the best option; otherwise, the PPPLA-NV scheme may be a better option.}
}


@article{DBLP:journals/ton/GuoSLCCH24,
	author = {Jiani Guo and
                  Shanshan Song and
                  Jun Liu and
                  Hao Chen and
                  Jun{-}Hong Cui and
                  Guangjie Han},
	title = {A Hybrid NOMA-Based {MAC} Protocol for Underwater Acoustic Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1187--1200},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3311682},
	doi = {10.1109/TNET.2023.3311682},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoSLCCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing a high-capacity Medium Access Control (MAC) protocol suffers from low bandwidth and long propagation delay in Underwater Acoustic Networks (UANs). Non-Orthogonal Multiple Access (NOMA) is a promising technology to assist MAC protocols in overcoming the above restrictions and improving UANs’ capacity. It enables multiple users to access the same frequency-time resource based on power or code differences of user classes. However, UANs lack MAC research employing NOMA’s physical advantages. Most existing NOMA-based MAC protocols are designed for terrestrial networks, which are inapplicable to UANs. In classifying users, they ignore the effects of harsh marine environments on acoustic channels and fail to decrease channels’ interference, resulting in conflicting communications. Moreover, such unreasonable classification results further affect resource allocation, leading to low transmission rate and high energy consumption in UANs. In this paper, we propose a Hybrid NOMA-based MAC protocol (HN-MAC) to achieve efficient concurrent communication for UANs. Specifically, HN-MAC combines power-domain and code-domain NOMA to classify users and allocate communication resources. For the user classification, we propose an Adaptive Clustering Algorithm (ACA), which dynamically determines the clusters’ number and classifies users based on channel gain and channel correlation under multipath conditions. In this way, HN-MAC decreases interference among multiple users in various ocean scenarios. During the resource allocation, we formulate a joint allocation problem of transmission power and codebook based on the clustering result to optimize transmission rate and energy consumption. Further, a genetic algorithm is proposed to solve the allocation problem by considering resource constraints. Simulation results show that HN-MAC provides more stable concurrent communications with less resource consumption than the state-of-the-art protocols in various UANs.}
}


@article{DBLP:journals/ton/FaisalMSM24,
	author = {Tooba Faisal and
                  Damiano Di Francesco Maesa and
                  Nishanth Sastry and
                  Simone Mangiante},
	title = {{JITRA:} Just-In-Time Resource Allocation Through the Distributed
                  Ledgers for 5G and Beyond},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1201--1211},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3318239},
	doi = {10.1109/TNET.2023.3318239},
	timestamp = {Sun, 19 Jan 2025 13:55:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FaisalMSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For the viability of future “mission-critical” applications such as remote surgery and connected cars, the customers must trust the network connection and operators must adhere to the Service Level Agreement (SLA). The key to enabling trust between the customer and the operator is the transparency and accountability of the SLA. That is, the operators ensure the transparent and appropriate execution of SLA clauses (e.g., Quality of Service (QoS) and compensations if the SLA is violated). In this work, we argue that today’s network is highly volatile. Therefore, it is convenient for the operators to provide service guarantees for short-term rather than traditional long-term methods. Consequently, we advocate short-term and dynamic service contracts considering spatial and temporal characteristics rather than typical long-term contracts. We propose a Distributed Ledger Technology (DLT)-focused end-to-end transparent, accountable and automated resource provisioning system architecture in which are installed as smart contracts. In our architecture, resources can be requested and allocated dynamically and automatically with Quality-of-Service (QoS) monitoring. Our architecture is scalable through a side-channel based QoS monitoring protocol that guarantees data integrity and minimises the Permissioned Distributed Ledgers (PDL) updates. To measure the viability of our proposal, we first evaluate resource provisioning in the context of network slicing. Then we assess the DLT performance for smart contract execution, in both permissioned and permission-less settings. In the end, we evaluate the monitoring tools and compare and contrast sketches and bloomfilters, and justify our choice of bloomfilters.}
}


@article{DBLP:journals/ton/WangLDBA24,
	author = {Juncheng Wang and
                  Ben Liang and
                  Min Dong and
                  Gary Boudreau and
                  Hatem Abou{-}Zeid},
	title = {Joint Online Optimization of Model Training and Analog Aggregation
                  for Wireless Edge Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1212--1228},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3318474},
	doi = {10.1109/TNET.2023.3318474},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangLDBA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider federated learning in a wireless edge network, where multiple power-limited mobile devices collaboratively train a global model, using their local data with the assistance of an edge server. Exploiting over-the-air computation, the edge server updates the global model via analog aggregation of the local models over noisy wireless fading channels. Unlike existing works that separately optimize computation and communication at each step of the learning algorithm, in this work, we jointly optimize the training of the global model and the analog aggregation of the local models over time. Our objective is to minimize the accumulated training loss at the edge server, subject to individual long-term transmit power constraints at the mobile devices. We propose an efficient algorithm, termed Online Model Updating with Analog Aggregation (OMUAA), to adaptively update the local and global models based on the time-varying communication environment. The trained model of OMUAA is channel- and power-aware, and it is in closed form incurring low computational complexity. We study the mutual impact between model training and analog aggregation over time, to derive performance bounds on the computation and communication performance metrics. Furthermore, we consider a variant of OMUAA with double regularization on both the local and global models, termed OMUAA-DR, and show that it can significantly reduce the convergence time to reach long-term transmit power constraints. In addition, we extend both OMUAA and OMUAA-DR to enable analog gradient aggregation, while preserving their performance bounds. Simulation results based on real-world image classification datasets and typical wireless network settings demonstrate substantial performance gain of OMUAA and OMUAA-DR over the known best alternatives.}
}


@article{DBLP:journals/ton/WangXCTLLCHHXP24,
	author = {Sihan Wang and
                  Tian Xie and
                  Min{-}Yue Chen and
                  Guan{-}Hua Tu and
                  Chi{-}Yu Li and
                  Xinyu Lei and
                  Po{-}Yi Chou and
                  Fu{-}Cheng Hsieh and
                  Yiwen Hu and
                  Li Xiao and
                  Chunyi Peng},
	title = {Dissecting Operational Cellular IoT Service Security: Attacks and
                  Defenses},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1229--1244},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3313557},
	doi = {10.1109/TNET.2023.3313557},
	timestamp = {Mon, 13 Jan 2025 11:14:42 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangXCTLLCHHXP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {More than 150 cellular networks worldwide have rolled out LTE-M (LTE-Machine Type Communication) and/or NB-IoT (Narrow Band Internet of Things) technologies to support massive IoT services such as smart metering and environmental monitoring. Such cellular IoT services share the existing cellular network architecture with non-IoT (e.g., smartphone) ones. When they are newly integrated into the cellular network, new security vulnerabilities may happen from imprudent integration. In this work, we explore the security vulnerabilities of the cellular IoT from both system-integrated and service-integrated aspects. We discover several vulnerabilities spanning cellular standard design defects, network operation slips, and IoT device implementation flaws. Threateningly, they allow an adversary to remotely identify IP addresses and phone numbers assigned to cellular IoT devices, interrupt their power saving services, and launch various attacks, including data/text spamming, battery draining, device hibernation against them. We validate these vulnerabilities over five major cellular IoT carriers in the U.S. and Taiwan using their certified cellular IoT devices. The attack evaluation result shows that the adversary can raise an IoT data bill by up to {\\$}226 with less than 120 MB spam traffic, increase an IoT text bill at a rate of {\\$}5 per second, and prevent an IoT device from entering/leaving power saving mode; moreover, cellular IoT devices may suffer from denial of IoT services. We finally propose, prototype, and evaluate recommended solutions.}
}


@article{DBLP:journals/ton/Li24,
	author = {Qian Li},
	title = {{TCP} FlexiS: {A} New Approach to Incipient Congestion Detection and
                  Control},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1245--1260},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3319441},
	doi = {10.1109/TNET.2023.3319441},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Li24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Best effort congestion controls strive to achieve an equitable distribution of network resources among competing flows. However, fair resource allocation becomes undesirable when a bandwidth/delay sensitive application shares a bottleneck with a greedy background application. Less than Best Effort (LBE) Congestion Control Algorithms (CCA) are specially designed for background applications, which do not have strict bandwidth/delay requirements. LBE CCAs give foreground applications higher priority in resource allocation by only utilizing spare bandwidth. This can greatly improve network utility at times of congestion. We propose FlexiS– a Flexible Sender side LBE CCA. Unlike most conventional LBE CCAs, which use queue size based congestion detectors and linear rate controllers, FlexiS employs a queue trend based congestion detector and a cubic increase multiplicative decrease rate controller. We have compared FlexiS with LEDBAT and LEDBAT++. Extensive emulation and preliminary Internet tests showed that: 1) FlexiS has comparatively low impact on concurrent best effort TCP flows; 2) it scales to a wide range of available bandwidths; 3) FlexiS flows in aggregation can effectively utilize available bandwidth; 4) contending FlexiS flows can, in most cases, equally share available bandwidth; 5) FlexiS adapts to route changes quickly; and 6) it maintains low priority even when AQM algorithms or shallow buffers are deployed.}
}


@article{DBLP:journals/ton/ZhangHCC24,
	author = {Liping Zhang and
                  Wenshuo Han and
                  Shukai Chen and
                  Kim{-}Kwang Raymond Choo},
	title = {An Efficient and Secure Health Data Propagation Scheme Using Steganography-Based
                  Approach for Electronic Health Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1261--1272},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3313160},
	doi = {10.1109/TNET.2023.3313160},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangHCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electronic health (e-health) networks enable users to enjoy convenient, flexible, and low-cost medical services at home, so they attract great attention and spread into the market quickly. In e-health networks, large amounts of various health data including personal privacy information and physiological signals are transmitted, which raises security risks. To protect the health data transmitted in e-health networks, steganography-based solutions have been widely researched. Although existing steganography-based solutions successfully hide health data in physiological signals such as electrocardiograms (ECG), forward secrecy is not fully considered. This means that adversaries are able to extract users’ health data hidden in previous stego signals by using compromised long-term secrets. Moreover, to reduce communication overhead, compression techniques are introduced in some steganography-based methods. However, the imperceptibility and embedding capacity of these solutions are sacrificed. To solve the above issues, in this study, we adopt Singular Value Decomposition (SVD) and the Bose-Chaudhuri-Hocquenghem (BCH) codes to design an efficient and secure health data propagation scheme based on steganography and compression. In our design, the BCH codes are used to update the encryption key and change the embedding locations in each steganography process, thus achieving forward secrecy and further enhancing the security of steganography. Moreover, a two-stage compression method is proposed in our scheme to compress the signals during signal processing and compression phases, which effectively reduces the communication overhead. Security analysis and the experimental results show that our proposed scheme enhances security while achieving an elaborate balance between imperceptibility, embedding capacity, and compression.}
}


@article{DBLP:journals/ton/WangZD24,
	author = {Xuehe Wang and
                  Shensheng Zheng and
                  Lingjie Duan},
	title = {Dynamic Pricing for Client Recruitment in Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1273--1286},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3312208},
	doi = {10.1109/TNET.2023.3312208},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangZD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Though federated learning (FL) well preserves clients’ data privacy, many clients are still reluctant to join FL given the communication cost and energy consumption in their mobile devices. It is important to design pricing compensations to motivate enough clients to join FL and distributively train the global model. Prior pricing mechanisms for FL are static and cannot adapt to clients’ random arrival pattern over time. We propose a new dynamic pricing solution in closed-form by constructing the Hamiltonian function to optimally balance the client recruitment time and the model training time, without knowing clients’ actual arrivals or training costs. During the client recruitment phase, we offer time-dependent monetary rewards per client arrival to trade off between the total payment and the FL model’s accuracy loss. Such reward gradually increases when we approach to the recruitment deadline or have greater data aging, and we also extend the deadline if the clients’ training time per iteration becomes shorter. Further, we extend to consider heterogeneous client types in training data size and training time per iteration. We successfully extend our dynamic pricing solution and develop an optimal algorithm of linear complexity to monotonically select client types for FL. Finally, we also show robustness of our solution against estimation error of clients’ data sizes, and run numerical experiments to validate our results.}
}


@article{DBLP:journals/ton/LiuX24,
	author = {Yongqiang Liu and
                  Xike Xie},
	title = {A Probabilistic Sketch for Summarizing Cold Items of Data Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1287--1302},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3316426},
	doi = {10.1109/TNET.2023.3316426},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional sketches on counting stream item frequencies use hash functions for mapping data items to a concise structure, e.g., a two-dimensional array, at the expense of overcounting due to hashing collisions. Despite the popularity, it is still challenging to handle cold (low-frequency) items, especially when the space is limited. The cold items can be misreported as hot (high-frequency) items as the accumulation of error in hashing collisions, leading to the estimation accuracy degrading. We find that a streaming item can be split into a set of compactly stored basic elements, which can be recomposed in a probabilistic manner to estimate the frequency of an item. Thus, we design a novel decomposition and recomposition framework, called the XY- sketch, which estimates the frequency of a stream item by estimating the probability of basic elements appearing in the data stream. By improving the estimation accuracy of cold items, we show that advanced streaming queries, such as top- k\nqueries and heavy change queries. Throughout, we conduct theoretical analysis and optimizations under space constraints. Experiments on real datasets are conducted to examine the effectiveness of our proposals.}
}


@article{DBLP:journals/ton/WangLLZZHD24,
	author = {Xiujun Wang and
                  Zhi Liu and
                  Alex X. Liu and
                  Xiao Zheng and
                  Hao Zhou and
                  Ammar Hawbani and
                  Zhe Dang},
	title = {A Near-Optimal Protocol for Continuous Tag Recognition in Mobile {RFID}
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1303--1318},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3317875},
	doi = {10.1109/TNET.2023.3317875},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangLLZZHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile radio frequency identification (RFID) systems typically experience the continual movement of many tags rapidly going in and out of the interrogating range of readers. Readers that are deployed to maintain a current, real-time list of tags, which are present in the interrogating zone at any moment, must repeatedly execute a series of reading cycles. Each of these reading cycles provides the readers very limited time to identify unknown tags (those newly entering into the reader’s range), and, at the same time, to detect missing tags (those just leaving the reader’s range). In this paper, we study the continuous tag recognition problem, which is critical for mobile RFID systems. First, we obtain a lower bound on communication time for solving this problem. We then design a near-OPTimal protocoL, called OPT-L, and prove that its communication time is approximately equal to the lower bound. Finally, we present extensive simulation and experimental results that demonstrate OPT-L’s superior performance over other existing protocols.}
}


@article{DBLP:journals/ton/PacificoDVRNV24,
	author = {Racyus D. G. Pac{\'{\i}}fico and
                  Lucas Ferreira Dos Santos Duarte and
                  Luiz Filipe M. Vieira and
                  Barath Raghavan and
                  Jos{\'{e}} A. M. Nacif and
                  Marcos A. M. Vieira},
	title = {eBPFlow: {A} Hardware/Software Platform to Seamlessly Offload Network
                  Functions Leveraging eBPF},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1319--1332},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3318251},
	doi = {10.1109/TNET.2023.3318251},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PacificoDVRNV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {NFV and SDN enable flexibility and programmability at the data plane. In addition, offloading packet processing to a hardware saves processing resources to compute other workloads. However, fulfilling requirements such as high throughput and low latency with a flexible and programmable data plane is challenging. This paper introduces eBPFlow, a platform for seamlessly accelerating network computation. It builds upon eBPF. eBPFlow combines flexibility and programmability in software with high performance using an FPGA. We implemented our system on the NetFPGA SUME, performing tests on a physical testbed. We built a range of NFs. Our results show that the eBPFlow supports offloading of NFs with throughput at the line rate, latency between 20~\\mu \\text{s} and 40~\\mu \\text{s} , communication with host, and consumption of 22 W. Moreover, eBPFlow processes 12.05 Mpps more than the kernel. eBPFlow has a throughput of 2.59 Gbps higher than the hXDP, a system similar to eBPFlow.}
}


@article{DBLP:journals/ton/YunEMJ24,
	author = {Jihyeon Yun and
                  Atilla Eryilmaz and
                  Jun Moon and
                  Changhee Joo},
	title = {Remote Estimation for Dynamic IoT Sources Under Sublinear Communication
                  Costs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1333--1345},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3314506},
	doi = {10.1109/TNET.2023.3314506},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YunEMJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate a remote estimation system with communication cost for multiple Internet-of-Things sensors, in which the state of each sensor changes according to a Wiener process. Under sublinear communication cost structure, in which the per-transmission cost decreases with the number of simultaneous transmissions, we address an interesting unexplored trade-off under source dynamics between frequent updates of a smaller number of sensors at a higher cost and sporadic updates of a larger number of sensors at a lower cost. We first suggest two benchmark strategies, an all-at-once policy and a multi-threshold policy, and generalize them to a unified framework, called the MAX- k policy. Furthermore, we address the problem of parameter optimization of the MAX- k policy by developing online learning algorithms with stochastic feedback and a continuous search space. Through simulations, we demonstrate that the joint solution of the MAX- k policy and particle swarm optimization-based online learning achieves a high performance, outperforming the well-known upper confidence bound-based competitor.}
}


@article{DBLP:journals/ton/QiuYCZMW24,
	author = {Tie Qiu and
                  Xinwei Yang and
                  Ning Chen and
                  Songwei Zhang and
                  Geyong Min and
                  Dapeng Oliver Wu},
	title = {A Self-Adaptive Robustness Optimization Method With Evolutionary Multi-Agent
                  for IoT Topology},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1346--1361},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3319499},
	doi = {10.1109/TNET.2023.3319499},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QiuYCZMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topology robustness is critical to the connectivity and lifetime of large-scale Internet-of-Things (IoT) applications. To improve robustness while reducing the execution cost, the existing robustness optimization methods utilize neural learning schemes, including neural networks, deep learning, and reinforcement learning. However, insufficient exploration of reinforcement learning agents for topological environments is likely to yield local optima. Moreover, convergence speed is influenced by the sparse reward problem generated while exploring topological environments. To address these problems, this study proposes a self-adaptive robustness optimization method with an evolutionary multi-agent for IoT topology (ROMEM). ROMEM introduces a new multi-agent co-evolution scheme that leverages a non-deterministic strategy to extend the exploration in multi-directions, enabling the reinforcement learning agent to transcend local optima. Furthermore, ROMEM presents a novel distributed training mechanism for multiple agents to accelerate convergence. Experimental results demonstrate that ROMEM can achieve multi-directional collaborative training and outperform other state-of-the-art learning-based robustness optimization methods in terms of convergence efficiency and robustness.}
}


@article{DBLP:journals/ton/WangXXJCZQ24,
	author = {Lun Wang and
                  Yang Xu and
                  Hongli Xu and
                  Zhida Jiang and
                  Min Chen and
                  Wuyang Zhang and
                  Chen Qian},
	title = {{BOSE:} Block-Wise Federated Learning in Heterogeneous Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1362--1377},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3316421},
	doi = {10.1109/TNET.2023.3316421},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangXXJCZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {At the network edge, federated learning (FL) has gained attention as a promising approach for training deep learning (DL) models collaboratively across a large number of devices while preserving user privacy. However, FL still faces specific challenges related to the limited, heterogeneous and dynamic resources of devices. In most FL systems, all devices train the same model, while the devices with constrained resources, referred to as stragglers, will significantly slow down overall training process. It is intuitive to alleviate computation and communication load on the stragglers by training and transmitting a part of the model. Inspired by multi-exit models, we divide an original DL model into several non-overlapping blocks, which can be trained separately on the low-capability devices. Furthermore, we propose BOSE, a novel FL system that performs adaptive block-wise model training under resource constraints. Considering the diverse impacts of different blocks on model convergence and the varying training loads they incur, a naive block assignment strategy, e.g., uniformly random assignment, may not yield optimal model performance and fail to fully utilize available resources. To this end, we introduce two metrics, including learning speed and device-wise divergence, to measure the potential of blocks in promoting model convergence. Given resource budget, BOSE initially identifies a set of candidate blocks for each device and subsequently selects specific training blocks based on their potential for promoting model convergence. In general, blocks with higher potential are more likely to be chosen for training. Extensive experiments on a physical platform show that BOSE provides a 1.4\\times \\sim 3.8\\times speedup without sacrificing model accuracy, compared to the baselines.}
}


@article{DBLP:journals/ton/GaoCW24,
	author = {Hefei Gao and
                  Naiyu Cui and
                  Wei Wang},
	title = {Deployment Optimization of Intelligent Wireless Sensor Network Using
                  Graph Similarity Based on Multi-Granularity Cross Representation and
                  Matching},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1378--1390},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3314497},
	doi = {10.1109/TNET.2023.3314497},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GaoCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent wireless sensor networks often face challenges such as redundancy and non-uniform deployment of sensor nodes, which can negatively impact monitoring performance and energy consumption. To address these challenges, we propose a novel method for identifying the sensor nodes that contribute the most to the monitoring quality of the network. It utilizes graph topology to transform the contribution weights of sensor nodes into the similarity of perturbation-based graphs. And we propose a multi-granularity cross representation and matching method to predict graph similarity, which consists of two stages: representation and matching. In the representation stage, we generate rich multi-granularity interaction features between graph pairs. In the matching stage, we integrate these features into higher-order and more abstract matching features for similarity prediction. To further evaluate the contribution weights of sensor nodes, we combine the obtained graph similarities with the weighted PageRank algorithm. The experimental results demonstrate that our algorithm effectively selects the nodes with greater contribution, leading to good monitoring quality and network performance. Moreover, compared with classical deployment optimization algorithms, the nodes selected by our algorithm are more representative.}
}


@article{DBLP:journals/ton/LuoZXYX24,
	author = {Luyao Luo and
                  Gongming Zhao and
                  Hongli Xu and
                  Zhuolong Yu and
                  Liguang Xie},
	title = {Achieving Cost Optimization for Tenant Task Placement in Geo-Distributed
                  Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1391--1406},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3319434},
	doi = {10.1109/TNET.2023.3319434},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoZXYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud infrastructure has gradually displayed a tendency of geographical distribution in order to provide anywhere, anytime connectivity to tenants all over the world. The tenant task placement in geo-distributed clouds comes with three critical and coupled factors: regional diversity in electricity prices, access delay for tenants, and traffic demand among tasks. However, existing works disregard either the regional difference in electricity prices or the tenant requirements in geo-distributed clouds, resulting in increased operating costs or low user QoS. To bridge the gap, we design a cost optimization framework for tenant task placement in geo-distributed clouds, called TanGo. However, it is non-trivial to achieve an optimization framework while meeting all the tenant requirements. To this end, we first formulate the electricity cost minimization for task placement problem as a constrained mixed-integer non-linear programming problem. We then propose a near-optimal algorithm with a tight approximation ratio (1-1/e) using an effective submodular-based method. Results of in-depth simulations based on real-world datasets show the effectiveness of our algorithm as well as the overall 10%-30% reduction in electricity expenses compared to commonly-adopted alternatives.}
}


@article{DBLP:journals/ton/HuWSLSPLR24,
	author = {Jiahui Hu and
                  Zhibo Wang and
                  Yongsheng Shen and
                  Bohan Lin and
                  Peng Sun and
                  Xiaoyi Pang and
                  Jian Liu and
                  Kui Ren},
	title = {Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving
                  Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1407--1422},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3317870},
	doi = {10.1109/TNET.2023.3317870},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuWSLSPLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) requires frequent uploading and updating of model parameters, which is naturally vulnerable to gradient leakage attacks (GLAs) that reconstruct private training data through gradients. Although some works incorporate differential privacy (DP) into FL to mitigate such privacy issues, their performance is not satisfactory since they did not notice that GLA incurs heterogeneous risks of privacy leakage (RoPL) with respect to gradients from different communication rounds and clients. In this paper, we propose an Adaptive Privacy-Preserving Federated Learning (Adp-PPFL) framework to achieve satisfactory privacy protection against GLA, while ensuring good performance in terms of model accuracy and convergence speed. Specifically, a leakage risk-aware privacy decomposition mechanism is proposed to provide adaptive privacy protection to different communication rounds and clients by dynamically allocating the privacy budget according to the quantified RoPL. In particular, we exploratively design a round-level and a client-level RoPL quantification method to measure the possible risks of GLA breaking privacy from gradients in different communication rounds and clients respectively, which only employ the limited information in general FL settings. Furthermore, to improve the FL model training performance (i.e., convergence speed and global model accuracy), we propose an adaptive privacy-preserving local training mechanism that dynamically clips the gradients and decays the noises added to the clipped gradients during the local training process. Extensive experiments show that our framework outperforms the existing differentially private FL schemes on model accuracy, convergence, and attack resistance.}
}


@article{DBLP:journals/ton/MacDavidCR24,
	author = {Robert MacDavid and
                  Xiaoqi Chen and
                  Jennifer Rexford},
	title = {Scalable Real-Time Bandwidth Fairness in Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1423--1434},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3317172},
	doi = {10.1109/TNET.2023.3317172},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MacDavidCR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network operators want to enforce fair bandwidth sharing between users without solely relying on congestion control running on end-user devices. However, in edge networks (e.g., 5G), the number of user devices sharing a bottleneck link far exceeds the number of queues supported by today’s switch hardware; even accurately tracking per-user sending rates may become too resource-intensive. Meanwhile, traditional software-based queuing on CPUs struggles to meet the high throughput and low latency demanded by 5G users. We propose Approximate Hierarchical Allocation of Bandwidth (AHAB), a per-user bandwidth limit enforcer that runs fully in the data plane of commodity switches. AHAB tracks each user’s approximate traffic rate and compares it against a bandwidth limit, which is iteratively updated via a real-time feedback loop to achieve max-min fairness across users. Using a novel sketch data structure, AHAB avoids storing per-user state, and therefore scales to thousands of slices and millions of users. Furthermore, AHAB supports network slicing, where each slice has a guaranteed share of the bandwidth that can be scavenged by other slices when under-utilized. Evaluation shows AHAB can achieve fair bandwidth allocation within 3.1ms, 13x faster than prior data-plane hierarchical schedulers.}
}


@article{DBLP:journals/ton/ChaturvediCLRRSW24,
	author = {Anya Chaturvedi and
                  Chandra Chekuri and
                  Mengxue Liu and
                  Andr{\'{e}}a W. Richa and
                  Matthias Rost and
                  Stefan Schmid and
                  Jamison Weber},
	title = {Improved Throughput for All-or-Nothing Multicommodity Flows With Arbitrary
                  Demands},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1435--1450},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3325437},
	doi = {10.1109/TNET.2023.3325437},
	timestamp = {Tue, 07 May 2024 20:25:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChaturvediCLRRSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Throughput is a main performance objective in communication networks. This paper considers a fundamental maximum throughput routing problem — the All-or-Nothing Multicommodity Flow (ANF) problem — in arbitrary directed graphs and in the practically relevant but challenging setting where demands can be (much) larger than the edge capacities, mandating the need for splittable flows (i.e., flows may not follow a single path). Formally, the input for the ANF problem is an edge-capacitated directed graph where we have a given number of source-destination node-pairs with their respective demands and strictly positive weights. The goal is to route a maximum weight subset of the given pairs (i.e., the weighted throughput), respecting the edge capacities: A commodity is routed if all of its demand is routed from its respective source to destination (this is the all-or-nothing aspect). We present a polynomial-time bi-criteria approximation randomized rounding framework for this NP-hard problem that yields an arbitrarily good approximation on the weighted throughput while violating the edge capacity constraints by at most a sublogarithmic multiplicative factor. We present two non-trivial linear programming relaxations that can be used in the framework; the first uses a novel edge-flow formulation and the second uses a packing formulation. We demonstrate the “equivalence” of these formulations and then highlight the advantages of each of the two approaches. We complement our theoretical results with a proof of concept empirical evaluation, considering a variety of network scenarios.}
}


@article{DBLP:journals/ton/RodioFMNL24,
	author = {Angelo Rodio and
                  Francescomaria Faticanti and
                  Othmane Marfoq and
                  Giovanni Neglia and
                  Emilio Leonardi},
	title = {Federated Learning Under Heterogeneous and Correlated Client Availability},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1451--1460},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3324257},
	doi = {10.1109/TNET.2023.3324257},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RodioFMNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Federated Learning (FL), devices– also referred to as clients– can exhibit heterogeneous availability patterns, often correlated over time and with other clients. This paper addresses the problem of heterogeneous and correlated client availability in FL. Our theoretical analysis is the first to demonstrate the negative impact of correlation on FL algorithms’ convergence rate and highlights a trade-off between optimization error (related to convergence speed) and bias error (indicative of model quality). To optimize this trade-off, we propose Correlation-Aware FL (CA-Fed), a novel algorithm that dynamically balances the competing objectives of fast convergence and minimal model bias. CA-Fed achieves this by dynamically adjusting the aggregation weight assigned to each client and selectively excluding clients with high temporal correlation and low availability. Experimental evaluations on diverse datasets demonstrate the effectiveness of CA-Fed compared to state-of-the-art methods. Specifically, CA-Fed achieves the best trade-off between training time and test accuracy. By dynamically handling clients with high temporal correlation and low availability, CA-Fed emerges as a promising solution to mitigate the detrimental impact of correlated client availability in FL.}
}


@article{DBLP:journals/ton/ZhaoWZZLTL24,
	author = {Junzhou Zhao and
                  Pinghui Wang and
                  Wei Zhang and
                  Zhaosong Zhang and
                  Maoli Liu and
                  Jing Tao and
                  John C. S. Lui},
	title = {Tracking Influencers in Decaying Social Activity Streams With Theoretical
                  Guarantees},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1461--1476},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3323028},
	doi = {10.1109/TNET.2023.3323028},
	timestamp = {Sun, 06 Oct 2024 21:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoWZZLTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence maximization (IM) is the fundamental problem in many real world applications such as viral marketing, political campaign, and network monitoring. Although extensively studied, most studies on IM assume that social influence is static and they cannot handle the dynamic influence challenge in reality, i.e., a user’s influence is varying over time. To address this challenge, we formulate a novel influencer tracking problem over a social activity stream. In order to keep the solutions up-to-date and forget outdated data in the stream smoothly, we propose a probabilistic-decaying social activity stream (PDSAS) model that enforces each social activity in the stream participating in the analysis with a probability decaying over time. Built on the PDSAS model, we propose a family of streaming optimization algorithms to solve the influencer tracking problem. SIEVE PAIT can identify influencers from a special kind of probabilistic addition-only social activity streams with high efficiency, and guarantees an (1/2-\\epsilon) approximation ratio. BASIC IT leverages SIEVE PAIT as a building block to identify influencers from general PDSASs, and also guarantees an (1/2-\\epsilon) approximation ratio. HIST IT improves the efficiency of BASIC IT, and still guarantees an (1/4-\\epsilon) approximation ratio. Experiments on real data show that our methods can find high quality solutions with much less computational costs than baselines.}
}


@article{DBLP:journals/ton/MalakMZY24,
	author = {Derya Malak and
                  Faruk Volkan Mutlu and
                  Jinkun Zhang and
                  Edmund M. Yeh},
	title = {Joint Power Control and Caching for Transmission Delay Minimization
                  in Wireless HetNets},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1477--1492},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3319674},
	doi = {10.1109/TNET.2023.3319674},
	timestamp = {Sun, 06 Oct 2024 21:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MalakMZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A fundamental challenge in wireless heterogeneous networks (HetNets) is to effectively utilize the limited transmission and storage resources in the presence of increasing deployment density and backhaul capacity constraints. To alleviate bottlenecks and reduce resource consumption, we design optimal caching and power control algorithms for multi-hop wireless HetNets. We formulate a joint optimization framework to minimize the average transmission delay as a function of the caching variables and the signal-to-interference-plus-noise ratios (SINR) which are determined by the transmission powers, while explicitly accounting for backhaul connection costs and the power constraints. Using convex relaxation and rounding, we obtain a reduced-complexity formulation (RCF) of the joint optimization problem, which can provide a constant factor approximation to the globally optimal solution. We then solve RCF in two ways: 1) alternating optimization of the power and caching variables by leveraging biconvexity, and 2) joint optimization of power control and caching. We characterize the necessary (KKT) conditions for an optimal solution to RCF, and use quasi-convexity to show that the KKT points are Pareto optimal for RCF. We then devise a subgradient projection algorithm to jointly update the caching and power variables under general SINR conditions. Finally, our analytical findings are supported by results from extensive numerical experiments.}
}


@article{DBLP:journals/ton/KonstantinidisVR24,
	author = {Konstantinos Konstantinidis and
                  Namrata Vaswani and
                  Aditya Ramamoorthy},
	title = {Detection and Mitigation of Byzantine Attacks in Distributed Training},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1493--1508},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3324697},
	doi = {10.1109/TNET.2023.3324697},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KonstantinidisVR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A plethora of modern machine learning tasks require the utilization of large-scale distributed clusters as a critical component of the training pipeline. However, abnormal Byzantine behavior of the worker nodes can derail the training and compromise the quality of the inference. Such behavior can be attributed to unintentional system malfunctions or orchestrated attacks; as a result, some nodes may return arbitrary results to the parameter server (PS) that coordinates the training. Recent work considers a wide range of attack models and has explored robust aggregation and/or computational redundancy to correct the distorted gradients. In this work, we consider attack models ranging from strong ones: q omniscient adversaries with full knowledge of the defense protocol that can change from iteration to iteration to weak ones: q randomly chosen adversaries with limited collusion abilities which only change every few iterations at a time. Our algorithms rely on redundant task assignments coupled with detection of adversarial behavior. We also show the convergence of our method to the optimal point under common assumptions and settings considered in literature. For strong attacks, we demonstrate a reduction in the fraction of distorted gradients ranging from 16%–99% as compared to the prior state-of-the-art. Our top-1 classification accuracy results on the CIFAR-10 data set demonstrate 25% advantage in accuracy (averaged over strong and weak scenarios) under the most sophisticated attacks compared to state-of-the-art methods.}
}


@article{DBLP:journals/ton/LiLHZM24,
	author = {Yang Li and
                  Zhenhua Li and
                  Zhenhua Han and
                  Quanlu Zhang and
                  Xiaobo Ma},
	title = {Automating Cloud Deployment for Real-Time Online Foundation Model
                  Inference},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1509--1523},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3321967},
	doi = {10.1109/TNET.2023.3321967},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLHZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural network (DNN) foundation models are currently exhibiting high prediction accuracy and strong adaptability to broad tasks with remarkably large model scales. They are increasingly becoming the backend support of DNN-driven real-time online services, e.g., Siri and Instagram. Such services require low-latency and cost-efficiency for quality-of-service and commercial competitiveness. When deployed in a cloud environment, these services call for an appropriate selection of cloud configurations (i.e., specific types of VM instances), as well as a considerate device placement plan that places the operations of the model to multiple GPUs via model parallelism for cost-efficiency. Currently, the deployment mainly relies on service providers’ manual efforts, which is not only onerous but also far from satisfactory oftentimes due to the huge joint search space of cloud configurations and device placement plans (for a same service, a poor deployment can incur significantly more costs by tens of times). In this paper, we attempt to efficiently automate the cloud deployment for real-time foundation model inference with minimum costs under the constraint of acceptably low latency. This attempt is enabled by 1) jointly leveraging the Bayesian Optimization and Deep Reinforcement Learning to adaptively unearth the (nearly) optimal cloud configuration and device placement with limited search time, and 2) enhancing the cost-efficiency of the deployment based on the probing-informed block multiplexing mechanism and Tensor Algebra SuperOptimizer. We implement a prototype system based on TensorFlow, conduct extensive experiments on top of Microsoft Azure, and demonstrate the generality and scalability of our solution. Results show that for lightweight DNN models and foundation models, our solution essentially saves inference costs by up to 15% and 47% with 57% and 38% lower search overheads respectively, compared with non-trivial baselines.}
}


@article{DBLP:journals/ton/YehGAMK24,
	author = {Chia{-}Yi Yeh and
                  Yasaman Ghasempour and
                  Yasith Amarasinghe and
                  Daniel M. Mittleman and
                  Edward W. Knightly},
	title = {Security and Angle-Frequency Coupling in Terahertz WLANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1524--1539},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3321641},
	doi = {10.1109/TNET.2023.3321641},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YehGAMK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the first security study of THz networks employing antennas with the angle-frequency coupling property. Using Leaky Wave Antennas (LWAs) as a representative, we explore the unique security properties due to the frequency-dependent radiation. We show via both analytical models and over-the-air experiments that LWA links exhibit non-uniform secrecy capacity across sub-channels, yielding advantages to an eavesdropper at edge frequencies. Yet, because different frequencies emit towards different angles, the eavesdropper is thwarted from easily intercepting an entire wideband transmission. The experiments diverge from the analytical model in that the model underpredicts the eavesdropper’s advantage at angles smaller than the target user and subsequent asymmetric performance across angles. Nonetheless, both the model and measurements show that increasingly wide bandwidth and correspondingly wide beams have only a modest marginal security penalty. Further, we find the LWA link secrecy not only depends on the target user angle (due to nonlinearity of LWA’s frequency-angle coupling), but also the beamwidth of the frequency components that constitute the collective LWA transmission.}
}


@article{DBLP:journals/ton/KimSSSV24,
	author = {Jungyeol Kim and
                  Rohan Saraogi and
                  Saswati Sarkar and
                  David Starobinski and
                  Santosh S. Venkatesh},
	title = {Capturing the Spread of Information in Heterogeneous {V2X} Through
                  Scalable Computation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1540--1555},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3321836},
	doi = {10.1109/TNET.2023.3321836},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KimSSSV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging V2X technology enables vehicles to exchange messages with each other (V2V) and with signaling infrastructure (I2V) on the roadways. Information propagation in transportation networks is highly influenced by both vehicle mobility and wireless communication. As for vehicle mobility, realistic traffic flow changes with time, exhibiting sharp time-triggered transitions, due to external factors such as traffic lights. Thus, mobility process is temporally heterogeneous and not smooth, which fundamentally alters the dynamics of V2X (V2V and I2V together) message propagation in a complex manner. As for wireless communication, communication heterogeneity is an integral component of V2X systems - different types of vehicles may have different communication capabilities, and V2V and I2V communications coexist. We propose a mathematical framework, based on a continuous-time Markov chain (CTMC), for characterizing the spatio-temporal spread of V2X information (1) when the traffic flow exhibits sharp time-triggered transitions and (2) when there exists communication heterogeneity comprising of different V2V commutation capabilities, different wireless communication conditions, and both V2V and I2V. We prove that the state evolutions under the CTMC model converge to a set of differential equations in the asymptotic limit of a large number of vehicles, enabling computations that gracefully scale with increase in network size and the number of vehicles. Our framework can accommodate arbitrary traffic synchronization patterns corresponding for example to incorporate the presence of an arbitrary number of traffic signals. Furthermore, numerical computations using this mathematical framework answer several questions that influence the practice of V2X network design and security.}
}


@article{DBLP:journals/ton/XuYLLXDXZ24,
	author = {Zichuan Xu and
                  Zhao Yuan and
                  Weifa Liang and
                  Dongqi Liu and
                  Wenzheng Xu and
                  Haipeng Dai and
                  Qiufen Xia and
                  Pan Zhou},
	title = {Learning-Driven Algorithms for Responsive {AR} Offloading With Non-Deterministic
                  Rewards in Metaverse-Enabled {MEC}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1556--1572},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3323514},
	doi = {10.1109/TNET.2023.3323514},
	timestamp = {Tue, 10 Dec 2024 12:30:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuYLLXDXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the coming era of Metaverse, Augmented Reality (AR) has become a key enabler of diverse applications including healthcare, education, smart cities, and entertainments. To provide users with interactive and immersive experience, most AR applications require extremely high responsiveness and ultra-low processing latency. Mobile edge computing (MEC) has demonstrated great potentials in meeting such stringent latency requirements and resource demands of AR applications, by implementing AR requests in edge servers within the proximity of users. In this paper, we investigate the reward maximization problem for AR applications with uncertain resource demands in an MEC network, such that the accumulative reward of services provided for AR applications is maximized, while ensuring that the responsiveness of AR applications is enhanced, subject to network resource capacity. To this end, we formulate an exact solution when the problem size is small, otherwise we devise an efficient approximation algorithm with a provable approximation ratio for the problem. We also develop an online learning algorithm with a bounded regret for the dynamic reward maximization problem without the knowledge of future arrivals of AR requests, by adopting the Multi-Armed Bandits (MAB) technique. Considering maximizing the reward may defer the implementations of some urgent yet low-award requests, we propose a fairness-aware online learning algorithm for the dynamic reward maximization problem, through a data rate prediction mechanism that adopts a multi-task and multi-timescale Long Short-Term Memory (MT2-LSTM) method. Finally, we evaluate the performance of the proposed algorithms for AR applications by building a real test bed. Experimental results show that the proposed algorithms outperform existing studies by improving the award by 13%.}
}


@article{DBLP:journals/ton/SunFL24,
	author = {Rong Sun and
                  Yingjie Fan and
                  Jingwei Liu},
	title = {A Repair-Efficient Piggybacking Design With Small Number of Substripes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1573--1583},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3328560},
	doi = {10.1109/TNET.2023.3328560},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of data scale and limitation on bandwidth in networks, coding scheme in distributed storage system is becoming the research hotspot. The piggybacking framework can solve the problem of high repair bandwidth of traditional MDS codes in distributed storage systems, while satisfying the properties of maximum-distance-separability (MDS) and high code rate which can guarantee the high storage efficiency. Several piggybacking designs have been proposed mainly aiming at the repair of systematic nodes. For the repair of parity nodes, these designs either adopt the traditional MDS code decoding or reduce the repair bandwidth by multiplying the number of substripes, which is not ideal in practice. In this paper, based on an single instance of repair efficiency piggybacking scheme, some specially designed simple finite field additions are introduced to encode parity symbols. The new design scheme is called NREPB. It can effectively reduce the repair bandwidth of parity nodes while ensuring no side-effect on the repair of systematic nodes. And the number of substripes in NREPB remains small, which is important in practical applications.}
}


@article{DBLP:journals/ton/GuoNWD24,
	author = {Jianxiong Guo and
                  Qiufen Ni and
                  Weili Wu and
                  Ding{-}Zhu Du},
	title = {Composite Community-Aware Diversified Influence Maximization With
                  Efficient Approximation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1584--1599},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3321870},
	doi = {10.1109/TNET.2023.3321870},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoNWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence Maximization (IM) is a well-known topic in mobile networks and social computing that aims to find a small subset of users that maximize the influence spread through an online information cascade. Recently, some cautious researchers have paid attention to the diversity of information dissemination, especially community-aware diversity, and formulated the diversified IM problem. Diversity is ubiquitous in many real-world applications, but these applications are all based on a given community structure. In social networks, we can form heterogeneous community structures for the same group of users according to different metrics. Therefore, how to quantify diversity based on multiple community structures is an interesting question. In this paper, we propose a Composite Community-Aware Diversified IM (CC-DIM) problem, which aims to select a seed set to maximize the influence spread and the composite diversity over all possible community structures under consideration. To address the NP-hardness of the CC-DIM problem, we adopt the technique of reverse influence sampling and design a random Generalized Reverse Reachable (G-RR) set to estimate the objective function. The composition of a random G-RR set is much more complex than the RR set used for the IM problem, which will lead to the inefficiency of traditional sampling-based approximation algorithms. Because of this, we further propose a two-stage algorithm, Generalized HIST (G-HIST). It can not only return a (1-1/e-\\varepsilon) approximate solution with at least (1-\\delta) probability but also improve the efficiency of sampling and ease the difficulty of searching by significantly reducing the average size of G-RR sets. Finally, we evaluate our proposed G-HIST on real datasets against existing algorithms. The experimental results show the effectiveness of our proposed algorithm and its superiority over other baseline algorithms.}
}


@article{DBLP:journals/ton/MalandrinoGKLC24,
	author = {Francesco Malandrino and
                  Giuseppe Di Giacomo and
                  Armin Karamzade and
                  Marco Levorato and
                  Carla{-}Fabiana Chiasserini},
	title = {Tuning {DNN} Model Compression to Resource and Data Availability in
                  Cooperative Training},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1600--1615},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3323023},
	doi = {10.1109/TNET.2023.3323023},
	timestamp = {Sat, 08 Jun 2024 13:14:29 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MalandrinoGKLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model compression is a fundamental tool to execute machine learning (ML) tasks on the diverse set of devices populating current- and next-generation networks, thereby exploiting their resources and data. At the same time, how much and when to compress ML models are very complex decisions, as they have to jointly account for such aspects as the model being used, the resources (e.g., computational) and local datasets available at each node, as well as network latencies. In this work, we address the multi-dimensional problem of adapting the model compression, data selection, and node allocation decisions to each other: our objective is to perform the DNN training at the minimum energy cost, subject to learning quality and time constraints. To this end, we propose an algorithmic framework called PACT, combining a time-expanded graph representation of the training process, a dynamic programming solution strategy, and a data-driven approach to the estimation of the loss evolution. We prove that PACT’s complexity is polynomial, and its decisions can get arbitrarily close to the optimum. Through our numerical evaluation, we further show how PACT can consistently outperform state-of-the-art alternatives and closely matches the optimal energy consumption.}
}


@article{DBLP:journals/ton/YouTWWDF24,
	author = {Lizhao You and
                  Zhirong Tang and
                  Pengbo Wang and
                  Zhaorui Wang and
                  Haipeng Dai and
                  Liqun Fu},
	title = {Quick and Reliable LoRa Data Aggregation Through Multi-Packet Reception},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1616--1630},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3323018},
	doi = {10.1109/TNET.2023.3323018},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YouTWWDF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a Long Range (LoRa) data aggregation system (LoRaPDA) that aggregates data (e.g., sum, average, min, max) directly in the physical layer. In particular, after coordinating a few nodes to transmit their data simultaneously, the gateway leverages a new multi-packet reception (MPR) approach to compute aggregate data from the phase-asynchronous superimposed signal. Different from the analog approach which requires additional power synchronization and phase synchronization, our MRP-based digital approach is compatible with commercial LoRa nodes and is more reliable. Different from traditional MPR approaches that are designed for the collision decoding scenario, our new MPR approach allows simultaneous transmissions with small packet arrival time offsets, and addresses a new co-located peak problem through the following components: 1) an improved channel and offset estimation algorithm that enables accurate phase tracking in each symbol, 2) a new symbol demodulation algorithm that finds the maximum likelihood sequence of nodes’ data, and 3) a soft-decision packet decoding algorithm that utilizes the likelihoods of several sequences to improve decoding performance. Trace-driven simulation results show that the symbol demodulation algorithm outperforms the state-of-the-art MPR decoder by 5.3\\times\nin terms of physical-layer throughput, and the soft decoder is more robust to unavoidable adverse phase misalignment and estimation error in practice. Moreover, LoRaPDA outperforms the state-of-the-art MPR scheme by at least 2.1\\times\nfor all SNRs in terms of network throughput, demonstrating quick and reliable data aggregation.}
}


@article{DBLP:journals/ton/GuoHYTYG24,
	author = {Xianwei Guo and
                  Fangwan Huang and
                  Dingqi Yang and
                  Chunyu Tu and
                  Zhiyong Yu and
                  Wenzhong Guo},
	title = {Spatiotemporal Fracture Data Inference in Sparse Mobile Crowdsensing:
                  {A} Graph- and Attention-Based Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1631--1644},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3323522},
	doi = {10.1109/TNET.2023.3323522},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoHYTYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsensing (MCS) is a sensing paradigm that enables large-scale smart city applications, such as environmental sensing and traffic monitoring. However, traditional MCS often suffers from performance degradation due to the limited spatiotemporal coverage of collected data. In this context, Sparse MCS has been proposed, which utilizes data inference algorithms to recover full data from sparse data collected by users. However, existing Sparse MCS approaches often overlook spatiotemporal fractures, where no data is observed either for a sensing subarea across all sensing time slots (temporal fracture), or for a sensing time slot in all sensing subarea (spatial fracture). Such spatiotemporal fractures pose great challenges to the data inference algorithms, as it is difficult to capture the complex spatiotemporal correlations of the sensing data from very limited observations. To address this issue, we propose a Graph- and Attention-based Matrix Completion (GAMC) method for the spatiotemporal fracture data inference problem in Sparse MCS. Specifically, we first pre-fill the general missing values using the classical Matrix Factorization (MF) technique. Then, we propose a neural network architecture based on Graph Attention Networks (GAT) and Transformer to capture complex spatiotemporal dependencies in the sensing data. Finally, we recover the complete data with a projection layer. We conduct extensive experiments on three real-world urban sensing datasets. The experimental results show the effectiveness of the proposed method.}
}


@article{DBLP:journals/ton/DaggittG24,
	author = {Matthew L. Daggitt and
                  Timothy G. Griffin},
	title = {Formally Verified Convergence of Policy-Rich {DBF} Routing Protocols},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1645--1660},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3326336},
	doi = {10.1109/TNET.2023.3326336},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DaggittG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we present new general convergence results about the behaviour of the Distributed Bellman-Ford (DBF) family of routing protocols, which includes distance-vector protocols (e.g. RIP) and path-vector protocols (e.g. BGP). Our results apply to “policy-rich” protocols, by which we mean protocols that can have complex policies (e.g. conditional route transformations) that violate traditional assumptions made in the standard presentation of Bellman-Ford protocols. First, we propose a new algebraic model for abstract routing problems which has fewer primitives than previous models and can represent more expressive policy languages. The new model is also the first to allow concurrent reasoning about distance-vector and path-vector protocols. Second, we explicitly demonstrate how DBF routing protocols are instances of a larger class of asynchronous iterative algorithms, for which there already exist powerful results about convergence. These results allow us to build upon conditions previously shown by Sobrinho to be sufficient and necessary for the convergence of path-vector protocols and generalise and strengthen them in various ways: we show that, with a minor modification, they also apply to distance-vector protocols; we prove they guarantee that the final routing solution reached is unique, thereby eliminating the possibility of anomalies such as BGP wedgies; we relax the model of asynchronous communication, showing that the results still hold if routing messages can be lost, reordered, and duplicated. Thirdly, our model and our accompanying theoretical results have been fully formalised in the Agda theorem prover. The resulting library is a powerful tool for quickly prototyping and formally verifying new policy languages. As an example, we formally verify the correctness of a policy language with many of the features of BGP including communities, conditional policy, path-inflation and route filtering.}
}


@article{DBLP:journals/ton/WangYYXGLZ24,
	author = {Xiaojian Wang and
                  Ruozhou Yu and
                  Dejun Yang and
                  Guoliang Xue and
                  Huayue Gu and
                  Zhouyu Li and
                  Fangtong Zhou},
	title = {Fence: Fee-Based Online Balance-Aware Routing in Payment Channel Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1661--1676},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3324136},
	doi = {10.1109/TNET.2023.3324136},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangYYXGLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scalability is a critical challenge for blockchain-based cryptocurrencies. Payment channel networks (PCNs) have emerged as a promising solution for this challenge. However, channel balance depletion can significantly limit the capacity and usability of a PCN. Specifically, frequent transactions that result in unbalanced payment flows from two ends of a channel can quickly deplete the balance on one end, thus blocking future payments from that direction. In this paper, we propose Fence, an online balance-aware fee setting algorithm to prevent channel depletion and improve PCN sustainability and long-term throughput. In our algorithm, PCN routers set transaction fees based on the current balance and level of congestion on each channel, in order to incentivize payment senders to utilize paths with more balance and less congestion. Our algorithm is guided by online competitive algorithm design, and achieves an asymptotically tight competitive ratio with constant violation in a unidirectional PCN. We further prove that no online algorithm can achieve a finite competitive ratio in a general PCN. Extensive simulations under a real-world PCN topology show that Fence achieves high throughput and keeps network channels balanced, compared to state-of-the-art PCN routing algorithms.}
}


@article{DBLP:journals/ton/LiGLWCXX24,
	author = {Jing Li and
                  Song Guo and
                  Weifa Liang and
                  Jianping Wang and
                  Quan Chen and
                  Zichuan Xu and
                  Wenzheng Xu},
	title = {AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered
                  Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1677--1690},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3324704},
	doi = {10.1109/TNET.2023.3324704},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiGLWCXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging digital twin technique enhances the network management efficiency and provides comprehensive insights on network performance, through mapping physical objects to their digital twins. The user satisfaction on digital twin-enabled service relies on the freshness of digital twin data, which is measured by the Age of Information (AoI). Due to long service delays, the use of the remote cloud for delay-sensitive service provisioning faces serious challenges. Mobile Edge Computing (MEC), as an ideal paradigm for delay-sensitive services, is able to realize real-time data communication between physical objects and their digital twins at the network edge. However, the mobility of physical objects and dynamics of user query arrivals make seamless service provisioning in MEC become challenging. In this paper, we investigate dynamic digital twin placements for improving user service satisfaction in MEC environments, by introducing a novel metric to measure user service satisfaction based on the AoI concept and formulating two user service satisfaction enhancement problems: the static and dynamic utility maximization problems under static and dynamic digital twin placement schemes. To this end, we first formulate an Integer Linear Programming (ILP) solution to the static utility maximization problem when the problem size is small; otherwise, we propose a performance-guaranteed approximation algorithm. We then propose an online algorithm with a provable competitive ratio for the dynamic utility maximization problem, by considering dynamic user query services. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform the comparison baseline algorithms, improving the algorithm performance by at least 10.7%, compared to the baseline algorithms.}
}


@article{DBLP:journals/ton/FuL24,
	author = {Qiang Fu and
                  Jiajia Liu},
	title = {Generalized Multi-Hop {NR} Sidelink Relay for Future {V2X} Communication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1691--1706},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3327754},
	doi = {10.1109/TNET.2023.3327754},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sidelink relay, authorized as an independent work item in both 3GPP R17 and R18, is a promising technology for facilitating future V2X communication in 5G New Radio (NR). As specified by 3GPP, several important factors of NR sidelink, including flexible hop-by-hop operation mode selection, sidelink path loss based power control, and resource sharing with uplink communications, were partially (if not totally) ignored in available studies. In light of this, we propose a general multi-hop NR sidelink relay scheme, namely, NSR- (\\rho,\\omega,m) , for flexible dissemination of a typical Decentralized Environmental Notification Message (DENM). To efficiently characterize the multi-hop DENM dissemination process under NSR- (\\rho,\\omega,m) , we further develop a stochastic geometry-based theoretical framework, by carefully taking into account the above 3GPP specified important factors. With the help of the theoretical framework, we are able to derive the end-to-end delivery probability and the total dissemination distance of a given DENM. Extensive numerical results are presented to verify the effectiveness of the theoretical framework. For a given DENM with a limited lifetime, it is also proved that the NSR- (\\rho,\\omega,m) scheme is of great potential in significantly expanding the achievable performance region, by properly tuning the parameters \\rho , \\omega , and m .}
}


@article{DBLP:journals/ton/XinLJLXLTZ24,
	author = {Yao Xin and
                  Wenjun Li and
                  Chengjun Jia and
                  Xianfeng Li and
                  Yang Xu and
                  Bin Liu and
                  Zhihong Tian and
                  Weizhe Zhang},
	title = {Recursive Multi-Tree Construction With Efficient Rule Sifting for
                  Packet Classification on {FPGA}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1707--1722},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3330381},
	doi = {10.1109/TNET.2023.3330381},
	timestamp = {Fri, 07 Feb 2025 15:45:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XinLJLXLTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a programmable accelerator, SmartNIC provides more opportunities for algorithmic packet classification. Our aim in this work is to achieve both line-speed rule search and efficient rule update, two highly desired metrics for SDN data plane. We leverage the parallelism offered by the FPGA in SmartNIC following an algorithm/hardware co-design paradigm. Particularly, we first design an algorithm that constructs multiple trees for the rule set with a recursive rule sifting process. Unlike traditional space-cutting-based multi-tree construction, our rule sifting mechanism breaks the space constraints of rule-to-tree mapping and enables bounded height on each tree, thus providing the potential of bounded worst-case and line-speed performance. We then design a flexible hardware architecture with multiple systolic arrays that can be implemented in parallel on FPGA. Each systolic array works as a coarse-grained pipeline, and the multiple trees constructed earlier will be mapped onto these pipeline stages. This hardware-software mapping enables bounded worst-case rule searching. Additionally, incremental rule update is achieved simply by traversing the pipeline in one pass, with little and bounded impact on rule searching. Experimental results show that our design achieves an average classification throughput of 600.8/147.5 MPPS and an update throughput of 8.2/5.9 MUPS for 10k/100k-scale 5-tuple and OpenFlow rule sets.}
}


@article{DBLP:journals/ton/YuXZYL24,
	author = {Haiyang Yu and
                  Runtong Xu and
                  Hui Zhang and
                  Zhen Yang and
                  Huan Liu},
	title = {{EV-FL:} Efficient Verifiable Federated Learning With Weighted Aggregation
                  for Industrial IoT Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1723--1737},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3328635},
	doi = {10.1109/TNET.2023.3328635},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuXZYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Industrial IoT (IIoT) opens up promising possibilities for data analysis and machine learning in IIoT networks. As a distributed paradigm, federated learning (FL) allows numerous IIoT devices to collaboratively train a global model without collecting their local data together in central servers. Unfortunately, a centralized server used to aggregate local gradients can be compromised and forge the result, which incurs the need for aggregation verification. Several approaches focusing on verifying the correctness of aggregation have been proposed. However, it is still an open problem since devices have to devote more computation resources for verification, which are especially not friendly to resource-constrained IIoT devices. Furthermore, verifying weighted aggregation has not been supported in existing approaches. In this paper, we propose an efficient verifiable federated learning approach for IIoT networks, which verifies the aggregation of gradients and requires lowest burden on IIoT devices by introducing zero-knowledge proof techniques. Moreover, our design supports weighted aggregation verification to validate the aggregation of weighted gradients in the cloud server. By comparing the proposed approach with the state-of-the-art schemes including VerifyNet and VeriFL, we demonstrate the superior performance of our approach for resource-constrained devices, which minimizes the computational overheads of the IIoT devices.}
}


@article{DBLP:journals/ton/WangLGLGC24,
	author = {Ping Wang and
                  Zhetao Li and
                  Bin Guo and
                  Saiqin Long and
                  Suiming Guo and
                  Jiannong Cao},
	title = {A UAV-Assisted Truth Discovery Approach With Incentive Mechanism Design
                  in Mobile Crowd Sensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1738--1752},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3331059},
	doi = {10.1109/TNET.2023.3331059},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangLGLGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incentive mechanisms are essential to incentive workers carrying mobile handheld devices to participate in mobile crowd sensing and finally achieve good truth discovery performance. However, malicious workers may report false or malicious data to defraud rewards, resulting in service quality degradation. Moreover, the existing incentive mechanism is challenging to identify malicious workers when recruiting workers in reality, which results in low accuracy of truth discovery and waste of cost. In this paper, we propose an Incentive-based Truth Discovery (ITD) scheme to incentive credible workers to submit high-quality data, thereby enhancing the accuracy of truth discovery. In the ITD, an unmanned aerial vehicle (UAV)-assisted split-aggregation truth discovery mechanism is proposed firstly to infer the truth. The addition of the UAV can improve the accuracy of truth discovery and assist in evaluating workers’ trust. Then, we evaluate the quality of participants’ data and propose a data quality-based trust meter to update each worker’s trust to guide future recruitment efforts. Finally, a Quality-aware Trustworthy Incentive (QTI) mechanism is proposed to select credible workers for data collection and provide them with reasonable payments. The experimental results show that ITD improves the accuracy of truth discovery by 98.47%, over the state-of-the-art, at a sensing cost reduced by as high as 43.89%.}
}


@article{DBLP:journals/ton/HuangWPWHLDLC24,
	author = {Sijiang Huang and
                  Yunze Wei and
                  Lingfeng Peng and
                  Mowei Wang and
                  Linbo Hui and
                  Peng Liu and
                  Zongpeng Du and
                  Zhenhua Liu and
                  Yong Cui},
	title = {xNet: Modeling Network Performance With Graph Neural Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1753--1767},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3329357},
	doi = {10.1109/TNET.2023.3329357},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangWPWHLDLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s network is notorious for its complexity and uncertainty. Network operators often rely on network models for efficient network planning, operation, and optimization. The network model is responsible for understanding the complex relationships between network performance metrics (e.g., delay and jitter) and network characteristics (e.g., traffic and configuration). However, we still lack a systematic approach to developing accurate and lightweight network models that are aware of the impact of network configurations (i.e., expressiveness) and provide fine-grained flow-level temporal predictions (i.e., granularity). In this paper, we propose xNet, a data-driven network modeling framework based on graph neural networks (GNN). It is worth noting that xNet is not a dedicated network model designed for a specific network scenario with constraint considerations. On the contrary, xNet provides a general approach to modeling the network characteristics of concern with relation graph representations and configurable GNN blocks. xNet learns the state transition functions between time steps and rolls them out to obtain the full fine-grained prediction trajectory. We implement and instantiate xNet with three use cases. The experimental results show that xNet can accurately predict different performance metrics (i.e. temporal and steady-state QoS) in different scenarios, with performance comparable to state-of-the-art domain-specific models. Compared with traditional packet-level simulators, xNet achieves a speed improvement of more than two orders of magnitude, demonstrating its promising application in real-time optimization of network configurations.}
}


@article{DBLP:journals/ton/LiuLXLWM24,
	author = {Jun Liu and
                  Jianchun Liu and
                  Hongli Xu and
                  Yunming Liao and
                  Zhiyuan Wang and
                  Qianpiao Ma},
	title = {{YOGA:} Adaptive Layer-Wise Model Aggregation for Decentralized Federated
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1768--1780},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3329005},
	doi = {10.1109/TNET.2023.3329005},
	timestamp = {Fri, 17 May 2024 21:41:58 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuLXLWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional Federated Learning (FL) is a promising paradigm that enables massive edge clients to collaboratively train deep neural network (DNN) models without exposing raw data to the parameter server (PS). To avoid the bottleneck on the PS, Decentralized Federated Learning (DFL), which utilizes peer-to-peer (P2P) communication without maintaining a global model, has been proposed. Nevertheless, DFL still faces two critical challenges, i.e., limited communication bandwidth and not independent and identically distributed (non-IID) local data, thus hindering efficient model training. Existing works commonly assume full model aggregation at periodic intervals, i.e., clients periodically collect models from peers. To reduce the communication cost, these methods allow clients to collect model(s) from selected peers, but often result in a significant degradation of model accuracy when dealing with non-IID data. Alternatively, the layer-wise aggregation mechanism has been proposed to alleviate communication overhead under the PS architecture, but its potential in DFL remains rarely explored yet. To this end, we propose an efficient DFL framework YOGA that adaptively performs layer-wise model aggregation and training. Specifically, YOGA first generates the ranking of layers in the model according to the learning speed and layer-wise divergence. Combining with the layer ranking and peers’ status information (i.e., data distribution and communication capability), we propose the max-match (MM) algorithm to generate the proper layer-wise model aggregation policy for the clients. Extensive experiments on DNN models and datasets show that YOGA saves communication cost by about 45% without sacrificing the model performance compared with the baselines, and provides 1.53- 3.5\\times\nspeedup on the physical platform.}
}


@article{DBLP:journals/ton/ZhengWT24,
	author = {Gao Zheng and
                  Ning Wang and
                  Regius Rahim Tafazolli},
	title = {{SDN} in Space: {A} Virtual Data-Plane Addressing Scheme for Supporting
                  {LEO} Satellite and Terrestrial Networks Integration},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1781--1796},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3330672},
	doi = {10.1109/TNET.2023.3330672},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengWT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating Low Earth Orbit (LEO) satellites with terrestrial network infrastructures to support ubiquitous Internet service coverage has recently received increasing research momentum. One fundamental challenge is the frequent topology change caused by the constellation behaviour of LEO satellites. In the context of Software Defined Networking (SDN), the controller function that is originally required to control the conventional data plane fulfilled by terrestrial SDN switches will need to expand its responsibility to cover their counterparts in the space, namely LEO satellites that are used for data forwarding. As such, seamless integration of the fixed control plane on the ground and the mobile data plane fulfilled by constellation LEO satellites will become a distinct challenge. For the very first time in the literature, we propose in this paper the Virtual Data-Plane Addressing (VDPA) scheme by leveraging IP addresses to represent virtual switches at the fixed space locations which are periodically instantiated by the nested LEO satellites traversing them in a predictable manner. With such a scheme the changing data-plane network topology incurred by LEO satellite constellations can be made completely agnostic to the control plane on the ground, thus enabling a native approach to supporting seamless communication between the two planes. Our simulation results prove the superiority of the proposed VDPA based flow rule manipulation mechanism in terms of control plane performance.}
}


@article{DBLP:journals/ton/GongHWCZL24,
	author = {Wei Gong and
                  Yimeng Huang and
                  Qiwei Wang and
                  Si Chen and
                  Jia Zhao and
                  Jiangchuan Liu},
	title = {Efficient Single-Symbol Backscatter With Uncontrolled Ambient {OFDM}
                  WiFi},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1797--1806},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3332220},
	doi = {10.1109/TNET.2023.3332220},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GongHWCZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of controlled excitation makes pervasive backscatter communication difficult to achieve and the redundant modulation severely limits the performance of the system. We present a novel WiFi backscatter system that can take uncontrolled OFDM WiFi signals as excitations and efficiently embed tag data at the single-symbol rate. Specifically, we are the first to discover the fundamental reason why the previous systems have to rely on multi-symbol modulation, which makes it possible to demodulate tag data on the single-symbol level. Further, we design deinterleaving-twins decoding that can reuse any uncontrolled WiFi signals as carriers to backscatter tag data. Moreover, we present how to robustly handle high-order excitations, including different demapping rules for diverse excitations and three different bit-translation methods for decoding. To verify the effectiveness of our proposal, we prototype our solution using various FPGAs and SDRs. Comprehensive evaluations show that our solution’s maximum throughput is 3.92x and 1.97x better than FreeRider and MOXcatter. In addition, with 16QAM excitations, the decoding BERs of majority voting are around 5%, which is 10x better than subsequence matching and jaccard similarity methods. Meanwhile, the throughput of deinterleaving-level demodulation is 2x better than payload-level demodulation with 16QAM ambient traffic.}
}


@article{DBLP:journals/ton/TangMLJWZ24,
	author = {Zhiqing Tang and
                  Fangyi Mou and
                  Jiong Lou and
                  Weijia Jia and
                  Yuan Wu and
                  Wei Zhao},
	title = {Multi-User Layer-Aware Online Container Migration in Edge-Assisted
                  Vehicular Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1807--1822},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3330255},
	doi = {10.1109/TNET.2023.3330255},
	timestamp = {Fri, 24 May 2024 22:53:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TangMLJWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge-assisted vehicular networks, containers are very suitable for deploying applications and providing services due to their lightweight and rapid deployment. To provide high-quality services, many existing studies show that the containers need to be migrated to follow the vehicles’ trajectory. However, it has been conspicuously neglected by existing work that making full use of the complex layer-sharing information of containers among multiple users can significantly reduce migration latency. In this paper, we propose a novel online container migration algorithm to reduce the overall task latency. Specifically: 1) we model the multi-user layer-aware online container migration problem in edge-assisted vehicular networks, comprehensively considering the initialization latency, computation latency, and migration latency. 2) A feature extraction method based on attention and long short-term memory is proposed to fully extract the multi-user layer-sharing information. Then, a policy gradient-based reinforcement learning algorithm is proposed to make the online migration decisions. 3) The experiments are conducted with real-world data traces. Compared with the baselines, our algorithms effectively reduce the total latency by 8% to 30% on average.}
}


@article{DBLP:journals/ton/CaoXYSZHL24,
	author = {Hao Cao and
                  Jingao Xu and
                  Zheng Yang and
                  Longfei Shangguan and
                  Jialin Zhang and
                  Xiaowu He and
                  Yunhao Liu},
	title = {Scaling Up Edge-Assisted Real-Time Collaborative Visual {SLAM} Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1823--1838},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3330763},
	doi = {10.1109/TNET.2023.3330763},
	timestamp = {Sat, 04 May 2024 10:56:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaoXYSZHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The edge-based multi-agent visual SLAM is crucial for emerging mobile applications like search-and-rescue, inventory automation, and industrial inspection. It uses a central node to manage the global map and schedule tasks for agents. However, as the number of agents increases, the system faces scalability challenges due to operational overhead, such as data redundancy, bandwidth consumption, and localization errors. In this paper, we introduce SwarmMap, a framework designed to enhance the scalability of collaborative visual SLAM service in edge offloading settings. SwarmMap consists of three system modules: a change log-based server-client synchronization mechanism, a priority-aware task scheduler, and a lean global map representation. These modules work together to address the challenges of data explosion problems. SwarmMap is open-source and compatible with the robotic operating system (ROS). Existing visual SLAM applications could incorporate SwarmMap through SwarmAPI, a set of well-packaged APIs, to compose SwarmMap’s function modules to enhance their performance and capacity in multi-agent scenarios. Comprehensive evaluations and a three-month case study at one of the world’s largest oilfields demonstrate that SwarmMap can serve 2\\times more agents (> 20 agents) than the state-of-the-arts with the same resource overhead, meanwhile maintaining an average trajectory error of 38cm , outperforming existing works by > 55%.}
}


@article{DBLP:journals/ton/KangEJ24,
	author = {Sunjung Kang and
                  Atilla Eryilmaz and
                  Changhee Joo},
	title = {Comparison of Decentralized and Centralized Update Paradigms for Distributed
                  Remote Estimation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {2},
	pages = {1839--1853},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3332590},
	doi = {10.1109/TNET.2023.3332590},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KangEJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we perform a comparative study of centralized and decentralized update strategies for the basic remote tracking problem of many distributed users/devices with randomly evolving states. Our goal is to reveal the impact of the fundamentally different tradeoffs that exist between information accuracy and communication cost under these two update paradigms. In one extreme, decentralized updates are triggered by distributed users/transmitters based on exact local state-information, but also at a higher cost due to the need for uncoordinated multi-user communication. In the other extreme, centralized updates are triggered by the common tracker/receiver based on estimated global state-information, but also at a lower cost due to the capability of coordinated multi-user communication. We use a generic superlinear function to model the communication cost with respect to the number of simultaneous updates for multiple sources. We characterize the conditions under which transmitter-driven decentralized update policies outperform their receiver-driven centralized counterparts for symmetric sources, and vice versa. Further, we extend the results to a scenario where system parameters are unknown and develop learning-based update policies that asymptotically achieve the minimum cost levels attained by the optimal policies.}
}


@article{DBLP:journals/ton/SunHLBZB24,
	author = {Haifeng Sun and
                  Qun Huang and
                  Patrick P. C. Lee and
                  Wei Bai and
                  Feng Zhu and
                  Yungang Bao},
	title = {Distributed Network Telemetry With Resource Efficiency and Full Accuracy},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1857--1872},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3327345},
	doi = {10.1109/TNET.2023.3327345},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunHLBZB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network telemetry is essential for administrators to monitor massive data traffic in a network-wide manner. Existing telemetry solutions often face the dilemma between resource efficiency (i.e., low CPU, memory, and bandwidth overhead) and full accuracy (i.e., error-free and holistic measurement). We break this dilemma via a network-wide architectural design OmniMon, which simultaneously achieves resource efficiency and full accuracy in flow-level telemetry for large-scale data centers. OmniMon carefully coordinates the collaboration among different types of entities in the whole network to execute telemetry operations, such that the resource constraints of each entity are satisfied without compromising full accuracy. It further addresses consistency in network-wide epoch synchronization and accountability in error-free packet loss inference. We prototype OmniMon in DPDK and P4. Testbed experiments on commodity servers and Tofino switches demonstrate the effectiveness of OmniMon over state-of-the-art solutions.}
}


@article{DBLP:journals/ton/ZhouLLZTYX24,
	author = {Guangmeng Zhou and
                  Qi Li and
                  Yang Liu and
                  Yi Zhao and
                  Qi Tan and
                  Su Yao and
                  Ke Xu},
	title = {FedPAGE: Pruning Adaptively Toward Global Efficiency of Heterogeneous
                  Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1873--1887},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3328632},
	doi = {10.1109/TNET.2023.3328632},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouLLZTYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When workers are heterogeneous in computing and transmission capabilities, the global efficiency of federated learning suffers from the straggler issue, i.e., the slowest worker drags down the overall training process. We propose a novel and efficient federated learning framework named FedPAGE, where workers perform distributed pruning adaptively towards global efficiency, i.e., fast training and high accuracy. For fast training, we develop a pruning rate learning approach generating an adaptive pruning rate for each worker, making the overall update time approximate to the fastest worker’s update time, i.e., no stragglers. For high accuracy, we find that structural similarity between sub-models is essential to global model accuracy in the distributed pruning, and thus propose the CIG_X pruning scheme to ensure maximum similarity. Meanwhile, we adopt the sparse training and design model aggregating of different size sub-models to cope with distributed pruning. We prove the convergence of FedPAGE and demonstrate the effectiveness of FedPAGE on image classification and natural language inference tasks. Compared with the state-of-the-art, FedPAGE achieves higher accuracy with the same speedup ratio.}
}


@article{DBLP:journals/ton/TianLZSZC24,
	author = {Han Tian and
                  Xudong Liao and
                  Chaoliang Zeng and
                  Decang Sun and
                  Junxue Zhang and
                  Kai Chen},
	title = {Efficient DRL-Based Congestion Control With Ultra-Low Overhead},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1888--1903},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3330737},
	doi = {10.1109/TNET.2023.3330737},
	timestamp = {Thu, 04 Jul 2024 22:03:57 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TianLZSZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Previous congestion control (CC) algorithms based on deep reinforcement learning (DRL) directly adjust flow sending rate to respond to dynamic bandwidth change, resulting in high inference overhead. Such overhead may consume considerable CPU resources and hurt the datapath performance. In this paper, we present SPINE, a hierarchical congestion control algorithm that fully utilizes the performance gain from deep reinforcement learning but with ultra-low overhead. At its heart, SPINE decouples the congestion control task into two subtasks in different timescales and handles them with different components: 1) lightweight CC executor that performs fine-grained control responding to dynamic bandwidth changes; and 2) RL agent that works at a coarse-grained level that generates control sub-policies for the CC executor. Such two-level control architecture can provide fine-grained DRL-based control with a low model inference overhead. Real-world experiments and emulations show that SPINE achieves consistent high performance across various network conditions with an ultra-low control overhead reduced by at least 80% compared to its DRL-based counterparts, similar to classic CC schemes such as Cubic.}
}


@article{DBLP:journals/ton/MaQSALLZLG24,
	author = {Xiaobo Ma and
                  Jian Qu and
                  Mawei Shi and
                  Bingyu An and
                  Jianfeng Li and
                  Xiapu Luo and
                  Junjie Zhang and
                  Zhenhua Li and
                  Xiaohong Guan},
	title = {Website Fingerprinting on Encrypted Proxies: {A} Flow-Context-Aware
                  Approach and Countermeasures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1904--1919},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3337270},
	doi = {10.1109/TNET.2023.3337270},
	timestamp = {Sun, 19 Jan 2025 13:55:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaQSALLZLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Website fingerprinting (WFP) could infer which websites a user is accessing via an encrypted proxy by passively inspecting the traffic characteristics of accessing different websites between the user and the proxy. Designing WFP attacks is crucial for understanding potential vulnerabilities of encrypted proxies, which guides the design of defensive measures against WFP. In this paper, we design a novel WFP attack against (popular) encrypted proxies that relay connections between the user and the proxy individually (e.g., Shadowsocks, V2Ray), and accordingly implement lightweight countermeasures to effectively defend against the attack. The attack features flow-context-aware and is both accurate and immediately deployable, because it fully considers the obstacle (dubbed training-testing asymmetry) that fundamentally limits the practicability of WFP and addresses the obstacle with built-in spatial-temporal flow correlation mechanism. We implement the countermeasure as middleboxes installed on both the client and server sides of encrypted proxies, without altering any existing infrastructures for compatibility. The middleboxes can obfuscate a website’s flow regularities across different visits. Large-scale experiments in real-world scenarios demonstrate that the WFP attack can generally achieve a detection rate above 98.8% with a false positive rate below 0.2%. The countermeasure forces the attack’s false positive rate to be above 0.2 and true positive rate to be below 0.9 with just five persistent TCP connections while introducing very limited bandwidth overhead (e.g., 0.49%) and almost-zero additional network latency.}
}


@article{DBLP:journals/ton/TangST24,
	author = {Haoyue Tang and
                  Yin Sun and
                  Leandros Tassiulas},
	title = {Sampling of the Wiener Process for Remote Estimation Over a Channel
                  With Unknown Delay Statistics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1920--1935},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3331266},
	doi = {10.1109/TNET.2023.3331266},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TangST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study an online sampling problem of the Wiener process. The goal is to minimize the mean squared error (MSE) of the remote estimator under a sampling frequency constraint when the transmission delay distribution is unknown. The sampling problem is reformulated into an optional stopping problem, and we propose an online sampling algorithm that can adaptively learn the optimal stopping threshold through stochastic approximation. We prove that the cumulative MSE regret grows with rate \\mathcal {O}(\\ln k)\n, where k\nis the number of samples. Through Le Cam’s two point method, we show that the worst-case cumulative MSE regret of any online sampling algorithm is lower bounded by \\Omega (\\ln k)\n. Hence, the proposed online sampling algorithm is minimax order-optimal. Finally, we validate the performance of the proposed algorithm via numerical simulations.}
}


@article{DBLP:journals/ton/ZhengXYLM24,
	author = {Xiaolong Zheng and
                  Dan Xia and
                  Fu Yu and
                  Liang Liu and
                  Huadong Ma},
	title = {Enabling Cross-Technology Communication From WiFi to LoRa With {IEEE}
                  802.11ax},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1936--1950},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3333355},
	doi = {10.1109/TNET.2023.3333355},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengXYLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent work proposes Cross-Technology Communication (CTC) from IEEE 802.11b to LoRa but has a low efficiency due to the extremely asymmetric data rates. In this paper, we propose WiRa that emulates LoRa waveform with IEEE 802.11ax. By taking advantage of the OFDMA in 802.11ax, WiRa uses only a small Resource Unit (RU) to emulate LoRa chirps and sets other RUs free for high-rate WiFi users. WiRa carefully selects the RU and adopts WiFi frame aggregation to emulate the long LoRa frame. We propose a subframe header mapping method to identify and remove invalid symbols caused by irremovable subframe headers in the aggregated frame. We also propose a mode flipping method to solve Cyclic Prefix (CP) errors, based on our finding that different CP modes have different impacts on the LoRa symbol. To cope with channel dynamics, we design an adaptation mechanism to maximize the goodput with a satisfying SER. We further extend WiRa to one-to-many transmission scenario by concurrently emulating LoRa chirps in different RUs. We implement a prototype of WiRa on the USRP platform and commodity LoRa device. Experiments demonstrate WiRa can efficiently transmit complete LoRa frames with the throughput of 40.037kbps and the SER lower than 0.1.}
}


@article{DBLP:journals/ton/NelsonYJKC24,
	author = {Wilson Ayyanthole Nelson and
                  Sreenivasa Reddy Yeduri and
                  Ajit Jha and
                  Abhinav Kumar and
                  Linga Reddy Cenkeramaddi},
	title = {RL-Based Energy-Efficient Data Transmission Over Hybrid BLE/LTE/Wi-Fi/LoRa
                  UAV-Assisted Wireless Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1951--1966},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3332296},
	doi = {10.1109/TNET.2023.3332296},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NelsonYJKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The lifetime of a UAV-assisted wireless network is determined by the amount of energy consumed by the UAVs during flight, data collection, and transmission to the ground station. Routing protocols are commonly used for data transmission in a communication network. However, because of the mobility of UAVs, using a routing protocol with a single communication technology results in higher delay and more energy consumption in a UAV-assisted wireless network. To overcome this, we propose two reinforcement learning (RL) algorithms, Q-learning and deep Q-network (DQN), for energy-efficient data transmission over a hybrid BLE/LTE/Wi-Fi/LoRa UAV-assisted wireless network. We consider BLE, LTE, Wi-Fi, and LoRa for communication over a UAV-GS link. The RL algorithms take any random network as input and learn the best policy to output the network with less energy consumption. The reward/penalty is chosen in such a way that the network with the highest energy consumption is penalized and the one with the lowest is rewarded, thereby minimizing total network energy consumption. Based on learning, it creates a hybrid BLE/LTE/Wi-Fi/LoRa UAV-assisted wireless network by assigning the best communication technology to a UAV-GS link. Further, we compare the performance of proposed RL algorithms with a rule-based algorithm and random hybrid scheme. In addition, we propose a theoretical framework for constructing hybrid network for both free space and free space multipath path loss models. We demonstrate the performance comparison of the proposed work with the conventional shortest path routing algorithm in terms of network energy consumption and average network delay using extensive results. Finally, the effect of the velocity of the UAV and the number of packets on the performance of the proposed framework is analyzed.}
}


@article{DBLP:journals/ton/TabatabaeeBB24,
	author = {Seyed Mohammadhossein Tabatabaee and
                  Anne Bouillard and
                  Jean{-}Yves Le Boudec},
	title = {Worst-Case Delay Analysis of Time-Sensitive Networks With Deficit
                  Round-Robin},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1967--1982},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3332247},
	doi = {10.1109/TNET.2023.3332247},
	timestamp = {Sun, 19 Jan 2025 13:55:30 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TabatabaeeBB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In feed-forward time-sensitive networks with Deficit Round-Robin (DRR), worst-case delay bounds were obtained by combining Total Flow Analysis (TFA) with the strict service curve characterization of DRR by Tabatabaee et al. The latter is the best-known single server analysis of DRR, however the former is dominated by Polynomial-size Linear Programming (PLP), which improves the TFA bounds and stability region, but was never applied to DRR networks. We first perform the necessary adaptation of PLP to DRR by computing burstiness bounds per-class and per-output aggregate and by enabling PLP to support non-convex service curves. Second, we extend the methodology to support networks with cyclic dependencies: This raises further dependency loops, as, on one hand, DRR strict service curves rely on traffic characteristics inside the network, which comes as output of the network analysis, and on the other hand, TFA or PLP requires prior knowledge of the DRR service curves. This can be solved by iterative methods, however PLP itself requires making cuts, which imposes other levels of iteration, and it is not clear how to combine them. We propose a generic method, called PLP-DRR, for combining all the iterations sequentially or in parallel. We show that the obtained bounds are always valid even before convergence; furthermore, at convergence, the bounds are the same regardless of how the iterations are combined. This provides the best-known worst-case bounds for time-sensitive networks, with general topology, with DRR. We apply the method to an industrial network, where we find significant improvements compared to the state-of-the-art.}
}


@article{DBLP:journals/ton/ShiMCSP24,
	author = {Junyang Shi and
                  Aitian Ma and
                  Xia Cheng and
                  Mo Sha and
                  Xi Peng},
	title = {Adapting Wireless Network Configuration From Simulation to Reality
                  via Deep Learning-Based Domain Adaptation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1983--1998},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3335346},
	doi = {10.1109/TNET.2023.3335346},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiMCSP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today, wireless mesh networks (WMNs) are deployed globally to support various applications, such as industrial automation, military operations, and smart energy. Significant efforts have been made in the literature to facilitate their deployments and optimize their performance. However, configuring a WMN well is challenging because the network configuration is a complex process, which involves theoretical computation, simulation, and field testing, among other tasks. Our study shows that the models for network configuration prediction learned from simulations may not work well in physical networks because of the simulation-to-reality gap. In this paper, we employ deep learning-based domain adaptation to close the gap and leverage a teacher-student neural network and a physical sampling method to transfer the network configuration knowledge learned from a simulated network to its corresponding physical network. Experimental results show that our method effectively closes the gap and increases the accuracy of predicting a good network configuration that allows the network to meet performance requirements from 30.10% to 70.24% by learning robust machine learning models from a large amount of inexpensive simulation data and a few costly field testing measurements.}
}


@article{DBLP:journals/ton/PrasadS24,
	author = {Reshma Prasad and
                  Albert Sunny},
	title = {QoS-Aware Scheduling in 5G Wireless Base Stations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {1999--2011},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3342867},
	doi = {10.1109/TNET.2023.3342867},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PrasadS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G and beyond networks are expected to support flows with varied Quality-of-Service (QoS) requirements under unpredictable traffic conditions. Consequently, designing policies ensuring optimal system utilization in such networks is challenging. Given this, we formulate a long-term time-averaged scheduling problem that minimizes a weighted function of packets dropped by the 5G wireless base station. We then present two policies for this problem. The first is a delay-guaranteed near-optimal policy, and the second is a delay-guaranteed sub-optimal policy that provides flow isolation. We perform extensive simulations to understand the performance of these policies. Further, we study these policies in the presence of a closed-loop flow rate-control mechanism.}
}


@article{DBLP:journals/ton/ZhaoZDLWWGX24,
	author = {Ruijie Zhao and
                  Mingwei Zhan and
                  Xianwen Deng and
                  Fangqi Li and
                  Yanhao Wang and
                  Yijun Wang and
                  Guan Gui and
                  Zhi Xue},
	title = {A Novel Self-Supervised Framework Based on Masked Autoencoder for
                  Traffic Classification},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2012--2025},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3335253},
	doi = {10.1109/TNET.2023.3335253},
	timestamp = {Tue, 08 Oct 2024 17:17:09 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoZDLWWGX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic classification is a critical task in network security and management. Recent research has demonstrated the effectiveness of the deep learning-based traffic classification method. However, the following limitations remain: (1) the traffic representation is simply generated from raw packet bytes, resulting in the absence of important information; (2) the model structure of directly applying deep learning algorithms does not take traffic characteristics into account; and (3) scenario-specific classifier training usually requires a labor-intensive and time-consuming process to label data. In this paper, we introduce a masked autoencoder (MAE) based traffic transformer with multi-level flow representation to tackle these problems. To model raw traffic data, we design a formatted traffic representation matrix with hierarchical flow information. After that, we develop an efficient Traffic Transformer, in which packet-level and flow-level attention mechanisms implement more efficient feature extraction with lower complexity. At last, we utilize MAE paradigm to pre-train our classifier with a large amount of unlabeled data, and perform fine-tuning with a few labeled data for a series of traffic classification tasks. Experiment findings reveal that our method outperforms state-of-the-art methods on five real-world traffic datasets by a large margin. The code is available at https://github.com/NSSL-SJTU/YaTC.}
}


@article{DBLP:journals/ton/XiaoZLZZJSCLL24,
	author = {Jingyu Xiao and
                  Xudong Zuo and
                  Qing Li and
                  Dan Zhao and
                  Hanyu Zhao and
                  Yong Jiang and
                  Jiyong Sun and
                  Bin Chen and
                  Yong Liang and
                  Jie Li},
	title = {FlexNF: Flexible Network Function Orchestration for Scalable On-Path
                  Service Chain Serving},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2026--2041},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3334237},
	doi = {10.1109/TNET.2023.3334237},
	timestamp = {Tue, 13 Aug 2024 14:11:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaoZLZZJSCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable Data Plane (PDP) has been leveraged to offload Network Functions (NFs). Due to its high processing capability, the PDP improves the performance of NFs by more than one order of magnitude. However, the coarse-grained NF orchestration on the PDP makes it hard to fulfill the dynamic service chain demands and unreasonable network function deployment causes long end-to-end delays. In this paper, we propose the Flexible Network Function (FlexNF) deployment on the PDP. First, we design an NF Selection Framework, leveraging the service selection label and re-entering operations for flexible NF orchestration. Second, to support runtime NF reconfiguration to meet the dynamic flow demands, we propose the Per-Flow On-Demand servicing mechanism, where one Match-Action Table with multiple mixed NFs works as different NFs for different flows. Third, to ensure the QoS of flows, on the one hand, we design an SP-aware NF Placement Algorithm to find a near-optimal placement solution that accommodates peak traffic volume while minimizing the overall routing path lengths of all the requests, on the other hand, we design a Two-Stage Service Path Construction Algorithm to provide on-path service while considering load balancing. We implement 15 types of network functions on the P4 switch, based on which we construct the comprehensive experiments. FlexNF reduces the traffic delay by 42.6% while increasing the service chain acceptance rate by five times compared with current solutions. Besides, when switching functions, the FlexNF improves the throughput by 2.04Gbps and reduces the packet loss by 8.269% compared with current solutions.}
}


@article{DBLP:journals/ton/YangG24,
	author = {Yifan Yang and
                  Wei Gong},
	title = {Universal WiFi Backscatter With Ambient Space-Time Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2042--2052},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3336922},
	doi = {10.1109/TNET.2023.3336922},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since backscatter communication has the advantage of low power, it is promising to be widely used in IoT applications. For a space-time stream backscatter, we envision it can efficiently leverage ready-to-use multiform space-time streams in the environment. Previous works have struggled to achieve this because they use various tag-data modulations on various types of ambient space-time stream signals. What’s worse is that it is challenging to identify the properties of a space-time stream in a passive way. To this end, we present STScatter, a WiFi backscatter that is universally applicable for ambient space-time streams. Besides, STScatter achieves efficient tag-data modulation at the symbol-level, allowing space-time stream backscatter to reach high tag-data throughput with multiform excitation. Additionally, unlike previous systems that decoded tag data from both the original ambient and backscattered signal, STScatter can decode ambient data and backscattered data simultaneously from the backscattered signal. STScatter was prototyped using commercial FPGAs and SDRs. Our extensive experiments show that the STScatter is transmitter-agnostic and universal with different ambient space-time streams. STScatter can achieve a goodput of up to 455.9 kbps in single-stream excitation and 460.0 kbps in multi-stream excitation, which is 4.2x and 157.5x better than MOXcatter. Besides, STScatter successfully uses signals from various types of WiFi hardware for excitation and achieves an average tag-data goodput of 293.62 kbps with real-life file-downloading traffic generated by a household WiFi router.}
}


@article{DBLP:journals/ton/AlamAKY24,
	author = {Md. Ibrahim Ibne Alam and
                  Elliot Anshelevich and
                  Koushik Kar and
                  Murat Yuksel},
	title = {Pricing for Efficient Traffic Exchange at IXPs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2053--2068},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3336352},
	doi = {10.1109/TNET.2023.3336352},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AlamAKY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We analyze traffic exchange between Internet Service Providers (ISPs) at an Internet Exchange Point (IXP) as a non-cooperative game with ISPs as self-interested agents. Each ISP has the choice of exchanging traffic either using the shared IXP facilities, or outside the IXP – through their transit providers or private peering. We analyze the efficiency (social cost optimality) of the traffic exchange equilibrium at the IXP taking into consideration the congestion cost experienced by the ISPs at the IXP. To model both non-profit and for profit IXPs, we consider several cases, i) where the IXP does not charge any price to ISPs for the traffic exchanged (zero pricing), ii) when it charges a price that is proportional to the aggregate level of congestion at the IXP (proportional pricing), and iii) when it charges a constant price per unit traffic (constant pricing). Further, we also analyze the profit earned by the IXP under these pricing policies, under two different models of the congestion cost (delay) functions. Simulations conducted using data for actual IXPs obtained from PeeringDB demonstrate that the theoretical bounds derived for social cost and profit optimality at equilibrium (measured as the Price of Anarchy) are fairly tight, and correctly capture the performance trends against the variation of key model parameters. Further, the results show that for proportional pricing, there is an operating price range that attains near-optimal social cost and near-optimal IXP profit simultaneously. We also demonstrate - through both theoretical analysis and simulations - that as compared to zero and constant pricing policies, proportional pricing attains better tradeoff between social cost and IXP profit, and also results in a performance that is more robust to price variations.}
}


@article{DBLP:journals/ton/MengZZWZLR24,
	author = {Qingkai Meng and
                  Yiran Zhang and
                  Shan Zhang and
                  Zhiyuan Wang and
                  Tong Zhang and
                  Hongbin Luo and
                  Fengyuan Ren},
	title = {Switch-Assistant Loss Recovery for {RDMA} Transport Control},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2069--2084},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3336661},
	doi = {10.1109/TNET.2023.3336661},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MengZZWZLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RoCEv2 (RDMA over Converged Ethernet version 2) is the canonical method for deploying RDMA in Ethernet-based datacenters. Traditionally, RoCEv2 runs over the lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, as the scale of the datacenter increases, PFC’s side effects, such as head-of-line blocking, congestion spreading, and pause frame storm, are amplified. Datacenter operators can no longer tolerate these problems. In hence, they are seeking PFC alternatives for RDMA networks. Rather than aiming at the lossless RDMA network, we instead handle packet loss effectively to support RDMA over Ethernet. In this paper, we propose Switch-assistant Loss Recovery (SLR), a switch building block to enhance RoCEv2’s loss recovery. Specifically, SLR-enabled switches send loss notifications to request fast retransmissions. To cooperate with go-back-N retransmission, SLR generates loss notifications only when expected packets (i.e., in-order packets expected by receivers) are dropped and then filters out unexpected packets, which can avoid timeouts and prevent exacerbating congestion. Further, we adapt SLR to multi-bottleneck scenarios by inferring expected packets among multiple switch views. We implement SLR prototypes on commodity programmable switches. Evaluations show that SLR reduces the 99.9th-percentile FCT slowdown by up to 21.6\\times\ncompared to PFC and other state-of-the-arts.}
}


@article{DBLP:journals/ton/YaoZGDH24,
	author = {Xiaopeng Yao and
                  Yunpeng Zhao and
                  Ningtuo Gao and
                  Hongwei Du and
                  Hejiao Huang},
	title = {Causal Related Rumors Controlling in Social Networks of Multiple Information},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2085--2098},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3337774},
	doi = {10.1109/TNET.2023.3337774},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YaoZGDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a huge amount of information generated in online social networks, which is filled with a lot of rumors. The spread of a rumor often leads to the generation of a causal related rumor, and when users believe the first kind of rumor, the probability of being influenced by another causal related rumor is larger. Therefore, the influence probability will change with the process of rumor spreading. In this paper, we design the Causal Rumors Enhance Cascade ( CREC\n) model to describe the spreading process of causal related rumors. Then we attempt to select a set of seed users that minimizes the number of users expected to be influenced by rumors, which we call the Causal Related Rumors Controlling (CRRC) problem. The main challenges of this problem are that the influence probability is constantly changing during the spread process, so the reverse sampling technique cannot be used, and the greedy mechanism is not suitable for massive-scale datasets. For the sake of overcoming these challenges and solving the problem, we put forward the Degree Trigonometric Metrology (DTM) algorithm, which uses the property of three-directed circles in the directed network to select seed nodes. Finally, experiments on three massive-scale datasets show that our algorithm outperforms the other algorithms.}
}


@article{DBLP:journals/ton/WangGJC24,
	author = {Yan Wang and
                  Quansheng Guan and
                  Fei Ji and
                  Weiqi Chen},
	title = {Impact and Analysis of Space-Time Coupling on Slotted {MAC} in UANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2099--2111},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3336459},
	doi = {10.1109/TNET.2023.3336459},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangGJC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The propagation delay is non-negligible in underwater acoustic networks (UANs) since the propagation speed is five orders of magnitude smaller than the speed of light. In this case, space and time factors are strongly coupled to determine the collisions of packet transmissions. To this end, this paper analyzes the impact of space-time coupling on slotted medium access control (MAC). We find that a sending node has specific location-dependent interference slots and slot-dependent interference regions. Thus, the collisions may span multiple slots, leading to both inter-slot and intra-slot collisions. Interestingly, the slot-dependent interference regions could be annulus inside the whole transmission range. It is pointed out that collision-free regions exist when a guard interval in a slot is larger than a packet duration. In this sense, the long slot brings spatial reuse within the transmission range. We then derive the closed-form expressions for the successful transmission probability of Slotted-ALOHA and the upper-bound and the lower-bound for the successful transmission probability in UANs. We further find that the optimal guard interval to reach the peak successful transmission probabilities is not larger than a packet duration, which is much shorter than the existing slot setting in the typical Slotted-ALOHA in the uniformly distributed UANs. Simulation results verify our findings, and also show that the performance of vertical transmissions is more sensitive to the spatial impact than horizontal transmissions in UANs.}
}


@article{DBLP:journals/ton/LuoHCWWCZ24,
	author = {Zhicheng Luo and
                  Qianyi Huang and
                  Xu Chen and
                  Rui Wang and
                  Fan Wu and
                  Guihai Chen and
                  Qian Zhang},
	title = {Spectrum Sensing Everywhere: Wide-Band Spectrum Sensing With Low-Cost
                  {UWB} Nodes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2112--2127},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3342977},
	doi = {10.1109/TNET.2023.3342977},
	timestamp = {Wed, 26 Jun 2024 08:34:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LuoHCWWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum sensing plays a crucial role in spectrum monitoring and management. However, due to the expensive cost of high-speed ADCs, wideband spectrum sensing is a long-standing challenge. In this paper, we present how to transform Ultra-wideband (UWB) devices into a spectrum sensor which can provide wideband spectrum monitoring at a low cost. Compared with the expensive high-speed ADCs which cost at least hundreds of dollars, a UWB device is only several dollars. As the low-cost UWB technology is not originally designed for spectrum sensing, we address the inherent limitations of low-cost devices such as limited memory, low SPI speed and low accuracy, and show how to obtain spectrum occupancy information from the noisy and spurious UWB channel impulse response. In this paper, we present WISE, which not only can give accurate channel occupancy information, but also can precisely estimate the signal power and bandwidth. WISE can also detect fleeting radar signals. We implement WISE and perform extensive evaluations with both controlled experiments and field tests. Results show that WISE can sense up to 900MHz bandwidth with frequency range from 0.5GHz to 7GHz, and the power estimation error is less than 3dB. WISE can also accurately detect busy 5G channels and can classify the encrypted traffic from the channel usage patterns. We believe that WISE provides a new paradigm for low-cost wideband spectrum sensing, which is critical for large-scale fine-grained spectrum monitoring.}
}


@article{DBLP:journals/ton/LiLXXPWLYJD24,
	author = {Shuyue Li and
                  Jing Li and
                  Chaocan Xiang and
                  Wenzheng Xu and
                  Jian Peng and
                  Ziming Wang and
                  Weifa Liang and
                  Xin{-}Wei Yao and
                  Xiaohua Jia and
                  Sajal K. Das},
	title = {Maximizing Network Throughput in Heterogeneous {UAV} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2128--2142},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3347557},
	doi = {10.1109/TNET.2023.3347557},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLXXPWLYJD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we study the deployment of an Unmanned Aerial Vehicle (UAV) network that consists of multiple UAVs to provide emergent communication service for people who are trapped in a disaster area, where each UAV is equipped with a base station that has limited computing capacity and power supply, and thus can only serve a limited number of people. Unlike most existing studies that focused on homogeneous UAVs, we consider the deployment of heterogeneous UAVs where different UAVs have different computing capacities. We study a problem of deploying K\nheterogeneous UAVs in the air to form a temporarily connected UAV network such that the network throughput– the number of users served by the UAVs, is maximized, subject to the constraint that the number of people served by each UAV is no greater than its service capacity. We then propose a novel O\\left({\\sqrt {\\frac {s}{K}}}\\right)\n-approximation algorithm for the problem, where s\nis a given positive integer with 1 \\le s\\le K\n, e.g., s=3\n. We also devise an improved heuristic, based on the approximation algorithm. We finally evaluate the performance of the proposed algorithms. Experimental results show that the numbers of users served by UAVs in the solutions delivered by the proposed algorithms are increased by 25% than state-of-the-arts.}
}


@article{DBLP:journals/ton/XiaoCDOLD24,
	author = {Tingting Xiao and
                  Chen Chen and
                  Mianxiong Dong and
                  Kaoru Ota and
                  Lei Liu and
                  Schahram Dustdar},
	title = {Multi-Agent Reinforcement Learning-Based Trading Decision-Making in
                  Platooning-Assisted Vehicular Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2143--2158},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3342020},
	doi = {10.1109/TNET.2023.3342020},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaoCDOLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Utilizing the stable underlying and cloud-native functions of vehicle platoons allows for flexible resource provisioning in environments with limited infrastructure, particularly for dynamic and compute-intensive applications. To maximize this potential, we propose the creation of a trading market to encourage interactions between service supporters (vehicle platoons) and requesters (task vehicles). Current trading decisions based on game and negotiations can lead to unpredicted handover costs and increased communication overhead in dynamic environments. Moreover, existing research tends to overlook a mutually beneficial trading philosophy by focusing on either the service supporters’ profitability or the user experience of resource-restrained requesters. Addressing these issues, we introduce a multi-objective optimization problem to model environmental dynamics and uncertainty, aiming to maximize both platoons’ and task vehicles’ long-term utilities while maintaining a satisfactory service access ratio. To tackle the problem within acceptable time frames, we develop a global-local training architecture, incorporating a hybrid action space and prioritized sampling into a multi-agent reinforcement learning algorithm that utilizes a twin delayed deep deterministic gradient (GL-HPMATD3). This approach facilitates consensus in the trading market on key issues, including service request selection, resource allocation, and trading pricing. Through extensive experimentation and comparison, we demonstrate our mechanism’s superior performance in convergence, service access ratio, player utility, execution latency, and trading pricing relative to several state-of-the-art and baseline methods.}
}


@article{DBLP:journals/ton/HuHW24,
	author = {Zhifeng Hu and
                  Chong Han and
                  Xudong Wang},
	title = {Deep Reinforcement Learning Based Cross-Layer Design in Terahertz
                  Mesh Backhaul Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2159--2173},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3342837},
	doi = {10.1109/TNET.2023.3342837},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Supporting ultra-high data rates and flexible reconfigurability, Terahertz (THz) mesh networks are attractive for next-generation wireless backhaul systems that empower the integrated access and backhaul (IAB). In THz mesh backhaul networks, the efficient cross-layer routing and long-term resource allocation is yet an open problem due to dynamic traffic demands as well as possible link failures caused by the high directivity and high non-line-of-sight (NLoS) path loss of THz spectrum. In addition, unpredictable data traffic and the mixed integer programming property with the NP-hard nature further challenge the effective routing and long-term resource allocation design. In this paper, a deep reinforcement learning (DRL) based cross-layer design in THz mesh backhaul networks (DEFLECT) is proposed, by considering dynamic traffic demands and possible sudden link failures. In DEFLECT, a heuristic routing metric is first devised to facilitate resource efficiency (RE) enhancement regarding energy and sub-array usages. Furthermore, a DRL based resource allocation algorithm is developed to realize long-term RE maximization and fast recovery from broken links. Specifically in the DRL method, the exploited multi-task structure cooperatively benefits joint power and sub-array allocation. Additionally, the leveraged hierarchical architecture realizes tailored resource allocation for each base station and learned knowledge transfer for fast recovery. Simulation results show that DEFLECT routing consumes less resource, compared to the minimal hop-count metric. Moreover, unlike conventional DRL methods causing packet loss and second-level latency, DEFLECT DRL realizes the long-term RE maximization with no packet loss and millisecond-level latency, and recovers resource-efficient backhaul from broken links within 1s.}
}


@article{DBLP:journals/ton/ZhangZXY24,
	author = {Qianyu Zhang and
                  Gongming Zhao and
                  Hongli Xu and
                  Peng Yang},
	title = {XAgg: Accelerating Heterogeneous Distributed Training Through XDP-Based
                  Gradient Aggregation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2174--2188},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3339524},
	doi = {10.1109/TNET.2023.3339524},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growth of model/dataset/system size for distributed model training in datacenters, the widely used Parameter Server (PS) architecture suffers from communication bottleneck of gradient transmission. Recent works attempt to utilize programmable switches to implement in-network gradient aggregation and alleviate communication bottlenecks on PSs. Due to the limited on-chip memory of programmable switches, gradient transmission requires strict synchronization to achieve ideal aggregation performance. However, the distributed training system is usually heterogeneous in datacenters (e.g., computation and bandwidth heterogeneity), and the gradient will reach the aggregation nodes asynchronously, thereby seriously affecting the aggregation performance. To solve the above issue, we propose XAgg, which accelerates heterogeneous gradient aggregation by deploying the eXpress Data Path (XDP) based aggregator on servers. Specifically, the abundant idle memory on servers can cache the entire gradient, so as to effectively deal with asynchronous gradient transmission in heterogeneous scenarios. Moreover, XDP can provide high-performance and low-latency gradient aggregation. We conduct microbenchmark and testbed with real-world DNN models and datasets. Experimental results show that XAgg improves the gradient aggregation throughput by 3.3 \\times\ncompared with TCP-based aggregation, reaching 100 Gbps with 10 CPU cores. In addition, XAgg reduces communication time by 49%-82% compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/ChenZZSH24,
	author = {Baojun Chen and
                  Jiawen Zhu and
                  Shuai Zhang and
                  Weiqiang Sun and
                  Weisheng Hu},
	title = {Performances of Traffic Offloading in Data Center Networks With Steerable
                  Free-Space Optical Communications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2189--2204},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3340713},
	doi = {10.1109/TNET.2023.3340713},
	timestamp = {Thu, 04 Jul 2024 22:03:59 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenZZSH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Steerable free-space optics (FSO) communications are flexible in link reconfiguration (LR), and easy to deploy, especially when the available physical space is limited. Thus it is considered as a good complement to the wired network in data centers. In this paper, we are interested in understanding the role of FSO when used as a hotspot offloading technique in data center networks. We aim to quantify the performance of FSO-based hotspot offloading, in the presence of link impairment, failed negotiations and reconfiguration overhead. We model this steerable FSO-based traffic offloading process as a vacation queueing system with limited service discipline. A general expression for the average delay is derived, and then equations for three different hybrid automatic repeat request (HARQ) protocols are separately derived. To overcome the inherent difficulty of calculating the average delay with limited service, efficient low-complexity approximations are developed. We then show that the system delay increases approximately linearly with the traffic rate until a point, beyond which it grows rapidly. Further, we analyze the effect of various factors on the delay performance and offer insights for the design configurations to achieve the desired performance. The simulation results verify the validity of the theoretical analysis.}
}


@article{DBLP:journals/ton/ShiZQ24,
	author = {Shouqian Shi and
                  Xiaoxue Zhang and
                  Chen Qian},
	title = {Concurrent Entanglement Routing for Quantum Networks: Model and Designs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2205--2220},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3343748},
	doi = {10.1109/TNET.2023.3343748},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum entanglement enables important computing applications such as quantum key distribution. Based on quantum entanglement, quantum networks are built to provide long-distance secret sharing between two remote communication parties. Establishing a multi-hop quantum entanglement exhibits a high failure rate, and existing quantum networks rely on trusted repeater nodes to transmit quantum bits. However, when the scale of a quantum network increases, it requires end-to-end multi-hop quantum entanglements in order to deliver secret bits without letting the repeaters know the secret bits. This work focuses on the entanglement routing problem, whose objective is to build long-distance entanglements via untrusted repeaters for concurrent source-destination pairs through multiple hops. Different from existing work that analyzes the traditional routing techniques on special network topologies, we present a comprehensive entanglement routing model that reflects the differences between quantum networks and classical networks as well as a new entanglement routing algorithm that utilizes the unique properties of quantum networks. Evaluation results show that the proposed algorithm Q-CAST increases the number of successful long-distance entanglements by a big margin compared to other methods. The model and simulator developed by this work may encourage more network researchers to study the entanglement routing problem.}
}


@article{DBLP:journals/ton/SunLCH24,
	author = {Peng Sun and
                  Guocheng Liao and
                  Xu Chen and
                  Jianwei Huang},
	title = {A Socially Optimal Data Marketplace With Differentially Private Federated
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2221--2236},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3351864},
	doi = {10.1109/TNET.2024.3351864},
	timestamp = {Sun, 19 Jan 2025 13:55:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunLCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables multiple data owners to collaboratively train machine learning (ML) models for different model requesters while keeping data localized. Thus, FL can mitigate privacy leakage in conventional data marketplaces for ML applications requiring raw data trading for centralized model training. Nevertheless, data owners involved in FL may still suffer potential privacy leakage from gradient exposure to the model requesters. In this work, we advocate a novel data marketplace with differentially private federated learning (DPFL) to reduce such threats and maximize the social welfare. Designing such a marketplace involves several challenges. First, it is difficult to determine the privacy budget that a data owner should choose for a model requester, since they have conflicting objectives and private utility/cost information. Second, each data owner sustains privacy costs from his friends’ participation in DPFL due to data correlations, which introduces a negative externality to the market. We design a social-aware iterative double auction (SARDA) mechanism to resolve these challenges and achieve socially optimal market operation. SARDA employs a broker to coordinate the interactions between data owners and model requesters and induces them to truthfully report by iteratively updating the allocation and pricing rules. Moreover, SARDA accounts for the negative externality by incorporating others’ bids to reimburse each data owner. We show that SARDA achieves the optimal social performance and creates up to 60% higher social welfare than the social-agnostic benchmark.}
}


@article{DBLP:journals/ton/MirGVG24,
	author = {Muhammad Sarmad Mir and
                  Borja Genov{\'{e}}s Guzm{\'{a}}n and
                  Ambuj Varshney and
                  Domenico Giustiniano},
	title = {LiFi for Low-Power and Long-Range {RF} Backscatter},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2237--2252},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3344316},
	doi = {10.1109/TNET.2023.3344316},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MirGVG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Light bulbs have been recently explored to design Light Fidelity (LiFi) communication to battery-free tags, thus complementing Radiofrequency (RF) backscatter in the uplink. In this paper, we show that LiFi and RF backscatter are complementary and have unexplored interactions. We introduce PassiveLiFi, a battery-free system that uses LiFi to transmit RF backscatter at a meagre power budget. We address several challenges on the system design in the LiFi transmitter, the tag and the RF receiver. We design the first LiFi transmitter that implements a chirp spread spectrum (CSS) using the visible light spectrum. We use a small bank of solar cells for both communication and harvesting, and reconfigure them based on the amount of harvested energy and desired data rate. We further alleviate the low responsiveness of solar cells with a new low-power receiver design in the tag. We design and implement a novel technique for embedding multiple symbols in the RF backscatter based on delayed chirps. Experimental results with an RF carrier of 17dBm show that we can generate RF backscatter with a range of 92.1 meters/ \\mu \\text{W} consumed in the tag, which is almost double with respect to prior work.}
}


@article{DBLP:journals/ton/ZhangHHZRCWM24,
	author = {Tao Zhang and
                  Ran Huang and
                  Jiawei Huang and
                  Shaojun Zou and
                  Chang Ruan and
                  Kai Chen and
                  Jianxin Wang and
                  Geyong Min},
	title = {Taming the Aggressiveness of Heterogeneous {TCP} Traffic in Data Center
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2253--2268},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3347048},
	doi = {10.1109/TNET.2023.3347048},
	timestamp = {Sun, 19 Jan 2025 13:55:30 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangHHZRCWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To achieve low latency and high link utilization, ECN-based transport protocols (i.e., DCTCP) are widely deployed in data center networks (DCN). In multi-tenant environment, however, the newly introduced ECN-enabled TCP greatly impairs the performance of applications with out-dated and misconfigured TCP stacks. The reason is that the ECN-enabled switch fails to treat the mixed TCP traffic fairly, resulting in the distinguished performance gap between the ECN-enabled and ECN-disabled TCPs. This paper proposes DDT (Dual Dynamic Thresholds), an active queue management algorithm (AQM) to achieve the flow-level fairness for coexisting heterogeneous TCP traffic. DDT monitors the switch queue in real time, and dynamically tunes the distance between ECN-marking and packet-dropping thresholds to mitigate the aggressiveness difference between the ECN-enabled and ECN-disabled TCPs. The results of real implementations and large-scaled simulations show that DDT elegantly fills the aggressiveness gap of heterogeneous TCP traffic without disturbing their own control loops, while only introducing acceptable deployment overhead at switch.}
}


@article{DBLP:journals/ton/MianoSRRA24,
	author = {Sebastiano Miano and
                  Alireza Sanaee and
                  Fulvio Risso and
                  G{\'{a}}bor R{\'{e}}tv{\'{a}}ri and
                  Gianni Antichi},
	title = {Morpheus: {A} Run Time Compiler and Optimizer for Software Data Planes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2269--2284},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3346286},
	doi = {10.1109/TNET.2023.3346286},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MianoSRRA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {State-of-the-art approaches to design, develop and optimize software packet-processing programs are based on static compilation: the compiler’s input is a description of the forwarding plane semantics and the output is a binary that can accommodate any control plane configuration or input traffic. In this paper, we demonstrate that tracking control plane actions and packet-level traffic dynamics at run time opens up new opportunities for code specialization. We present Morpheus, a system working alongside static compilers that continuously optimizes the targeted networking code. We introduce a number of new techniques, from static code analysis to adaptive code instrumentation, and we implement a toolbox of domain specific optimizations that are not restricted to a specific data plane framework or programming language. We apply Morpheus to several systems, from eBPF and DPDK programs including Katran, Meta’s production-grade load balancer to container orchestration solutions such a Kubernets. We compare Morpheus to state-of-the-art optimization frameworks and show that it can bring up to 2x throughput improvement, while halving the 99th percentile latency.}
}


@article{DBLP:journals/ton/HouXWZ24,
	author = {Ningning Hou and
                  Xianjin Xia and
                  Yifeng Wang and
                  Yuanqing Zheng},
	title = {One Shot for All: Quick and Accurate Data Aggregation for LPWANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2285--2298},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3353792},
	doi = {10.1109/TNET.2024.3353792},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HouXWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents our design and implementation of a fast and accurate data aggregation strategy for LoRa networks named One-shot. To facilitate data aggregation, One-shot assigns distinctive chirps for different LoRa nodes to encode individual data. One-shot coordinates the nodes to concurrently transmit encoded packets. Receiving concurrent transmissions, One-shot gateway examines the frequencies of superimposed chirp signals and computes application-defined aggregate functions (e.g., sum, max, count, etc.), which give a quick overview of sensor data in a large monitoring area. One-shot develops techniques to handle a series of practical challenges involved in frequency and time synchronization of concurrent chirps. We evaluate the effectiveness of One-shot with extensive experiments. Results show that One-shot substantially outperforms state-of-the-art data aggregation methods in terms of aggregation accuracy as well as query efficiency.}
}


@article{DBLP:journals/ton/WangST24,
	author = {Xin Wang and
                  Hong Shen and
                  Hui Tian},
	title = {Scheduling Coflows in Hybrid Optical-Circuit and Electrical-Packet
                  Switches With Performance Guarantee},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2299--2314},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3354245},
	doi = {10.1109/TNET.2024.3354245},
	timestamp = {Sun, 19 Jan 2025 13:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scheduling of coflows, each a collection of parallel flows sharing the same objective, is an important task of data transmission that arises in the networks supporting data-intensive applications such as data center networks (DCNs). The hybrid switch design combining the optical circuit switch (OCS) and electrical packet switch (EPS) for transmitting high-volume and low-volume traffic separately has received considerable research attention. To support this design, efficient scheduling of coflows on hybrid network links is crucial for reducing the overall communication time. However, because it needs to consider both reconfiguration delay of circuit switching in the OCS and bandwidth limitation of packet switching in the EPS, coflow scheduling on hybrid network links is more challenging than on monotonic network links of either OCS or EPS. The existing coflow scheduling algorithms in hybrid switches are all heuristic and provide no performance guarantees. In this work, we first propose an approximation algorithm with a worst-case performance guarantee of 2\\tau , where \\tau \\le N is the maximum number of non-zero elements of each row and column of coflow’s demand matrix, for single coflow scheduling in an N\\times N hybrid switch to minimize the coflow completion time (CCT). We then extend the algorithm for scheduling multiple coflows to minimize the total weighted CCT with a provable performance guarantee of \\mu \\tau _{\\max } , where \\mu =4M\\cdot \\frac {w_{\\max }}{w_{\\min }} , \\tau _{\\max }=\\max _{1\\le m\\le M}\\tau _{m}\\leq N , w_{\\max } and w_{\\min } are respectively the maximum and minimum weights of the M coflows. Extensive simulations using Facebook data traces show that our algorithms outperform the state-of-the-art coflow scheduling schemes. Specifically, our algorithms transmit a single coflow up to 1.08\\times faster than Solstice (hybrid switch) and 1.42\\times faster than Reco-Sin (pure OCS), and multiple coflows up to 1.11\\times faster than Solstice and 1.17\\times faster than Reco-Mul+ (pure OCS).}
}


@article{DBLP:journals/ton/LiYPWC24,
	author = {Fuliang Li and
                  Qianchen Yuan and
                  Tian Pan and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {MTU-Adaptive In-Band Network-Wide Telemetry},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2315--2330},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3351672},
	doi = {10.1109/TNET.2024.3351672},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYPWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-band network telemetry (INT) allows for fine-grained network monitoring, without requiring communication with the controller at each hop. Existing INT-based network-wide telemetry systems achieve low-overhead monitoring with non-overlapping path planning algorithms. However, these systems do not constrain the length of the generated probing paths, which will lead to packet loss when the size of the packet with collected telemetry data exceeds the MTU limit. To address this issue, we propose MTU-adaptive path segmentation algorithms step by step in this paper. Initially, we present two single-path planning algorithms: the INT-optimize algorithm, which produces a single path that covers the entire network with the lowest southbound communication overhead, and the INT-low-cost algorithm, which further accelerates the INT-optimize. Next, to consider the MTU limit, we propose the single-MTU adaptive INT-Segment algorithm to divide the single long path generated in the previous step into multiple path segments. In addition, we generalize the MTU-adaptive network telemetry problem and propose a multi-MTU adaptive INT-Segment solution to achieve high-performance network telemetry in networks with multiple MTU settings. Extensive evaluations demonstrate that our proposed MTU-adaptive solutions can achieve sub-second network-wide telemetry for large-scale networks, with less than 2.9ms to calculate the probing paths for an 18-pod FatTree. Furthermore, our multi-MTU adaptive INT-Segment solution significantly reduces the number of INT Sinks and INT Sources by 13.25%-42.39% when deployed in multi-MTU networks while maintaining stable telemetry data collection time. Compared with the state-of-the-art INT-path, our solution adapts the probing path to the network MTU limit, producing a telemetry data collection efficiency improvement of 10%-94%.}
}


@article{DBLP:journals/ton/GongWLC24,
	author = {Wei Gong and
                  Haoyu Wang and
                  Siyi Li and
                  Si Chen},
	title = {{GLAC:} High-Precision Tracking of Mobile Objects With {COTS} {RFID}
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2331--2343},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2023.3348950},
	doi = {10.1109/TNET.2023.3348950},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GongWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents GLAC, the first 3D localization system that enables millimeter-level object manipulation for robotics using only COTS RFID devices. The key insight of GLAC is that mobility reduces ambiguity (One-to-many mapping relationship between phase and distance) and thus improves accuracy. Unlike state-of-the-art systems that require extra time or hardware to boost performance, it draws the power of modeling mobility in a delicate way. In particular, we build a novel framework for real-time tracking using the Hidden Markov Model (HMM). In our framework, multiple Kalman filters are designed to take a single phase observation for updating mobility states, and a fast inference algorithm is proposed to efficiently process an exponentially large number of candidate trajectories. We prototype GLAC with only UHF tags and a commercial reader of four antennas. Comprehensive experiments show that the median position accuracies of x/y/z dimensions are within 1 cm for both LoS and NLoS cases. The median position accuracy for slow-moving targets is 0.41 cm, which is 2.2\\times , 17.3\\times , and 14.9\\times better than TurboTrack, Tagoram, and RF-IDraw, respectively. Also, its median velocity accuracy is at least 20\\times better than all three competitors for fast-moving targets. Besides accuracy, it achieves more than 4\\times localization time gains over state-of-the-art systems.}
}


@article{DBLP:journals/ton/HuCPFXH24,
	author = {Pihe Hu and
                  Yu Chen and
                  Ling Pan and
                  Zhixuan Fang and
                  Fu Xiao and
                  Longbo Huang},
	title = {Multi-User Delay-Constrained Scheduling With Deep Recurrent Reinforcement
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2344--2359},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3359911},
	doi = {10.1109/TNET.2024.3359911},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuCPFXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-user delay-constrained scheduling is a crucial challenge in various real-world applications, such as wireless communication, live streaming, and cloud computing. The scheduler must make real-time decisions to guarantee both delay and resource constraints simultaneously, without prior information on system dynamics that can be time-varying and challenging to estimate. Additionally, many practical scenarios suffer from partial observability issues due to sensing noise or hidden correlation. To address these challenges, we propose a deep reinforcement learning (DRL) algorithm called Recurrent Softmax Delayed Deep Double Deterministic Policy Gradient ( \\mathtt {RSD4} ) (https://github.com/hupihe/RSD4), which is a data-driven method based on a Partially Observed Markov Decision Process (POMDP) formulation. \\mathtt {RSD4} guarantees resource and delay constraints by Lagrangian dual and delay-sensitive queues, respectively. It also efficiently handles partial observability with a memory mechanism enabled by the recurrent neural network (RNN). Moreover, it introduces user-level decomposition and node-level merging to support large-scale multihop scenarios. Extensive experiments on simulated and real-world datasets demonstrate that \\mathtt {RSD4} is robust to system dynamics and partially observable environments and achieves superior performance over existing methods.}
}


@article{DBLP:journals/ton/ZhangZWLM24,
	author = {Huanhuan Zhang and
                  Anfu Zhou and
                  Guangping Wang and
                  Chaoyue Li and
                  Huadong Ma},
	title = {Toward Optimal Live Video Streaming QoE: {A} Deep Feature-Fusion Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2360--2375},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3351832},
	doi = {10.1109/TNET.2024.3351832},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZWLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maximizing the quality of experience (QoE) for live video streaming is a long-standing challenge. Traditional video transport protocols, represented by a few deterministic rules, can hardly adapt to the highly heterogeneous and dynamic modern Internet environments. Emerging learning-based algorithms have demonstrated the potential to meet above challenge. However, our measurement study reveals an alarming long-tail performance issue: these learning-based algorithms tend to be bottlenecked by occasional catastrophic events due to the built-in exploration mechanisms. In this work, we propose Loki-plus, which improves the robustness of learning-based model by coherently integrating it with a rule-based algorithm. To enable integration at deep feature level, we first reverse-engineer the rule-based algorithm into an equivalent “black-box” neural network, and then devise a transformer-based continual learning model with effective historical feature reservation. Then, we design a network feature-aware fusion mechanism to fuse the two models in a deep manner. We train Loki-plus in a full-fledged live video system through online learning, and evaluate it over massive video sessions, in comparison to state-of-the-art rule-based and learning-based solutions. The results show that Loki-plus improves not only the average but also the tail performance substantially.}
}


@article{DBLP:journals/ton/QiaoYZR24,
	author = {Nan Qiao and
                  Sheng Yue and
                  Yongmin Zhang and
                  Ju Ren},
	title = {PoPeC: PAoI-Centric Task Offloading With Priority Over Unreliable
                  Channels},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2376--2390},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3350198},
	doi = {10.1109/TNET.2024.3350198},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QiaoYZR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Freshness-aware computation offloading has garnered increasing attention recently in the realm of edge computing, driven by the need to promptly obtain up-to-date information and mitigate the transmission of outdated data. However, most of the existing works assume that channels are reliable, neglecting the intrinsic fluctuations and uncertainty in wireless communication. More importantly, offloading tasks typically have diverse freshness requirements. Accommodation of various task priorities in the context of freshness-aware task scheduling and resource allocation remains an open and unresolved problem. To overcome these limitations, we cast the freshness-aware task offloading problem as a multi-priority optimization problem, considering the unreliability of wireless channels, prioritized users, and the heterogeneity of edge servers. Building upon the nonlinear fractional programming and the ADMM-Consensus method, we introduce a joint resource allocation and task offloading algorithm to solve the original problem iteratively. In addition, we devise a distributed asynchronous variant for the proposed algorithm to further enhance its communication efficiency. We rigorously analyze the performance and convergence of our approaches and conduct extensive simulations to corroborate their efficacy and superiority over the existing baselines.}
}


@article{DBLP:journals/ton/ShiLJ24,
	author = {Ming Shi and
                  Xiaojun Lin and
                  Lei Jiao},
	title = {Combining Regularization With Look-Ahead for Competitive Online Convex
                  Optimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2391--2405},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3350990},
	doi = {10.1109/TNET.2024.3350990},
	timestamp = {Fri, 24 Jan 2025 11:36:55 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been significant interest in leveraging limited look-ahead to achieve low competitive ratios for online convex optimization (OCO). However, existing online algorithms (such as Averaging Fixed Horizon Control (AFHC)) that can leverage look-ahead to reduce the competitive ratios still produce competitive ratios that grow unbounded as the coefficient ratio (i.e., the maximum ratio of the switching-cost coefficient and the service-cost coefficient) increases. On the other hand, the regularization method can attain a competitive ratio that remains bounded when the coefficient ratio is large, but it does not benefit from look-ahead. In this paper, we propose a new algorithm, called Regularization with Look-Ahead (RLA), that can get the best of both AFHC and the regularization method, i.e., its competitive ratio decreases with the look-ahead window size when the coefficient ratio is small, and remains bounded when the coefficient ratio is large. Moreover, we provide a matching lower bound for the competitive ratios of all online algorithms with look-ahead, which differs from the achievable competitive ratio of RLA within a factor that only depends on the problem size. Further, the competitive analysis of RLA involves a non-trivial generalization of online primal-dual analysis to the case with look-ahead.}
}


@article{DBLP:journals/ton/LiCYHD24,
	author = {Feng Li and
                  Yuqi Chai and
                  Huan Yang and
                  Pengfei Hu and
                  Lingjie Duan},
	title = {Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing:
                  From Off-Line and On-Line Perspectives},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2406--2421},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3355169},
	doi = {10.1109/TNET.2024.3355169},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiCYHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to incentivize strategic workers using limited budget is a very fundamental problem for crowdsensing systems; nevertheless, since the sensing abilities of the workers may not always be known as prior knowledge due to the diversities of their sensor devices and behaviors, it is difficult to properly select and pay the unknown workers. Although the uncertainties of the workers can be addressed by the standard Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through a trade-off between exploration and exploitation, we may not have sufficient budget to enable the trade-off among the individual workers, especially when the number of the workers is huge while the budget is limited. Moreover, the standard CMAB usually assumes the workers always stay in the system, whereas the workers may join in or depart from the system over time, such that what we have learnt for an individual worker cannot be applied after the worker leaves. To address the above challenging issues, in this paper, we first propose an off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in leveraging the exploration-exploitation trade-off in an elaborately partitioned context space instead of the individual workers, to effectively incentivize the massive unknown workers with a very limited budget. We also extend the above basic idea to the on-line setting where unknown workers may join in or depart from the systems dynamically, and propose an on-line version of the CACI mechanism. Specifically, by the exploitation-exploration trade-off in the context space, we learn to estimate the sensing ability of any unknown worker (even it never appeared in the system before) according to its context information. We perform rigorous theoretical analysis to reveal the upper bounds on the regrets of our CACI mechanisms and to prove their truthfulness and individual rationality, respectively. Extensive experiments on both synthetic and real datasets are also conducted to verify the efficacy of our mechanisms.}
}


@article{DBLP:journals/ton/JinBZZZL24,
	author = {Xin Jin and
                  Zhihao Bai and
                  Zhen Zhang and
                  Yibo Zhu and
                  Yinmin Zhong and
                  Xuanzhe Liu},
	title = {DistMind: Efficient Resource Disaggregation for Deep Learning Workloads},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2422--2437},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3355010},
	doi = {10.1109/TNET.2024.3355010},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JinBZZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) systems suffer from low resource utilization due to 1) monolithic server model that tightly couples compute and memory; and 2) limited sharing between different inference applications, and across inference and training, because of strict service level objectives (SLOs). To address this problem, we present DistMind, a disaggregated DL system that enables efficient multiplexing of DL applications with near-optimal resource utilization. DistMind decouples compute from host memory, and exposes the abstractions of a GPU pool and a memory pool, each of which can be independently provisioned. The key challenge is to dynamically allocate GPU resources to different applications based on their real-time demands while meeting strict SLOs. We tackle this challenge by exploiting the power of high-speed 100 Gbps networks, and design three-stage pipelining, cache-aware load balancing, and DNN-aware sharding mechanisms based on the characteristics of DL workloads, to achieve millisecond-scale application loading overhead and improve system efficiency. We have implemented a prototype of DistMind and integrated it with PyTorch. Experimental results on AWS EC2 show that DistMind achieves near 100% resource utilization, and compared with NVIDIA MPS and Ray, DistMind improves the throughput by up to 279% and reduces the inference latency by up to 94%.}
}


@article{DBLP:journals/ton/PaulDM24,
	author = {Avishek Paul and
                  Subhasish Dhal and
                  Neenu Mary},
	title = {Relaying Vehicle Selection Protocol for a Road Intersection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2438--2446},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3354970},
	doi = {10.1109/TNET.2024.3354970},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PaulDM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An autonomous Vehicle to Vehicle (V2V) communication mode, which allows V2V communication in out of eNB (Evolved Node B) coverage areas, has been recently introduced into the Long Term Evolution (LTE) standard. Recent research has studied the performance of this LTE V2V autonomous mode for an urban intersection use case, where non-line-of-sight (NLOS) communication links are present and it is observed that the overall message transmission performance in LTE V2V is found to degrade drastically in NLOS link conditions. To improve the performance, a vehicle-assisted relaying strategy was proposed. In this scheme, at each time instance, distance of each vehicle from intersection point is computed. Then the vehicle close to the intersection point is nominated as relaying vehicle and will rebroadcast the BSMs (Basic Safety Message) in a selective manner. However, security aspects while relaying the BSM is not being addressed in the suggested model. We cannot reject the possibility that a vehicle can be malicious. A malicious vehicle can manipulate its distance from the intersection point, thereby nominating itself as the relaying vehicle with malicious motives such as to use extra infrastructural faciltiy, curiosity in the messages sent by other vehicles, etc. This work proposes a secure algorithm for finding the most appropriate relaying vehicle to address this security vulnerability so that a vehicle can no longer falsify its location information.}
}


@article{DBLP:journals/ton/ThijmBGW24,
	author = {Timothy Alberdingk Thijm and
                  Ryan Beckett and
                  Aarti Gupta and
                  David Walker},
	title = {Kirigami, the Verifiable Art of Network Cutting},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2447--2462},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3360371},
	doi = {10.1109/TNET.2024.3360371},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ThijmBGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satisfiability Modulo Theories (SMT)-based analysis allows exhaustive reasoning over complex distributed control plane routing behaviors, enabling verification of converged routing states under arbitrary conditions. To improve scalability of SMT solving, we introduce a modular verification approach to network control plane verification, where we cut a network into smaller fragments. Users specify an annotated cut which describes how to generate these fragments from the monolithic network, and we verify each fragment independently, using these annotations to define assumptions and guarantees over fragments akin to assume-guarantee reasoning. We prove that any converged states of the fragments are converged states of the monolithic network, and there exists an annotated cut that can generate fragments corresponding to any converged state of the monolithic network. We implement this procedure as Kirigami, an extension of the network verification language and tool NV, and evaluate it on industrial topologies with synthesized policies. We observe a 10x improvement in end-to-end NV verification time, with SMT solve time improving by up to 6 orders of magnitude.}
}


@article{DBLP:journals/ton/KohliAABDMHKGMFCDVZ24,
	author = {Manav Kohli and
                  Abhishek Adhikari and
                  Gulnur Avci and
                  Sienna Brent and
                  Aditya Dash and
                  Jared Moser and
                  Sabbir Hossain and
                  Igor Kadota and
                  Carson Garland and
                  Shivan Mukherjee and
                  Rodolfo Feick and
                  Dmitry Chizhik and
                  Jinfeng Du and
                  Reinaldo A. Valenzuela and
                  Gil Zussman},
	title = {Outdoor-to-Indoor 28 GHz Wireless Measurements in Manhattan: Path
                  Loss, Environmental Effects, and 90{\%} Coverage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2463--2478},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3355842},
	doi = {10.1109/TNET.2024.3355842},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KohliAABDMHKGMFCDVZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outdoor-to-indoor signal propagation poses significant challenges to millimeter-wave link budgets. To gain insight into outdoor-to-indoor millimeter-wave at 28GHz, we conducted an extensive measurement campaign consisting of over 2,200 link measurements in West Harlem, New York City, covering seven highly diverse buildings. A path loss model constructed over all measured links shows an average of 30dB excess loss over free space at distances beyond 50m. We find the type of glass to be the dominant factor in outdoor-to-indoor loss, with 20dB observed difference between grouped scenarios with low- and high-loss glass. Other factors such as the presence of scaffolding, tree foliage, or elevated subway tracks, as well as difference in floor height are also found to have a 5–10dB impact. We show that for urban buildings with high-loss glass, outdoor-to-indoor downlink capacity up to 400Mb/s is supported for 90% of indoor customer premises equipment by a base station up to 40m away. For buildings with low-loss glass, such as our case study covering multiple classrooms of a public school, downlink capacity over 2.8/1.4Gb/s is possible from a base station 57/133m away within line-of-sight. We expect these results to help inform the planning of millimeter-wave networks targeting outdoor-to-indoor deployments in dense urban environments, as well as provide insight into the development of scheduling and beam management algorithms.}
}


@article{DBLP:journals/ton/GuVCAH24,
	author = {Zhouyou Gu and
                  Branka Vucetic and
                  Kishore Chikkam and
                  Pasquale Aliberti and
                  Wibowo Hardjawana},
	title = {Graph Representation Learning for Contention and Interference Management
                  in Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2479--2494},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3355935},
	doi = {10.1109/TNET.2024.3355935},
	timestamp = {Sun, 19 Jan 2025 13:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuVCAH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Restricted access window (RAW) in Wi-Fi 802.11ah networks manages contention and interference by grouping users and allocating periodic time slots for each group’s transmissions. We will find the optimal user grouping decisions in RAW to maximize the network’s worst-case user throughput. We review existing user grouping approaches and highlight their performance limitations in the above problem. We propose formulating user grouping as a graph construction problem where vertices represent users and edge weights indicate the contention and interference. This formulation leverages the graph’s max cut to group users and optimizes edge weights to construct the optimal graph whose max cut yields the optimal grouping decisions. To achieve this optimal graph construction, we design an actor-critic graph representation learning (AC-GRL) algorithm. Specifically, the actor neural network (NN) is trained to estimate the optimal graph’s edge weights using path losses between users and access points. A graph cut procedure uses semidefinite programming to solve the max cut efficiently and return the grouping decisions for the given weights. The critic NN approximates user throughput achieved by the above-returned decisions and is used to improve the actor. Additionally, we present an architecture that uses the online-measured throughput and path losses to fine-tune the decisions in response to changes in user populations and their locations. Simulations show that our methods achieve 30\\%\\sim 80\\% higher worst-case user throughput than the existing approaches and that the proposed architecture can further improve the worst-case user throughput by 5\\%\\sim 30\\% while ensuring timely updates of grouping decisions.}
}


@article{DBLP:journals/ton/MaatoukABLBZ24,
	author = {Ali Maatouk and
                  Fadhel Ayed and
                  Shi Biao and
                  Wenjie Li and
                  Harvey Bao and
                  Enrico Zio},
	title = {A Framework for the Evaluation of Network Reliability Under Periodic
                  Demand},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2495--2510},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3354516},
	doi = {10.1109/TNET.2024.3354516},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaatoukABLBZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study network reliability in relation to a periodic time-dependent utility function that reflects the system’s functional performance. When an anomaly occurs, the system incurs a loss of utility that depends on the anomaly’s timing and duration. We analyze the long-term average utility loss by considering exponential anomalies’ inter-arrival times and general distributions of maintenance duration. We show that the expected utility loss converges in probability to a simple form. We then extend our convergence results to more general distributions of anomalies’ inter-arrival times and to particular families of non-periodic utility functions. To validate our results, we use data gathered from a cellular network consisting of 660 base stations and serving over 20k users. We demonstrate the quasi-periodic nature of users’ traffic and the exponential distribution of the anomalies’ inter-arrival times, allowing us to apply our results and provide reliability scores for the network. We also discuss the convergence speed of the long-term average utility loss, the interplay between the different network’s parameters, and the impact of non-stationarity on our convergence results.}
}


@article{DBLP:journals/ton/SongQGS24,
	author = {Jiayi Song and
                  Jiaming Qiu and
                  Roch Gu{\'{e}}rin and
                  Henry Sariowan},
	title = {On the Benefits of Traffic "Reprofiling" the Single Hop
                  Case},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2511--2524},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3356863},
	doi = {10.1109/TNET.2024.3356863},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SongQGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need to guarantee hard delay bounds to traffic flows with deterministic traffic profiles, e.g., token buckets, arises in several network settings. It is of interest to offer such guarantees while minimizing network bandwidth. The paper explores a basic building block, namely, a single hop configuration, towards realizing such a goal. The main results are in the form of optimal solutions for meeting local deadlines under schedulers of varying complexity and therefore cost. The results demonstrate how judiciously modifying flows’ traffic profiles, i.e., reprofiling them, can help simple schedulers reduce the bandwidth they require, often performing nearly as well as more complex ones.}
}


@article{DBLP:journals/ton/LiuMRYPYW24,
	author = {Tang Liu and
                  Yuzhuo Ma and
                  Meixuan Ren and
                  Jin Yang and
                  Jian Peng and
                  Jilin Yang and
                  Di{\'{e}} Wu},
	title = {Concurrent Charging With Wave Interference for Multiple Chargers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2525--2538},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3361321},
	doi = {10.1109/TNET.2024.3361321},
	timestamp = {Sun, 04 Aug 2024 19:47:25 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuMRYPYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve the charging performance, employing multiple wireless chargers to charge sensors concurrently is an effective way. In such charging scenarios, the radio waves radiated from multiple chargers will interfere with each other. Though a few work have realized the wave interference, they do not fully utilize the high power caused by constructive interference while avoiding the negative impacts brought by the destructive interference. In this paper, we aim to investigate the power distribution regularity of concurrent charging and take full advantage of the high power to enhance the charging efficiency. Specifically, we formulate a concurrent charGing utility mAxImizatioN (GAIN) problem and build a practical charging model with wave interference. Further, we propose a concurrent charging scheme, which not only can improve the power of interference enhanced regions by deploying chargers, but also find a set of points with the highest power to locate sensors. Finally, we conduct both simulations and field experiments to evaluate the proposed scheme. The results demonstrate that our scheme outperforms the comparison algorithms by 40.48% on average.}
}


@article{DBLP:journals/ton/QiMZWR24,
	author = {Shixiong Qi and
                  Leslie Monis and
                  Ziteng Zeng and
                  Ian{-}Chin Wang and
                  K. K. Ramakrishnan},
	title = {{SPRIGHT:} High-Performance eBPF-Based Event-Driven, Shared-Memory
                  Processing for Serverless Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2539--2554},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3366561},
	doi = {10.1109/TNET.2024.3366561},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiMZWR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. ‘Cold-start’ latency is another deterrent. We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF’s socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with 10\\times\nless CPU usage under realistic workloads. Additionally, eBPF benefits SPRIGHT, by replacing heavyweight serverless components, allowing us to keep functions ‘warm’ with negligible penalty. Our preliminary experimental results show that SPRIGHT achieves an order of magnitude improvement in throughput and latency compared to Knative, while substantially reducing CPU usage, and obviates the need for ‘cold-start’.}
}


@article{DBLP:journals/ton/ZhengXBKBBVBZ24,
	author = {Changgang Zheng and
                  Zhaoqi Xiong and
                  Thanh T. Bui and
                  Siim Kaupmees and
                  Riyad Bensoussane and
                  Antoine Bernabeu and
                  Shay Vargaftik and
                  Yaniv Ben{-}Itzhak and
                  Noa Zilberman},
	title = {IIsy: Hybrid In-Network Classification Using Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2555--2570},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364757},
	doi = {10.1109/TNET.2024.3364757},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengXBKBBVBZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The soaring use of machine learning leads to increasing processing demands. As data volume keeps growing, providing classification services with good machine learning performance, high throughput, low latency, and minimal equipment overheads becomes a challenge. Offloading machine learning tasks to network switches can be a scalable solution to this problem, providing high throughput and low latency. However, network devices are resource constrained, and lack support for machine learning functionality. In this paper, we introduce IIsy - a novel mapping tool of machine learning classification models to off-the-shelf switches. Using an efficient encoding algorithm, IIsy enables fitting a range of classification models on switches, co-existing with standard switch functionality. To overcome resource constraints, IIsy adopts a hybrid approach for ensemble models, running a small model on a switch and a large model on the backend. The evaluation shows that IIsy achieves near-optimal classification results, within minimum resource overheads, and while reducing the load on the backend by 70% for data-intensive use cases.}
}


@article{DBLP:journals/ton/LiLYGWC24,
	author = {Fuliang Li and
                  Yiming Lv and
                  Yangsheng Yan and
                  Chengxi Gao and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {Learning-Based Sketch for Adaptive and High-Performance Network Measurement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2571--2585},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364176},
	doi = {10.1109/TNET.2024.3364176},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLYGWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of network measurement technologies, a hybrid measurement architecture can effectively optimize the sketch structure in switches, making it more adaptable to the current complex and volatile network environment. However, current optimization technologies based on hybrid measurement architectures generally suffer from insufficient automation, difficulty of learning effective numerical features, and lack of generality, resulting in poor scalability in real deployment. To solve these problems, we propose the TalentSketch framework, based on which we further develop DeepSketch for effective sketch optimization. First, we use Seq2Seq to automatically identify target flows instead of relying on manual thresholds. Second, we propose a new training strategy that extracts low-precision flows for models with weak learning capabilities. Last, we develop a new sketch optimization framework that can optimize different kinds of sketches only by changing the training data for generality. A large number of experimental results show that DeepSketch exhibits superior performance. For example: (1) the accuracy of optimized sketches has increased by 20% to 73%, (2) Without replacing the model structure, the accuracy of the optimized sketches can generally reach over 80%. (3) The impact of low sampling rates on accuracy is less than 1% on various sketches.}
}


@article{DBLP:journals/ton/ZhangWZYPZGCWPH24,
	author = {Jiao Zhang and
                  Yuqing Wang and
                  Xiaolong Zhong and
                  Mingxuan Yu and
                  Haoyu Pan and
                  Yali Zhang and
                  Zixuan Guan and
                  Biyao Che and
                  Zirui Wan and
                  Tian Pan and
                  Tao Huang},
	title = {{PACC:} {A} Proactive {CNP} Generation Scheme for Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2586--2599},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3361771},
	doi = {10.1109/TNET.2024.3361771},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWZYPZGCWPH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid upgrade of link speed and the prosperity of new applications in data center networks (DCNs) lead to a rigorous demand for ultra-low latency and high throughput. To mitigate the overhead of traditional software-based packet processing at end-hosts, RDMA (Remote Direct Memory Access) has been widely adopted in DCNs. Particularly, congestion control (CC) mechanisms designed for RDMA have attracted much attention to avoid performance deterioration when packets lose. However, through comprehensive analysis, we found that existing RDMA CC schemes have limitations of a sluggish response to congestion and unawareness of tiny microbursts due to the long end-to-end control loop. In this paper, we propose PACC, a proactive and accurate switch-driven RDMA CC algorithm with easy deployability. PACC is driven by PI controller-based computation, threshold-based flow discrimination and weight-based allocation at the switch. It leverages real-time queue length to generate accurate congestion feedback proactively and piggybacks it to the corresponding source without modification to end-hosts. We theoretically analyze the stability, convergence and key parameter settings of PACC. Then, we implement PACC in a testbed consisting of DPDK-based end-hosts and Tofino P4 switches. In our evaluation, PACC achieves better fairness, fast reaction, high throughput, and 6~69% lower FCT (Flow Completion Time) than DCQCN, TIMELY, HPCC and RoCC.}
}


@article{DBLP:journals/ton/DengLXZZRY24,
	author = {Yongheng Deng and
                  Feng Lyu and
                  Tengxi Xia and
                  Yuezhi Zhou and
                  Yaoxue Zhang and
                  Ju Ren and
                  Yuanyuan Yang},
	title = {A Communication-Efficient Hierarchical Federated Learning Framework
                  via Shaping Data Distribution at Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2600--2615},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3363916},
	doi = {10.1109/TNET.2024.3363916},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DengLXZZRY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables collaborative model training over distributed computing nodes without sharing their privacy-sensitive raw data. However, in FL, iterative exchanges of model updates between distributed nodes and the cloud server can result in significant communication cost, especially when the data distributions at distributed nodes are imbalanced with requiring more rounds of iterations. In this paper, with our in-depth empirical studies, we disclose that extensive cloud aggregations can be avoided without compromising the learning accuracy if frequent aggregations can be enabled at edge network. To this end, we shed light on the hierarchical federated learning (HFL) framework, where a subset of distributed nodes can play as edge aggregators to support edge aggregations. Under the HFL framework, we formulate a communication cost minimization (CCM) problem to minimize the total communication cost required for model learning with a target accuracy by making decisions on edge aggragator selection and node-edge associations. Inspired by our data-driven insights that the potential of HFL lies in the data distribution at edge aggregators, we propose ShapeFL, i.e., SHaping dAta distRibution at Edge, to transform and solve the CCM problem. In ShapeFL, we divide the original problem into two sub-problems to minimize the per-round communication cost and maximize the data distribution diversity of edge aggregator data, respectively, and devise two light-weight algorithms to solve them accordingly. Extensive experiments are carried out based on several opened datasets and real-world network topologies, and the results demonstrate the efficacy of ShapeFL in terms of both learning accuracy and communication efficiency.}
}


@article{DBLP:journals/ton/LiLJWTC24,
	author = {Yixin Li and
                  Liang Liang and
                  Yunjian Jia and
                  Wanli Wen and
                  Chaowei Tang and
                  Zhengchuan Chen},
	title = {Blockchain for Data Sharing at the Network Edge: Trade-Off Between
                  Capability and Security},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2616--2630},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364023},
	doi = {10.1109/TNET.2024.3364023},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLJWTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blokchain is a promising technology to enable distributed and reliable data sharing at the network edge. The high security in blockchain is undoubtedly a critical factor for the network to handle important data item. On the other hand, according to the dilemma in blockchain, an overemphasis on distributed security will lead to poor transaction-processing capability, which limits the application of blockchain in data sharing scenarios with high-throughput and low-latency requirements. To enable demand-oriented distributed services, this paper investigates the relationship between capability and security in blockchain from the perspective of block propagation and forking problem. First, a Markov chain is introduced to analyze the gossiping-based block propagation among edge servers, which aims to derive block propagation delay and forking probability. Then, we study the impact of forking on blockchain capability and security metrics, in terms of transaction throughput, confirmation delay, fault tolerance, and the probability of malicious modification. The analytical results show that with the adjustment of block generation time or block size, transaction throughput improves at the sacrifice of fault tolerance, and vice versa. Meanwhile, the decline in security can be offset by adjusting confirmation threshold, at the cost of increasing confirmation delay. The analysis of capability-security trade-off can provide a theoretical guideline to manage blockchain networks based on the requirements of data sharing scenarios.}
}


@article{DBLP:journals/ton/PingWZLX24,
	author = {Haodi Ping and
                  Yongcai Wang and
                  Yu Zhang and
                  Deying Li and
                  Lihua Xie},
	title = {Understanding Hidden Knowledge in Generic Graphs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2631--2645},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364177},
	doi = {10.1109/TNET.2024.3364177},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PingWZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When the edge between two nodes is not measured, is there any hint to know the edge property, and will the inferred edge property be useful? To answer these questions, this paper uniformly defines the properties of unmeasurable edges in generic graphs. For an unmeasurable edge (i,j) , it is called rangeable if its length is unique in any realization of the graph, rigid if the number of its possible lengths is finite, and flexible if it has infinite possible lengths. The rangeable edge can provide deterministic hidden knowledge as if the edge is measured. A condition for an unmeasured edge being rangeable in 2D space is firstly proposed, based on which a centralized identification algorithm (DRE) is designed. However, the centralized rangeable edge identification has the overhead of global information collection. Therefore distributed condition and algorithm to identify rangeable edges are further investigated. We prove that an unmeasurable edge (i,j) is rangeable if there are at least two Disjoint Minimally Rigid Branches (DMRBs) between i and j . The unmeasurable edge (i,j) is rigid and flexible when the number of DMRB is one and zero, respectively. A distributed Branching and Blacklisting (BB) algorithm is proposed to find DMRBs, so that rangeable edges are identified distributively. Then, the applications of rangeable, rigid, and flexible edges are discussed. Experimental evaluations show that the centralized and distributed algorithms can identify a rich set of unmeasurable but rangeable edges in distance graphs, even more than the number of directly measured edges. Moreover, BB has a similar identification performance as the centralized DRE algorithm and outperforms existing distributed unmeasurable edge inference algorithms significantly.}
}


@article{DBLP:journals/ton/ZhangZWTH24,
	author = {Renli Zhang and
                  Ruiting Zhou and
                  Yufeng Wang and
                  Haisheng Tan and
                  Kun He},
	title = {Incentive Mechanisms for Online Task Offloading With Privacy-Preserving
                  in UAV-Assisted Mobile Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2646--2661},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364141},
	doi = {10.1109/TNET.2024.3364141},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZWTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) have emerged as a promising technology to provide low-latency mobile edge computing (MEC) services. To fully utilize the potential of UAV-assisted MEC in practice, both technical and economic challenges need to be addressed: how to optimize UAV trajectory for online task offloading and incentivize the participation of UAVs without compromising the privacy of user equipment (UE). In this work, we consider unique features of UAVs, i.e., high mobility as well as limited energy and computing capacity, and propose privacy-preserving auction frameworks, Ptero, to schedule offloading tasks on the fly and incentivize UAVs’ participation. Specifically, Ptero first decomposes the online task offloading problem into a series of one-round problems by scaling the UAV’s energy constraint into the objective. To protect UE’s privacy, Ptero calculates UAV’s coverage based on subset-anonymity. At each round, Ptero schedules UAVs greedily, computes remuneration for working UAVs, and processes unserved tasks in the cloud to maximize the system’s utility (i.e., minimize social cost). Theoretical analysis proves that Ptero achieves truthfulness, individual rationality, computational efficiency, privacy-preserving and a nontrivial competitive ratio. Trace-driven evaluations further verify that Ptero can reduce the social cost by up to 116% compared with four state-of-the-art algorithms.}
}


@article{DBLP:journals/ton/LuoGZZ24,
	author = {Changyun Luo and
                  Huaxi Gu and
                  Lijing Zhu and
                  Huixia Zhang},
	title = {FlowStar: Fast Convergence Per-Flow State Accurate Congestion Control
                  for InfiniBand},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2662--2674},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3363658},
	doi = {10.1109/TNET.2024.3363658},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LuoGZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {According to the latest TOP500 list, InfiniBand (IB) is the most widely used network architecture in the top 10 supercomputers. IB relies on Credit-based Flow Control (CBFC) to provide a lossless network and InfiniBand congestion control (IB CC) to relieve congestion, however, this can lead to the problem of victim flow since messages are mixed in the same queue and long-lived congestion spreading due to slow convergence. To deal with these problems, in this paper, we propose FlowStar, a fast convergence per-flow state accurate congestion control for InfiniBand. FlowStar includes two core mechanisms: 1) optimized per-flow CBFC mechanism provides flow state control to detect real congestion; and 2) rate adjustment rules make up for the mismatch between the original IB CC rate regulation and the per-hop CBFC to alleviate congestion spreading. FlowStar implements a per-flow congestion state on switches and can obtain in-flight packet information without additional parameter settings to ensure a lossless network. Evaluations show that FlowStar improves average and tail message complete time under different workloads.}
}


@article{DBLP:journals/ton/YangZ24,
	author = {Hao Yang and
                  Zuqing Zhu},
	title = {Traffic-Aware Configuration of All-Optical Data Center Networks Based
                  on Hyper-FleX-LION},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2675--2688},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3363841},
	doi = {10.1109/TNET.2024.3363841},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advantages of optical circuit switching (OCS), all-optical data center networks (DCNs) have attracted intensive research interests recently. Hyper-FleX-LION is a highly-flexible all-optical DCN architecture that operates with the OCS based on wavelength-division multiplexing (WDM). In this work, we study how to realize traffic-aware configuration of all-optical DCNs in Hyper-FleX-LION. We formulate an integer linear programming (ILP) model for the problem to jointly optimize the configuration of Hyper-FleX-LION and the provisioning schemes of demands in it for minimizing its port usage. To ensure the practicalness of the optimization, we assume that each top-of-rack (ToR) switch can not only receive the traffic targeting to its rack but also forward traffic to other racks as an intermediate node. We also classifier traffic demands as normal and latency-sensitive ones, and set the maximum hop-count for routing latency-sensitive demands. By analyzing the complexity of the problem theoretically, we prove its \\mathcal {APX} -hardness, i.e., there does not exist a polynomial-time approximation algorithm for it unless \\mathcal {P}=\\mathcal {NP} . Then, we propose a polynomial-time heuristic JTRO based on iterative optimization to solve the problem effectively and time-efficiently. Extensive numerical simulations verify the effectiveness of our proposed algorithm. We also build a small-scale but real all-optical DCN testbed in Hyper-FleX-LION to interconnect four racks, and leverage distributed machine learning (DML) as the network services in it to demonstrate the performance of our proposal experimentally.}
}


@article{DBLP:journals/ton/GuiLDC24,
	author = {Jinsong Gui and
                  Liyan Lin and
                  Xiaoheng Deng and
                  Lin Cai},
	title = {Spectrum-Energy-Efficient Mode Selection and Resource Allocation for
                  Heterogeneous {V2X} Networks: {A} Federated Multi-Agent Deep Reinforcement
                  Learning Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2689--2704},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364161},
	doi = {10.1109/TNET.2024.3364161},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuiLDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous communication environments and broadcast feature of safety-critical messages bring great challenges to mode selection and resource allocation problem. In this paper, we propose a federated multi-agent deep reinforcement learning (DRL) scheme with action awareness to solve mode selection and resource allocation problem for ensuring quality of service (QoS) in heterogeneous V2X environments. The proposed scheme includes an action-observation-based DRL and a model parameter aggregation algorithm considering local model historical parameters. By observing the actions of adjacent agents and dynamically balancing the historical samples of rewards, the action-observation-based DRL can ensure fast convergence of each agent’ individual model. By randomly sampling historical model parameters and adding them to the foundation model aggregation process, the model parameter aggregation algorithm improves foundation model generalization. The generalized model is only sent to each new agent, so each old agent can retain the personality of its individual model. Simulation results show that the proposed scheme outperforms the comparison algorithms in the key performance indicators.}
}


@article{DBLP:journals/ton/NguyenSHNNXD24,
	author = {Chi{-}Hieu Nguyen and
                  Yuris Mulya Saputra and
                  Dinh Thai Hoang and
                  Diep N. Nguyen and
                  Van{-}Dinh Nguyen and
                  Yong Xiao and
                  Eryk Dutkiewicz},
	title = {Encrypted Data Caching and Learning Framework for Robust Federated
                  Learning-Based Mobile Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2705--2720},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3365815},
	doi = {10.1109/TNET.2024.3365815},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NguyenSHNNXD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) plays a pivotal role in enabling artificial intelligence (AI)-based mobile applications in mobile edge computing (MEC). However, due to the resource heterogeneity among participating mobile users (MUs), delayed updates from slow MUs may deteriorate the learning speed of the MEC-based FL system, commonly referred to as the straggling problem. To tackle the problem, this work proposes a novel privacy-preserving FL framework that utilizes homomorphic encryption (HE) based solutions to enable MUs, particularly resource-constrained MUs, to securely offload part of their training tasks to the cloud server (CS) and mobile edge nodes (MENs). Our framework first develops an efficient method for packing batches of training data into HE ciphertexts to reduce the complexity of HE-encrypted training at the MENs/CS. On that basis, the mobile service provider (MSP) can incentivize straggling MUs to encrypt part of their local datasets that are uploaded to certain MENs or the CS for caching and remote training. However, caching a large amount of encrypted data at the MENs and CS for FL may not only overburden those nodes but also incur a prohibitive cost of remote training, which ultimately reduces the MSP’s overall profit. To optimize the portion of MUs’ data to be encrypted, cached, and trained at the MENs/CS, we formulate an MSP’s profit maximization problem, considering all MUs’ and MENs’ resource capabilities and data handling costs (including encryption, caching, and training) as well as the MSP’s incentive budget. We then show that the problem is convex and can be efficiently solved using an interior point method. Extensive simulations on a real-world human activity recognition dataset show that our proposed framework can achieve much higher model accuracy (improving up to 24.29%) and faster convergence rate (by 2.86 times) than those of the conventional FedAvg approach when the straggling probability varies between 20% and 80%. Moreover, the proposed framework can improve the MSP’s profit up to 2.84 times compared with other baseline FL approaches without MEN-assisted training.}
}


@article{DBLP:journals/ton/ChenBYYYL24,
	author = {Chao Chen and
                  Seungjun Baek and
                  Rui Yin and
                  Shengtian Yang and
                  Xiaohan Yu and
                  Chuanhuang Li},
	title = {Practical and Efficient Coded Transmission for Full-Duplex Relay Networks
                  Without {CSI}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2721--2735},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3366697},
	doi = {10.1109/TNET.2024.3366697},
	timestamp = {Thu, 04 Jul 2024 22:04:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenBYYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We jointly consider full-duplex operation and network coding in two-hop relay networks to enhance the throughput of the block transmission of packets over erasure channels. Two coded transmission schemes, termed Fewest Broadcast Packet First (FBPF) and Buffer Contents-based Coded Transmission (BCCT), are proposed, where random linear network coding is employed at the Base Station (BS) and the Relay Station (RS), respectively. Both schemes do not rely on users’ Channel State Information (CSI), buffer status, channel parameters, etc., and hence are practically viable. We derive closed-form upper bounds on the throughput of both schemes. We prove that both schemes achieve the optimal throughput when the BS-to-RS channel is perfect. Through extensive simulations, we demonstrate that both schemes incur substantially higher throughput than the traditional uncoded Automatic Repeat-reQuest (ARQ) scheme and perform close to a general upper bound on the system throughput. Furthermore, even with imperfect Self-Interference Cancellation (SIC) at the full-duplex RS, our schemes are shown to be superior to state-of-the-art coded transmission schemes designed for half-duplex relay networks, given that the impact of imperfect SIC on the BS-to-RS channel quality is not high.}
}


@article{DBLP:journals/ton/HuZWZGXHC24,
	author = {Jinbin Hu and
                  Chaoliang Zeng and
                  Zilong Wang and
                  Junxue Zhang and
                  Kun Guo and
                  Hong Xu and
                  Jiawei Huang and
                  Kai Chen},
	title = {Load Balancing With Multi-Level Signals for Lossless Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {3},
	pages = {2736--2748},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3366336},
	doi = {10.1109/TNET.2024.3366336},
	timestamp = {Sun, 04 Aug 2024 19:47:25 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuZWZGXHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various datacenter network (DCN) load balancing schemes have been proposed in the past decade. Unfortunately, most of these solutions designed for lossy DCNs do not work well for Priority Flow Control (PFC) enabled lossless DCNs, primarily due to the reason that the individual congestion signals used in these solutions, e.g., link load, queue length, Round Trip Time (RTT) and Explicit Congestion Notification (ECN), may not be able to correctly or timely reflect the hop-by-hop PFC pausing. This paper first reveals the above problems via extensive experiments, and then based on the insights learned, we present Proteus, a PFC-aware load balancing scheme that is resilient to PFC pausing by exploring a combination of multi-level congestion signals. At its heart, Proteus leverages RTT-level signals (i.e., RTT and link utilization) to detect path status for initial routing decision, and exploits sub-RTT level signal (i.e., cumulative sojourn time) to reflect instantaneous PFC pausing and make timely rerouting choices based on the idea of better-late-than-never. We have implemented Proteus in the hardware programmable switch. Our testbed experiments as well as large-scale simulations show that Proteus can effectively handle PFC pausing under realistic workloads and achieve up to 35%, 31%, 28%, 22% and 46%, 42%, 34%, 29% better average FCT and 99^{th}\npercentile FCT than CONGA, DRILL, Hermes and MP-RDMA, respectively.}
}


@article{DBLP:journals/ton/ChenE24,
	author = {Yutao Chen and
                  Anthony Ephremides},
	title = {Minimizing Age of Incorrect Information Over a Channel With Random
                  Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2752--2764},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3389964},
	doi = {10.1109/TNET.2024.3389964},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a transmitter-receiver pair in a slotted-time system. The transmitter observes a dynamic source and sends updates to a remote receiver through an error-free communication channel that suffers a random delay. We consider two cases. In the first case, the update is guaranteed to be delivered within a certain number of time slots. In the second case, the update is immediately discarded once the transmission time exceeds a predetermined value. The receiver estimates the state of the dynamic source using the received updates. In this paper, we adopt the Age of Incorrect Information (AoII) as the performance metric and investigate the problem of optimizing the transmitter’s action in each time slot to minimize AoII. We first characterize the optimization problem using the Markov decision process and investigate the performance of the threshold policy, under which the transmitter transmits updates only when the transmission is allowed and the AoII exceeds the threshold \\tau . By delving into the characteristics of the system evolution, we precisely compute the expected AoII achieved by the threshold policy using the Markov chain. Then, we prove that the optimal policy exists. Furthermore, by leveraging the policy improvement theorem, we theoretically prove that, under an easily verifiable condition, the optimal policy is the threshold policy with \\tau =1 . Finally, numerical results are presented to highlight the performance of the optimal policy.}
}


@article{DBLP:journals/ton/AbolhassaniTEY24,
	author = {Bahman Abolhassani and
                  John Tadrous and
                  Atilla Eryilmaz and
                  Serdar Y{\"{u}}ksel},
	title = {Optimal Push and Pull-Based Edge Caching for Dynamic Content},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2765--2777},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3352029},
	doi = {10.1109/TNET.2024.3352029},
	timestamp = {Fri, 20 Sep 2024 14:02:39 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AbolhassaniTEY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a framework and optimal ‘fresh’ caching for a content distribution network (CDN) comprising a front-end local cache and a back-end database. The data content is dynamically updated at a back-end database and end-users are interested in the most-recent version of that content. We formulate the average cost minimization problem that captures the system’s cost due to the service of aging content as well as the regular cache update cost. We consider the cost minimization problem from two individual perspectives based on the available information to either side of the CDN: the back-end database perspective and the front-end local cache perspective. For the back-end database, the instantaneous version of content is observable but the exact demand is not. Caching decisions made by the back-end database are termed ‘push-based caching.’ For the front-end local cache, the age of content version in the cache is not observable, yet the instantaneous demand is. Caching decisions made by the front-end local cache are termed ‘pull-based caching.’ Our investigations reveal which type of information, updates, or demand dynamic, is of higher value towards achieving the minimum cost based on other network parameters including content popularity, update rate, and demand intensity.}
}


@article{DBLP:journals/ton/ChiangWYLC24,
	author = {Sheng{-}Hao Chiang and
                  Chih{-}Hang Wang and
                  De{-}Nian Yang and
                  Wanjiun Liao and
                  Wen{-}Tsuen Chen},
	title = {Online Multicast Traffic Engineering for Multi-View Videos With View
                  Synthesis in {SDN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2778--2793},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3366166},
	doi = {10.1109/TNET.2024.3366166},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChiangWYLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view videos (MVV) have emerged to provide users with immersively interactive experiences with 3D multimedia content. Compared with traditional 2D videos, MVV offers multiple view angles to avoid generating occluded regions from a single viewpoint and allows users to receive different view angles, which consume much higher bandwidth. In this paper, we leverage multicast and view synthesis to effectively reduce the number of transmitted views and total bandwidth consumption in software-defined content delivery networks (SD-CDN). By exploiting the SDN architecture, SD-CDN can optimize traffic engineering and the selection of multi-view sources to serve users. We formulate a new optimization problem, Online Multicast with View Synthesis (OMVS), prove the NP-hardness, and design an online algorithm with the ideas of View Popularity Cost Ratio, View Watching Possibility, and synthesis tree, to achieve the tightest competitive ratio. The experiment on real networks and implementation in an experimental SDN manifest that the proposed algorithm outperforms state-of-the-art algorithms regarding the total cost, bandwidth consumption, synthesis quality, and link and node utilization.}
}


@article{DBLP:journals/ton/GongZSLWC24,
	author = {Chen Gong and
                  Zhenzhe Zheng and
                  Yunfeng Shao and
                  Bingshuai Li and
                  Fan Wu and
                  Guihai Chen},
	title = {{ODE:} An Online Data Selection Framework for Federated Learning With
                  Limited Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2794--2809},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3365534},
	doi = {10.1109/TNET.2024.3365534},
	timestamp = {Wed, 12 Feb 2025 16:03:51 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GongZSLWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) models have been deployed in mobile networks to deal with massive data from different layers to enable automated network management. To overcome high communication cost and severe privacy concerns of centralized ML, federated learning (FL) has been proposed to achieve distributed ML among numerous networked devices. While the computation and communication limitation has been widely studied, the impact of limited storage of mobile devices on the performance of FL is still not explored. Without an effective data selection policy to filter the massive streaming networked data on devices, classical FL can suffer from much longer model training time ( 4\\times ) and dramatic inference accuracy reduction (7%), observed in our experiments. In this work, we take the first step to consider the online data selection for FL with limited on-device storage. We first define a new data valuation metric for data selection in FL with theoretical guarantee for simultaneously accelerating model convergence and enhancing final accuracy. We further design ODE, an Online Data sElection framework for FL, to coordinate networked devices to store valuable data samples collaboratively. Experimental results on one industrial dataset and three public datasets show the remarkable advantages of ODE over the state-of-the-art approaches. Particularly, on the industrial dataset, ODE achieves as high as 2.5\\times speedup of training time and 6% increase in final accuracy, and is robust to various factors in practical environments.}
}


@article{DBLP:journals/ton/MisaDRW24,
	author = {Chris Misa and
                  Ramakrishnan Durairajan and
                  Reza Rejaie and
                  Walter Willinger},
	title = {DynATOS+: {A} Network Telemetry System for Dynamic Traffic and Query
                  Workloads},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2810--2825},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3367432},
	doi = {10.1109/TNET.2024.3367432},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MisaDRW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network telemetry systems provide critical visibility into the state of network traffic. By leveraging modern programmable switch hardware, significant progress has been made to scale these systems to production network traffic workloads. Less attention has been paid towards efficiently utilizing these hardware targets’ limited resources in the face of dynamics such as the composition of the traffic workload as well as the number and types of queries running at any given point in time. However, both of these dynamics have implications on resource requirements and query accuracy. Building on our prior work DynATOS, which argues that this dynamics problem motivates reframing telemetry systems as resource schedulers, we present in this paper the design, implementation, and evaluation of DynATOS+. DynATOS+ relies on the same efficient time-division approximation and scheduling algorithm that DynATOS uses and that allows for user-defined query accuracy and latency specifications that are intended to result in tradeoffs with respect to query execution to reduce hardware resource usage. However, unlike DynATOS, DynATOS+ significantly reduces the burden on end users to express their queries by allowing them to use simple-to-state accuracy goals. For example, the method for specifying per-query accuracy goals in DynATOS+ no longer requires end users to either know the average range of query results in advance or to submit multiple trial queries to tune their accuracy goal specifications. We perform extensive simulation-based evaluations that (i) show that this new functionality of DynATOS+ works in practice, (ii) illustrate in detail the tradeoffs that result with respect to query execution and hardware resource usage for a wide range of systems parameters, and (iii) allow for an assessment of system performance under changing query workloads on top of changes in the composition of traffic workloads that has eluded previous work in this area.}
}


@article{DBLP:journals/ton/QuanES24,
	author = {Guocong Quan and
                  Atilla Eryilmaz and
                  Ness B. Shroff},
	title = {Optimal Edge Caching for Individualized Demand Dynamics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2826--2841},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3369611},
	doi = {10.1109/TNET.2024.3369611},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QuanES24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-growing end user data demands, and the reductions in memory costs are fueling edge-caching deployments. Caching at the edge is substantially different from that at the core and needs to consider the nature of individualized data demands. For example, an individual user may not be interested in requesting the same data item again, if it has recently requested it. Such individualized dynamics are not apparent in the aggregated data requests at the core and have not been considered in popularity-driven caching designs for the core. Hence, these traditional caching policies could induce significant inefficiencies when applied at the edges. To address this issue, we develop new edge caching policies optimized for the individualized demands that also leverage overhearing opportunities at the wireless edge. With the objective of maximizing the hit ratio, the proposed policies will actively evict the data items that are not likely to be requested in the near future, and strategically bring them back into the cache via overhearing when they become popular again. Both theoretical analysis and numerical simulations demonstrate that the proposed edge caching policies could outperform the popularity-driven policies that are optimal at the core.}
}


@article{DBLP:journals/ton/ChenLXHZZZWLY24,
	author = {Xiang Chen and
                  Hongyan Liu and
                  Qingjiang Xiao and
                  Qun Huang and
                  Dong Zhang and
                  Haifeng Zhou and
                  Boyang Zhou and
                  Chunming Wu and
                  Xuan Liu and
                  Qiang Yang},
	title = {Hermes: Low-Overhead Inter-Switch Coordination in Network-Wide Data
                  Plane Program Deployment},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2842--2857},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3361324},
	doi = {10.1109/TNET.2024.3361324},
	timestamp = {Mon, 16 Sep 2024 10:28:05 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenLXHZZZWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network administrators usually realize network functions in data plane programs. They employ the network-wide program deployment that decomposes input programs into match-action tables (MATs) while deploying each MAT on a specific switch. Since MATs may be deployed on different switches, existing solutions propose the inter-switch coordination that uses the per-packet header space to deliver crucial packet processing information among switches. However, such coordination incurs non-trivial per-packet byte overhead, leading to end-to-end performance degradation. We propose Hermes, a framework that aims to minimize the per-packet byte overhead. The key idea is to formulate network-wide program deployment as a mixed-integer programming (MIP) problem with the objective of minimizing the per-packet byte overhead. Also, Hermes offers a greedy-based heuristic that solves the problem in a near-optimal and timely manner. We have implemented Hermes on Tofino switches. Compared to existing frameworks, Hermes decreases the per-packet byte overhead by 156bytes while preserving end-to-end performance in terms of flow completion time and goodput.}
}


@article{DBLP:journals/ton/RottenstreichY24,
	author = {Ori Rottenstreich and
                  Jose Yallouz},
	title = {Edge-Disjoint Tree Allocation for Multi-Tenant Cloud Security in Datacenter
                  Topologies},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2858--2874},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3364173},
	doi = {10.1109/TNET.2024.3364173},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/RottenstreichY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource sharing with its implied mutual interference has been considered a major concern for running applications of multiple tenants in shared cloud datacenters. Besides its security benefits, the isolation of traffic might ensure a quality of service (QoS) performance guarantee avoiding interference among tenants. Traffic isolation can be achieved by dedicating the usage of link resources in the network to a single tenant preventing its sharing among others. Accordingly, tenants should be connected through an edge-disjoint tree to enable isolated communication among its hosts. In this paper, we study the problem of establishing edge-disjoint trees in common datacenter topologies. We show that the availability of such trees is highly affected by the mapping of the tenants to hosts of the topology. Specifically, with the flexibility to map tenants in the datacenter topology, we describe a mapping algorithm and an optimal tree establishment for the optimization problem. Given the mapping of the tenants, we prove the problem turns out to be NP-Hard and provide comprehensive heuristics for the problem. Finally, we conduct experiments using real workloads to examine tree availability under various scenarios.}
}


@article{DBLP:journals/ton/BocheSPF24,
	author = {Holger Boche and
                  Rafael F. Schaefer and
                  H. Vincent Poor and
                  Frank H. P. Fitzek},
	title = {On the Need of Neuromorphic Twins to Detect Denial-of-Service Attacks
                  on Communication Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2875--2887},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3369018},
	doi = {10.1109/TNET.2024.3369018},
	timestamp = {Sun, 19 Jan 2025 13:55:30 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BocheSPF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As we become more and more dependent on communication technologies, resilience against any attacks on communication networks is important to guarantee the digital sovereignty of our society. New developments of communication networks approach the problem of resilience through in-network computing approaches for higher protocol layers, while the physical layer remains an open problem. This is particularly true for wireless communication systems which are inherently vulnerable to adversarial attacks due to the open nature of the wireless medium. In denial-of-service (DoS) attacks, an active adversary is able to completely disrupt the communication and it has been shown that Turing machines are incapable of detecting such attacks. As Turing machines provide the fundamental limits of digital information processing and therewith of digital twins, this implies that even the most powerful digital twins that preserve all information of the physical network error-free are not capable of detecting such attacks. This stimulates the question of how powerful the information processing hardware must be to enable the detection of DoS attacks. Therefore, in this paper the need of neuromorphic twins is advocated and by the use of Blum-Shub-Smale machines a first implementation that enables the detection of DoS attacks is shown. This result holds for both cases of with and without constraints on the input and jamming sequences of the adversary.}
}


@article{DBLP:journals/ton/XiaLSCG24,
	author = {Junxu Xia and
                  Lailong Luo and
                  Bowen Sun and
                  Geyao Cheng and
                  Deke Guo},
	title = {Parallelized In-Network Aggregation for Failure Repair in Erasure-Coded
                  Storage Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2888--2903},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3367995},
	doi = {10.1109/TNET.2024.3367995},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaLSCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To repair a failed block in the erasure-coded storage system, multiple related blocks have to be retrieved from other storage nodes across the network. Such a process can lead to significant incast-type repair traffics and delays. The existing efforts mainly try to schedule the transmission of the requested blocks across different storage nodes to avoid network congestion. At their cores, they utilize part of the involved hosts to rely on or aggregate the file blocks from others. While we notice that, the programmability and capability of today’s network devices (i.e., routers and switches) bring a great opportunity to further speed up the repair progress by aggregating the file blocks with such devices. By mitigating the aggregation operations from the network edges to network cores, it is possible to save more time and bandwidth. With this intuition, we propose Paint, a parallelized in-network aggregation framework for failure repair. Paint utilizes programmable switches to aggregate relevant data and improves the repair performance by implementing multiple parallelized repair pipelines. We propose a series of novel and time-friendly algorithms to construct the routing paths for Paint and design the Aggregation Control Protocol to implement Paint in production clusters. For all we know, this is the first work to explore and implement parallelized in-network repair with programmable switches. The extensive experiments on the prototype system and real-world datasets indicate that Paint can significantly improve repair performance while effectively reducing bandwidth overhead.}
}


@article{DBLP:journals/ton/ZhangLCTDGL24,
	author = {Jiuwu Zhang and
                  Xiulong Liu and
                  Sheng Chen and
                  Xinyu Tong and
                  Zeyu Deng and
                  Tao Gu and
                  Keqiu Li},
	title = {Toward Robust {RFID} Localization via Mobile Robot},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2904--2919},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3373770},
	doi = {10.1109/TNET.2024.3373770},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLCTDGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A wide range of scenarios, such as warehousing, and smart manufacturing, have used RFID mobile robots for the localization of tagged objects. The state-of-the-art RFID-robot based localization works are based on the premise of stable speed. However, in reality this assumption can hardly be guaranteed because Commercial-Off-The-Shelf (COTS) robots typically have inconsistent moving speeds, and a small speed inconsistency will cause a large localization error. To this end, we propose a Speed Inconsistency-Immune approach to mobile RFID robot Localization (SILoc) system, which accurately locates targets when the robot moving speed varies or is even unknown. We propose an optimized unwrapping method to maximize the use of data, and a lightweight algorithm to calculate the locations in both 2D and 3D spaces. By utilizing the characteristics of tag-antenna distance and combining the phase data from multiple antennas, SILoc can effectively eliminate the side effects of speed inconsistency. To increase the flexibility, we further optimize the system and propose SILoc+, which enables the system to achieve localization with part of the data, keeping speed inconsistency-immune. Extensive experiments demonstrate that SILoc and SILoc+ can achieve a centimeter-level localization accuracy in the scenario with an inconsistent or unknown robot moving speed.}
}


@article{DBLP:journals/ton/LiaoLHZWZC24,
	author = {Zimo Liao and
                  Zhicheng Luo and
                  Qianyi Huang and
                  Linfeng Zhang and
                  Fan Wu and
                  Qian Zhang and
                  Guihai Chen},
	title = {Gesture Recognition Using Visible Light on Mobile Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2920--2935},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3369996},
	doi = {10.1109/TNET.2024.3369996},
	timestamp = {Mon, 03 Mar 2025 10:35:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoLHZWZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-air gesture control extends a touch screen and enables contactless interaction, thus has become a popular research direction in the past few years. Prior work has implemented this functionality based on cameras, acoustic signals, and Wi-Fi via existing hardware on commercial devices. However, these methods have low user acceptance. Solutions based on cameras and acoustic signals raise privacy concerns, while WiFi-based solutions are vulnerable to background noise. As a result, these methods are not commercialized and recent flagship smartphones have implemented in-air gesture recognition by adding extra hardware on-board, such as mmWave radar and depth camera. The question is, can we support in-air gesture control on legacy devices without any hardware modifications? To answer this question, in this work, we propose SMART, an in-air gesture recognition system leveraging the screen and ambient light sensor (ALS), which are ordinary modalities on mobile devices. For the transmitter side, we design a screen display mechanism to embed spatial information and preserve the viewing experience; for the receiver side, we develop a framework to recognize gestures from low-quality ALS readings. We implement and evaluate SMART on both a tablet and several smartphones. Results show that SMART can recognize 9 types of frequently used in-air gestures with an average accuracy of 96.1%.}
}


@article{DBLP:journals/ton/HuangTMYZZW24,
	author = {Haojun Huang and
                  Jialin Tian and
                  Geyong Min and
                  Hao Yin and
                  Cheng Zeng and
                  Yangming Zhao and
                  Dapeng Oliver Wu},
	title = {Parallel Placement of Virtualized Network Functions via Federated
                  Deep Reinforcement Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2936--2949},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3366950},
	doi = {10.1109/TNET.2024.3366950},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangTMYZZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Function Virtualization (NFV) introduces a new network architecture that offers different network services flexibly and dynamically in the form of Service Function Chains (SFCs), which refer to a set of Virtualization Network Functions (VNFs) chained in a specific order. However, the service latency often increases linearly with the length of SFCs due to the sequential execution of VNFs, resulting in sub-optimal performance for most delay-sensitive applications. In this paper, a novel Parallel VNF Placement (PVFP) approach is proposed for real-world networks via Federated Deep Reinforcement Learning (FDRL). PVFP has three remarkable characteristics distinguishing from previous work: 1) PVFP designs a specific parallel principle, with three parallelism identification rules, to reasonably decide partial VNF parallelism; 2) PVFP considers SFC partition in multi-domains built on their remaining resources and potential parallel VNFs to ensure that VNFs can be reasonably distributed for resource balancing among domains; 3) FDRL-based framework of parallel VNF placement is designed to train a global intelligent model, with time-variant local autonomy explorations, for cross-domain SFC deployment, avoiding data sharing among domains. Simulation results in different scenarios demonstrate that PVFP can significantly reduce the end-to-end latency of SFCs at the medium resource expenditures to place VNFs in multiple administrative domains, compared with the state-of-the-art mechanisms.}
}


@article{DBLP:journals/ton/MazzolaSMB24,
	author = {Fabr{\'{\i}}cio M. Mazzola and
                  Augusto Setti and
                  Pedro de B. Marcos and
                  Marinho P. Barcellos},
	title = {Analyzing Remote Peering Deployment and Its Implications for Internet
                  Routing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2950--2959},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3375898},
	doi = {10.1109/TNET.2024.3375898},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MazzolaSMB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet eXchange Points (IXPs) have significantly transformed the structure and economics of the Internet by allowing many nearby networks to connect directly, avoiding the need for service providers. These large IXPs are so beneficial that they are not just used by nearby networks, but also by far away Autonomous Systems (AS). This is made possible by Remote Peering (RP), which typically involves the use of RP resellers to access remote IXPs. In this paper, we evaluate the effects of RP on four different routing aspects, using a representative group of IXPs located on three continents: (a) growth of RP deployment over one and a half years; (b) presence of route announcement mispractices (when networks prioritize the remote IXP over the local IXP), which are associated to routing anomalies; (c) reliability of RP interfaces and (d) adoption of RP-related BGP communities, i.e. to perform traffic engineering to remote peers. We make our data and results available to the community via a web portal.}
}


@article{DBLP:journals/ton/Anselmi24,
	author = {Jonatha Anselmi},
	title = {Asynchronous Load Balancing and Auto-Scaling: Mean-Field Limit and
                  Optimal Design},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2960--2971},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3368130},
	doi = {10.1109/TNET.2024.3368130},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Anselmi24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We develop a Markovian framework for load balancing that combines classical algorithms such as Power-of- d\nwith auto-scaling mechanisms that allow the net service capacity to scale up or down in response to the current load on the same timescale as job dynamics. Our framework is inspired by serverless platforms, such as Knative, where servers are software functions that can be flexibly instantiated in milliseconds according to scaling rules defined by the users of the serverless platform. The main question is how to design such scaling rules to minimize user-perceived delay performance while ensuring low energy consumption. For the first time, we investigate this problem when the auto-scaling and load balancing processes operate asynchronously (or proactively), as in Knative. In contrast to the synchronous (or reactive) paradigm, asynchronism brings the advantage that jobs do not necessarily need to wait any time a scale-up decision is taken. In our main result, we find a general condition on the structure of scaling rules able to drive mean-field dynamics to delay and relative energy optimality, i.e., a situation where both the user-perceived delay and the relative energy waste induced by idle servers vanish in the limit where the network demand grows to infinity in proportion to the nominal service capacity. The identified condition suggests to scale up the current net capacity if and only if the mean demand exceeds the rate at which servers become idle and active. Finally, we propose a family of scaling rules that satisfy our optimality condition. Numerical simulations demonstrate that these rules provide better delay performance than existing synchronous auto-scaling schemes while inducing almost the same power consumption.}
}


@article{DBLP:journals/ton/FuLX24,
	author = {Chuanpu Fu and
                  Qi Li and
                  Ke Xu},
	title = {Flow Interaction Graph Analysis: Unknown Encrypted Malicious Traffic
                  Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2972--2987},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3370851},
	doi = {10.1109/TNET.2024.3370851},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FuLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays traffic on the Internet has been widely encrypted to protect its confidentiality and privacy. However, traffic encryption is always abused by attackers to conceal their malicious behaviors. Since encrypted malicious traffic is similar to benign flows, it can easily evade traditional detection. In particular, the existing encrypted traffic detection methods are supervised which rely on the prior knowledge of known attacks (e.g., labeled datasets). Detecting unknown encrypted malicious traffic, which does not require prior knowledge, is still an open problem. In this paper, we propose HyperVision, an unsupervised machine learning (ML) based malicious traffic detection system. Particularly, HyperVision is able to detect unknown patterns of encrypted malicious traffic by utilizing a graph built upon flow interaction patterns, instead of learning the features of specific known attacks. We develop an unsupervised graph learning method to detect abnormal interaction patterns by analyzing the graph features, which allows HyperVision to detect unknown attacks without requiring any labeled datasets. Moreover, we establish an information theory model to prove the effectiveness of HyperVision. We show the performance of HyperVision by real-world experiments with 140 attacks. The experimental results illustrate that HyperVision outperforms the state-of-the-art methods by 13.9% accuracy improvement. Moreover, HyperVision achieves 15.82 Mpps detection throughput with the average detection latency of 0.29s.}
}


@article{DBLP:journals/ton/ShaoCH24,
	author = {Qi Shao and
                  Man Hon Cheung and
                  Jianwei Huang},
	title = {Strategic Pricing and Information Disclosure in Crowdfunding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {2988--3001},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3374748},
	doi = {10.1109/TNET.2024.3374748},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShaoCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a crowdfunding campaign, the project creator determines various campaign decisions, such as the pricing and information revelation strategy, to maximize the funding. Each contributor has a high or low valuation for the project. In this paper, we present a study on how the contributors’ random arrival and pledging process affect the creator’s campaign decisions. Specifically, a creator first determines and announces the pricing and information disclosure strategy before the campaign starts. Then contributors randomly arrive and choose their pledging decisions. Contributors make their decisions based on not only the disclosed status so far but also the estimation of the later contributors’ random arrival and pledging decisions. This randomness and complicated decision coupling among contributors render the analysis challenging. Nonetheless, we prove that contributors’ equilibrium decisions follow a threshold structure. Based on this, we propose an algorithm to solve the creator’s optimal campaign decisions. Our analysis on the creator’s strategic information disclosure shows that the contributors’ prior belief on the fraction of high-valuation contributors is critical. Specifically, when the prior belief is high, the creator withholds the pledging status information from the contributors until the campaign ends. When the prior belief is low, the creator should disclose the information update immediately once the first contributor arrives. Such an early disclosure increases the confidence of later contributors and motivates their pledging decisions. Our numerical results show that with strategic information disclosure, the creator can increase the funding by 100% on average, compared with the immediate-disclosure policy widely adopted by crowdfunding platforms.}
}


@article{DBLP:journals/ton/ZhuLLPZSJCKLYXYWY24,
	author = {Shunmin Zhu and
                  Jianyuan Lu and
                  Biao Lyu and
                  Tian Pan and
                  Shize Zhang and
                  Xiaoqing Sun and
                  Chenhao Jia and
                  Xin Cheng and
                  Daxiang Kang and
                  Yilong Lv and
                  Fukun Yang and
                  Xiaobo Xue and
                  Xihui Yang and
                  Zhiliang Wang and
                  Jiahai Yang},
	title = {Proactive Telemetry in Large-Scale Multi-Tenant Cloud Overlay Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3002--3017},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3381786},
	doi = {10.1109/TNET.2024.3381786},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuLLPZSJCKLYXYWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {At present, public clouds have served millions of tenants. To provide reliable services, cloud vendors need to perceive health status of the cloud network by building a telemetry system to detect possible network failures. While telemetry systems for physical networks have been extensively studied, research on telemetry systems for virtual networks is still insufficient. Different from physical networks, we conclude that building a virtual network telemetry system faces new challenges of feasibility, efficiency, and effectiveness. Specifically, we need to 1) protect privacy of tenants and adapt to heterogeneous middleboxes at the data plane; 2) handle frequent virtual network topology updates and compress large-scale measurement paths for millions of tenants at the control plane; 3) analyze telemetry results to locate network failures at the analysis plane. To address these challenges, we present Zoonet, a proactive virtual network telemetry system for multi-tenant clouds. At the data plane, Zoonet uses host agent and arp-ping to protect tenants’ privacy and defines an elegant generalization of ping and traceroute, which can work on heterogeneous middleboxes. At the control plane, Zoonet conducts update batch processing and substantial probing path pruning to lessen the overhead. At the analysis plane, Zoonet reduces noises and aggregates alerts based on temporal and spatial correlation and conducts the hop-by-hop telemetry mode to locate failures. Zoonet has been deployed in Alibaba Cloud for over two years, covering tens of cloud regions, hundreds of thousands of servers. We become increasingly reliant on Zoonet as it reduces 86% of the personnel engaged in troubleshooting.}
}


@article{DBLP:journals/ton/ChengWLW24,
	author = {Minquan Cheng and
                  Youlong Wu and
                  Xianxian Li and
                  Dianhua Wu},
	title = {Asymptotically Optimal Coded Distributed Computing via Combinatorial
                  Designs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3018--3033},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3372698},
	doi = {10.1109/TNET.2024.3372698},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChengWLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coded distributed computing (CDC) introduced by Li et al. can greatly reduce the communication load for MapReduce computing systems. In the cascaded CDC with K workers, N input files and Q output functions, each input file will be mapped by r workers and each output function will be computed by s workers such that coding techniques can be applied to create multicast opportunities. The main drawback of most existing CDC schemes is that they require the original data to be split into a large number of input files that grows exponentially with K , which would significantly increase the coding complexity and degrade the system performance. In this paper, we first use a classical combinatorial structure t -design, for any integer t\\geq 2 , to develop a low-complexity and communication-efficient CDC with r=s . Our scheme has much smaller N and Q than the existing schemes under the same parameters K , r , and s ; and achieves smaller communication loads compared with the state-of-the-art schemes when K is relatively large. Remarkably, unlike the previous schemes that realize on large operation fields, our scheme operates in one-shot communication on the minimum binary field \\mathbb {F}_{2} . With a derived lower bound on the communication load under one-shot linear delivery, we show that the t -design scheme is asymptotically optimal. Furthermore, we show that our construction method can incorporate the other combinatorial structures that have a similar property to t -design. For instance, we use t -GDD to obtain another one-shot asymptotically optimal CDC scheme over \\mathbb {F}_{2} that has different parameters from t -design. Finally, we show that our construction method can also be used to construct CDC schemes with r\\neq s that have small file number and output function number.}
}


@article{DBLP:journals/ton/HuangGH24,
	author = {Jiatai Huang and
                  Leana Golubchik and
                  Longbo Huang},
	title = {When Lyapunov Drift Based Queue Scheduling Meets Adversarial Bandit
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3034--3044},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3374755},
	doi = {10.1109/TNET.2024.3374755},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangGH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study scheduling of a queueing system with zero knowledge of instantaneous network conditions. We consider a one-hop single-server queueing system consisting of K queues, each with time-varying and non-stationary arrival and service rates. Our scheduling approach builds on an innovative combination of adversarial bandit learning and Lyapunov drift minimization, without knowledge of the instantaneous network state (the arrival and service rates) of each queue. We then present two novel algorithms SoftMW (SoftMaxWeight) and SSMW (Sliding-window SoftMaxWeight), both capable of stabilizing systems that can be stabilized by some (possibly unknown) sequence of randomized policies whose time-variation satisfies a mild condition. We further generalize our results to the setting where arrivals and departures only have bounded moments instead of being deterministically bounded and propose SoftMW+ and SSMW+ that are capable of stabilizing the system. As a building block of our new algorithms, we also extend the classical EXP3.S algorithm for multi-armed bandits to handle unboundedly large feedback signals, which can be of independent interest.}
}


@article{DBLP:journals/ton/MiYWDSZCF24,
	author = {Liang Mi and
                  Tingting Yuan and
                  Weijun Wang and
                  Haipeng Dai and
                  Lin Sun and
                  Jiaqi Zheng and
                  Guihai Chen and
                  Xiaoming Fu},
	title = {Accelerated Neural Enhancement for Video Analytics With Video Quality
                  Adaptation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3045--3060},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3375108},
	doi = {10.1109/TNET.2024.3375108},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MiYWDSZCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quality of the video stream is the key to neural network-based video analytics. However, low-quality video is inevitably collected by existing surveillance systems because of poor-quality cameras or over-compressed/pruned video streaming protocols, e.g., as a result of upstream bandwidth limit. To address this issue, existing studies use quality enhancers (e.g., neural super-resolution) to improve the quality of videos (e.g., resolution) and eventually ensure inference accuracy. Nevertheless, directly applying quality enhancers does not work in practice because it will introduce unacceptable latency. In this paper, we present AccDecoder, a novel accelerated decoder for real-time and neural-enhanced video analytics, selects a few frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality and inference then reuse on the unselected ones. Next, we extend AccDecoder to AccDecoder+ by formulating the resolution-involved Markov decision process (MDP) to achieve resolution adaptation; it aims to trade accuracy and latency corresponding under various video resolutions. Proved by experiments, AccDecoder provides efficient inference capability via filtering important frames using DRL for DNN-based inference and reusing the results for the other frames via extracting the reference relationship among frames and blocks, which contributes 6-21% accuracy improvement and a latency reduction of 20-80% than baselines. Compared with AccDecoder, AccDecoder+ achieves an additional 2-7% accuracy improvement.}
}


@article{DBLP:journals/ton/WangHL24,
	author = {Fei Wang and
                  Ethan Hugh and
                  Baochun Li},
	title = {More Than Enough is Too Much: Adaptive Defenses Against Gradient Leakage
                  in Production Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3061--3075},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3377655},
	doi = {10.1109/TNET.2024.3377655},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With increasing concerns on privacy leakage from gradients, various attack mechanisms emerged to recover private data from gradients, which challenged the primary advantage of privacy protection in federated learning. However, we cast doubt upon the real impact of these gradient leakage attacks on production federated learning systems. By taking away several impractical assumptions that the literature has made, we find that these attacks pose a limited degree of threat to the privacy of raw data. In this paper, through a comprehensive evaluation of existing gradient leakage attacks in a federated learning system with practical assumptions, we have systematically analyzed their effectiveness under a wide range of configurations. We first present key priors required to make the attack possible or stronger, such as a narrow distribution of initial model weights, as well as inversion at early stages of training. We then propose a new lightweight defense mechanism that provides sufficient and self-adaptive protection against time-varying levels of the privacy leakage risk throughout the federated learning process. Our proposed defense, called OUTPOST, selectively adds Gaussian noise to gradients at each update iteration according to the Fisher information matrix, where the level of noise is determined by the privacy leakage risk quantified by the spread of model weights at each layer. To limit the computation overhead and training performance degradation, OUTPOST only performs perturbation with iteration-based decay. Our experimental results demonstrate that OUTPOST can achieve a much better tradeoff than the state-of-the-art with respect to convergence performance, computational overhead, and protection against gradient leakage attacks.}
}


@article{DBLP:journals/ton/ChenHTLWSXHXPTL24,
	author = {Min{-}Yue Chen and
                  Yiwen Hu and
                  Guan{-}Hua Tu and
                  Chi{-}Yu Li and
                  Sihan Wang and
                  Jingwen Shi and
                  Tian Xie and
                  Ren{-}Chieh Hsu and
                  Li Xiao and
                  Chunyi Peng and
                  Zhaowei Tan and
                  Songwu Lu},
	title = {Taming the Insecurity of Cellular Emergency Services {(9-1-1):} From
                  Vulnerabilities to Secure Designs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3076--3091},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3379292},
	doi = {10.1109/TNET.2024.3379292},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenHTLWSXHXPTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular networks, vital for delivering emergency services, enable mobile users to dial emergency calls (e.g., 9–1-1 in the U.S.), which are forwarded to public safety answer points (PSAPs). Regulatory requirements allow anonymous user equipment (UE) without a SIM card or valid mobile subscription to access these services. However, supporting emergency services for anonymous UEs introduces different operations, expanding the attack surface of cellular infrastructure. In this study, we explore the insecurity of cellular emergency services, identifying six security vulnerabilities. These vulnerabilities can be exploited for free data service attacks against carriers and data DoS/overcharge and denial of cellular emergency service (DoCES) attacks against mobile users. Experimental validation in networks of three major U.S. carriers and two major Taiwan carriers demonstrates the global impact of our findings. Finally, we propose and prototype standard-compliant remedies to mitigate these vulnerabilities.}
}


@article{DBLP:journals/ton/SalehiDRPDIC24,
	author = {Batool Salehi and
                  Utku Demir and
                  Debashri Roy and
                  Suyash Pradhan and
                  Jennifer G. Dy and
                  Stratis Ioannidis and
                  Kaushik R. Chowdhury},
	title = {Multiverse at the Edge: Interacting Real World and Digital Twins for
                  Wireless Beamforming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3092--3110},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3377114},
	doi = {10.1109/TNET.2024.3377114},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SalehiDRPDIC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Creating a digital world that closely mimics the real world with its many complex interactions and outcomes is possible today through advanced emulation software and ubiquitous computing power. Such a software-based emulation of an entity that exists in the real world is called a ‘digital twin’. In this paper, we consider a twin of a wireless millimeter-wave band radio that is mounted on a vehicle and show how it speeds up directional beam selection in mobile environments. To achieve this, we go beyond instantiating a single twin and propose the ‘ Multiverse’ paradigm, with several possible digital twins attempting to capture the real world at different levels of fidelity. Towards this goal, this paper describes (i) a decision strategy at the vehicle that determines which twin must be used given the latency limitation, and (ii) a self-learning scheme that uses the Multiverse-guided beam outcomes to enhance DL-based decision-making in the real world over time. Our work is distinguished from prior works as follows: First, we use a publicly available RF dataset collected from an autonomous car for creating different twins. Second, we present a framework with continuous interaction between the real world and Multiverse of twins at the edge, as opposed to a one-time emulation that is completed prior to actual deployment. Results reveal that Multiverse offers up to 79.43% and 85.22% top-10 beam selection accuracy for LOS and NLOS scenarios, respectively. Moreover, we observe 67.70–90.79% improvement in beam selection time compared to 802.11ad standard and 5G-NR standards.}
}


@article{DBLP:journals/ton/LiuWWW24,
	author = {Yang Liu and
                  Xi Wang and
                  Xiaoqi Wang and
                  Zhen Wang},
	title = {Fast Outbreak Sense and Effective Source Inference via Minimum Observer
                  Set},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3111--3125},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3382546},
	doi = {10.1109/TNET.2024.3382546},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the Fast outbreak Sensing and Effective diffusion source Inferring (FSEI) problem, which assumes that the state of nodes in a particularly chosen observer set can be monitored if necessary and aims to optimize the observer set such that outbreaks can be timely detected and their sources can be effectively targeted. We propose three approaches to tackle the FSEI problem: Greedy Strategy (GS), Network-Topology-based Method (NTM), and Hybrid Method (HM). Among them, GS relies on collected outbreaks and constructs the observer set by iteratively choosing and removing the node that minimizes the product of sensing time and source targeting cost of the remaining network. For NTM, we also consider the remaining network and introduce a novel strategy to optimize its topology via simultaneously minimizing the adjoining component size and ratio of the first and second moments. HM is a combination of GS and NTM, considering the submodular property of GS on the minimization of the sensing time and well approximation of the component size on the optimization of the source targeting. We perform extensive experiments on over 200 empirical networks, using various diffusion models, to validate the proposed methods. The results demonstrate that our approaches consistently outperform the state-of-the-art. We believe that the model and methodology presented in this paper can be readily applied to real-world scenarios such as combating misinformation and controlling diffusions of information or disease.}
}


@article{DBLP:journals/ton/WangXGWFLWW24,
	author = {Xiaoliang Wang and
                  Ke Xu and
                  Yangfei Guo and
                  Haiyang Wang and
                  Songtao Fu and
                  Qi Li and
                  Bin Wu and
                  Jianping Wu},
	title = {Toward Practical Inter-Domain Source Address Validation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3126--3141},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3377116},
	doi = {10.1109/TNET.2024.3377116},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangXGWFLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet Protocol (IP) is the most fundamental building block of the Internet. However, it provides no explicit notion of packet-level authenticity. Such a weakness allows malicious actors to spoof IP packet headers and launch a wide variety of attacks. Meanwhile, the highly decentralized management of Internet infrastructure makes large-scale source address validation challenging in terms of overhead, validity, and flexibility. This paper presents a practical anti-spoofing approach, Source Address Validation Architecture eXternal (SAVA-X). SAVA-X introduces the concept of Address Domain to enable address validation in finer, prefix-level granularity. The address domains are organized in nested hierarchies to provide higher scalability and lower maintenance costs for partial deployment. We implement SAVA-X on commercial backbone routers and the P4 platform. The experiments indicate that the hardware implementation of SAVA-X can achieve 98% throughput on 100 Gbps links and close to the native IP forwarding in per-packet overhead, with less than 10 microseconds additional processing latency.}
}


@article{DBLP:journals/ton/HanWQCZSFLG24,
	author = {Rongxin Han and
                  Jingyu Wang and
                  Qi Qi and
                  Dezhi Chen and
                  Zirui Zhuang and
                  Haifeng Sun and
                  Xiaoyuan Fu and
                  Jianxin Liao and
                  Song Guo},
	title = {Dynamic Network Slice for Bursty Edge Traffic},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3142--3157},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3376794},
	doi = {10.1109/TNET.2024.3376794},
	timestamp = {Fri, 20 Sep 2024 15:38:09 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HanWQCZSFLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge network slicing promises better utilization of network resources by dynamically allocating resources on demand. However, addressing the imbalance between slice resources and user demands becomes challenging when complex user behaviors lead to bursty traffic within the edge network. Hence, we propose a comprehensive dynamic slice strategy with two coupled sub-strategies (i) bursty-sensitive slice resource coordination and (ii) proactive demand resource matching to find an optimal balance. For obtaining stable strategies, the edge network with bursty traffic is formulated as a bi-level Lyapunov optimization problem. Then we propose a resource allocation and request redirection (RA-RR) algorithm with polynomial complexity by introducing deep reinforcement learning to guarantee real-time. Specifically, two agents are trained to solve two sub-strategies, and the Lyapunov drift-plus-penalty function is used as the reward to keep queues stable. RA-RR is responsive to fluctuations in demand and realizes an efficient interaction of coupled decision-making. Moreover, a training method based on alternating optimization is designed to ensure convergence of the RA-RR algorithm. Experiments demonstrate that the proposal can maximize network revenue while ensuring the stability of slice services when edge traffic bursts, and has an average improvement of 20.4% compared with comparisons.}
}


@article{DBLP:journals/ton/CordeschiRB24,
	author = {Nicola Cordeschi and
                  Floriano De Rango and
                  Andrea Baiocchi},
	title = {Optimal Back-Off Distribution for Maximum Weighted Throughput in {CSMA}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3158--3172},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3387322},
	doi = {10.1109/TNET.2024.3387322},
	timestamp = {Mon, 09 Dec 2024 22:46:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CordeschiRB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a generalized version of Carrier-Sense Multiple Access (CSMA), where the contention window size is a constant and the back-off probability distribution can be varied. We address the optimization of a weighted throughput metric, identifying the optimal back-off Probability Density Function (PDF). We give a simple fixed-point algorithm to compute the optimal PDF and prove that the solution is unique. The weighted throughput definition caters for aspects other than the mere channel utilization. It reduces to plain utilization (normalized throughput) when all weights are equal to 1. We also reconnect our result to the classic analysis of saturated non-persistent CSMA, as introduced in the seminal paper by Tobagi and Kleinrock, proving formally that the modeling assumptions of that work, that lead to a Geometric PDF of back-off, actually correspond to the throughput-optimal choice, provided that the ratio of the Geometric PDF is suitably chosen.}
}


@article{DBLP:journals/ton/Wang24,
	author = {Chih{-}Chun Wang},
	title = {Optimal AoI for Systems With Queueing Delay in Both Forward and Backward
                  Directions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3173--3188},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3379895},
	doi = {10.1109/TNET.2024.3379895},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Wang24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age-Of-Information (AoI) is a metric that focuses directly on the application-layer objectives, and a canonical AoI minimization problem is the update-through-queues models. Existing results in this direction fall into two categories: The open-loop setting for which the sender is oblivious of the packet departure time, versus the closed-loop setting for which the decision is based on instantaneous Acknowledgment (ACK). Neither setting perfectly reflects modern networked systems, which almost always rely on feedback that experiences some delay. Motivated by this observation, this work subjects the ACK traffic to a second queue so that the closed-loop decision is made based on delayed feedback. Near-optimal schedulers have been devised, which smoothly transition from the instantaneous-ACK to the open-loop schemes depending on how long the feedback delay is. The results quantify the benefits of delayed feedback for AoI minimization in the update-through-queues systems.}
}


@article{DBLP:journals/ton/LiuRZYZ24,
	author = {Jiani Liu and
                  Ju Ren and
                  Yongmin Zhang and
                  Sheng Yue and
                  Yaoxue Zhang},
	title = {{SESAME:} {A} Resource Expansion and Sharing Scheme for Multiple Edge
                  Services Providers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3189--3204},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3377908},
	doi = {10.1109/TNET.2024.3377908},
	timestamp = {Mon, 09 Sep 2024 14:52:21 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuRZYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a potential computing solution for fast-growing mobile and IoT applications, edge computing has been developed rapidly. However, due to the relatively limited resources of each edge node, it is difficult for edge nodes to provide quality-guaranteed services to dynamic and massive computation tasks individually. To address this challenge, this paper proposes a two-stage resource expansion and sharing scheme, named SESAME, to enable resource sharing among the edge nodes within/across multiple edge service providers (ESPs), to improve the overall efficiency of the edge computing system. To facilitate the operation and reduce complexity, the resource management scheme has both long-term and short-term decision periods. During the long-term period, an optimal conservative estimation-based resource expansion and pricing strategy has been designed to ensure the system stability and the interests of ESPs. During the short-term period, a resource-sharing strategy considering the internal and external behaviors of ESPs has been proposed to reduce resource-sharing costs while fully utilizing internal resources. In such a way, the resources from different ESPs can collaborate efficiently. Extensive experiments on real datasets show that our algorithm can effectively reduce ESP costs and improve system stability.}
}


@article{DBLP:journals/ton/ZhangAZLLPZLM24,
	author = {Huanhuan Zhang and
                  Congkai An and
                  Anfu Zhou and
                  Chaoyue Li and
                  Xi Liu and
                  Jialiang Pei and
                  Yifan Zhu and
                  Liang Liu and
                  Huadong Ma},
	title = {Reviving Peer-to-Peer Networking for Scalable Crowdsourced Live Video
                  Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3205--3220},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3380395},
	doi = {10.1109/TNET.2024.3380395},
	timestamp = {Mon, 20 Jan 2025 08:50:01 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangAZLLPZLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rising crowdsourced live video streaming (CLVS) poses great challenges to Internet transport scalability, where a broadcaster’s live video is expected to reach thousands and even millions of viewers in real time. To accommodate such huge concurrent video traffic, the de-facto solution is to employ content delivery network (CDN), which distributes the traffic spatially relative to end viewers, using geographically distributed servers. However, our measurement study over a top operational CLVS platform reveals that CDN is not scalable enough, i.e., it loses efficacy, particularly during busy time and leads to tremendous QoE degradation, e.g., 33.3% video bitrate reduction, in comparison to network idle time. In this work, we propose Spider, which revives the peer-to-peer (P2P) networking principle to extend the scalability of CLVS system. Beyond traditional P2P for elastic data transmission, Spider retrofits P2P to meet the stringent low-latency requirements of CLVS: proposing a “pair-push” streaming mode to tame the excessive signaling latency; designing a QoE-driven peer pairing algorithm to tackle the Internet path variation and CLVS viewer dynamics. We implement, deploy and evaluate Spider in real-world over 20.9 thousand video sessions. Compared to the de-facto CDN solution, Spider achieves remarkable gains, e.g., video stall rate reductions of 52.57%, video quality gains of 8.22%, and even 66% CDN bandwidth saving. The results validate the feasibility and practicability of embracing P2P for low-latency live video communication for the first time.}
}


@article{DBLP:journals/ton/MouhoubLM24,
	author = {Noureddine Mouhoub and
                  Mohamed Lamine Lamali and
                  Damien Magoni},
	title = {A Transitive Closure Algorithm for Routing With Automatic Tunneling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3221--3236},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3378717},
	doi = {10.1109/TNET.2024.3378717},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MouhoubLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most current routing protocols are based on path computation algorithms in graphs (e.g., Dijkstra, Bellman-Ford, etc.). These algorithms have been studied for a long time and are very well understood, both in a centralized and distributed context, as long as they are applied to a network having a single communication protocol. The problem becomes more complex in the multi-protocol case, where there is a possibility of encapsulation of some network protocols into others, therefore inducing nested tunnels. The classic algorithms cited above no longer work in this case because they cannot manage the protocol encapsulations and the corresponding protocol stacks. In this work, we propose a highly parallelizable algorithm that takes into account protocol encapsulations as well as protocol conversions in order to compute shortest paths in a multi-protocol network. To achieve this computation efficiently, we study the transitive closure between subpaths (i.e., the concatenation of two subpaths to obtain a longer one) in the case where each subpath induces a protocol stack, and thus tunnels. Leveraging on Software-Defined Networks with a controller having a highly parallel architecture enables us to compute the routing tables of all nodes in a very efficient way. Experimentation results on both random and realistic topologies show that our algorithm outperforms the previous solutions proposed in the literature.}
}


@article{DBLP:journals/ton/YangCSPS24,
	author = {Wenjun Yang and
                  Lin Cai and
                  Shengjie Shu and
                  Jianping Pan and
                  Amir Sepahi},
	title = {{MAMS:} Mobility-Aware Multipath Scheduler for {MPQUIC}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3237--3252},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3382269},
	doi = {10.1109/TNET.2024.3382269},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangCSPS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-homing technologies are promising to support seamless handoff and non-interrupted transmissions. Scheduling packets across multiple paths, however, has the known issue of out-of-order (OFO) due to the heterogeneity of the paths, which is detrimental to users’ quality of experience (QoE). Wireless link characteristics undergo a fast change over time in mobile environments, thus aggravating the OFO issue. In this paper, we present a novel mobility-aware multipath QUIC (MMQUIC) framework in which interactions between link and transport layers are introduced so that the scheduler at a mobile sender is aware of uplink variations, and a new ACK packet structure is designed to inform the scheduler of downlink variations when the receiver is mobile. Based on MMQUIC, a Mobility-Aware Multipath Scheduler (MAMS) is developed, which forecasts the path conditions in successive time slots based on historical and current end-to-end (E2E) path conditions, along with wireless uplink/downlink conditions, and pre-allocates packets on multiple paths accordingly. We conduct a series of experiments to evaluate the performance of MAMS using network simulator 3 (ns-3). Simulation results demonstrate that MAMS effectively leverages the information related to mobility, achieving substantial performance gains w.r.t. the goodput and packet delay distribution under different mobility patterns.}
}


@article{DBLP:journals/ton/ShreedharKY24,
	author = {Tanya Shreedhar and
                  Sanjit K. Kaul and
                  Roy D. Yates},
	title = {{ACP+:} An Age Control Protocol for the Internet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3253--3268},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3380622},
	doi = {10.1109/TNET.2024.3380622},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShreedharKY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present the age control protocol ACP+, a transport layer protocol that regulates the rate at which update packets carrying information from a source are sent over the Internet to a monitor. The source would like to minimize the average age of information at the monitor. Extensive experimentation helps shed light on age control over the current Internet and its implications for sources sending updates over a shared wireless access to monitors in the cloud. Surprisingly, age minimizing rates over fast Internet paths are about 0.5 Mbps, which is a small fraction, for example, of link rates supported by WiFi wireless access technology. We also show that congestion control algorithms employed by the Transmission Control Protocol (TCP), including hybrid approaches that achieve higher throughputs at lower delays than traditional loss-based congestion control, are unsuitable for age control.}
}


@article{DBLP:journals/ton/LiuDXLBLGZC24,
	author = {Jiaqian Liu and
                  Haipeng Dai and
                  Rui Xia and
                  Meng Li and
                  Ran Ben Basat and
                  Rui Li and
                  Rong Gu and
                  Jiaqi Zheng and
                  Guihai Chen},
	title = {A Generic Framework for Finding Special Quadratic Elements in Data
                  Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3269--3284},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3392029},
	doi = {10.1109/TNET.2024.3392029},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuDXLBLGZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding special items in data streams, like heavy hitters, top-k items, and persistent items, has always been a hot topic in the field of network measurement. While data streams nowadays are usually high-dimensional, most prior works optimize data structures to accurately find special items according to a certain primary dimension and yield little insight into the correlations between dimensions, where the dimension can be a single data dimension or a combination of multiple data dimensions. Therefore, we propose to find special quadratic elements in data streams to reveal the close correlations between the primary and secondary dimensions. Here, both the primary and secondary dimensions are selected according to specific application purposes. Based on the special items mentioned above, we extend our problem to three applications related to heavy hitters, top-k, and persistent items, and design a generic framework DUET to process them. We analyze the error bound of our algorithm theoretically and conduct extensive experiments on four publicly available data sets. Our experimental results show that DUET can achieve 3.5 times higher throughput and three orders of magnitude lower average relative error than cutting-edge algorithms. Moreover, we propose an optimized framework based on DUET, namely O-DUET, to further improve the estimation accuracy. We also discuss a hardware-version DUET and deploy it on Tofino.}
}


@article{DBLP:journals/ton/ZhuG24,
	author = {Jianhang Zhu and
                  Jie Gong},
	title = {Optimizing Peak Age of Information in {MEC} Systems: Computing Preemption
                  and Non-Preemption},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3285--3300},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3384706},
	doi = {10.1109/TNET.2024.3384706},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhuG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness. In many applications, update packets need to be computed before being delivered to a destination. Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness. In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system. We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing. We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions. In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution. Our numerical simulation results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems.}
}


@article{DBLP:journals/ton/HuangCMWLW24,
	author = {Haojun Huang and
                  Yiming Cai and
                  Geyong Min and
                  Haozhe Wang and
                  Gaoyang Liu and
                  Dapeng Oliver Wu},
	title = {Accurate Prediction of Network Distance via Federated Deep Reinforcement
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3301--3314},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3383479},
	doi = {10.1109/TNET.2024.3383479},
	timestamp = {Thu, 20 Feb 2025 13:59:50 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangCMWLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A large number of distributed applications necessitate accurate network distance, for example, in the form of delay or latency, to ensure the Quality of Service (QoS). Due to high network measurement overhead and severe traffic congestion, network distance prediction has been introduced, instead of direct network measurements, to infer the unknown network distance with the partial measurements. However, most existing efforts neglect to fully capitalize on the potential latent factors, such as spatial correlations, long-existing temporal results and multi-rule exploration fusion, to achieve better accuracies with quicker convergence. To fill this gap, in this paper, we propose an Accurate Prediction of Network Distance (APND) solution via Federated Deep Reinforcement Learning (FDRL), which has four novel features distinguishing from the previous work. Firstly, a local feature-based matrix with low rank is established in each network cluster, referring to a set of neighbor nodes, to represent the potential spatial correlations among reachable node-pairs. Secondly, the parallel FDRL-based matrix factorization with multi-rule exploration fusion is introduced into APND and executed in all local clusters to minimize prediction errors and accelerate learning convergence. Thirdly, the long-existing learning experience is designed for local model training via Deep Reinforcement Learning (DRL) with rapid convergence. Fourthly, following the real-world routing paths, the cross-domain network nodes are simultaneously classified into adjacent clusters, built on the spatial correlations among them, and their coordinates will be further refined with error-based and average-based policies. Extensive experiments built on available real-world datasets illustrate that APND can accurately predict network distance compared with state-of-the-art approaches at the moderate computing cost.}
}


@article{DBLP:journals/ton/XiaCHZG24,
	author = {Xianjin Xia and
                  Qianwu Chen and
                  Ningning Hou and
                  Yuanqing Zheng and
                  Tao Gu},
	title = {HyLink: Toward High Throughput LPWANs With LoRa Compatible Communication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3315--3330},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3388890},
	doi = {10.1109/TNET.2024.3388890},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaCHZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the design and implementation of HyLink which aims to fill the gap between limited link capacity of LoRa and the diverse bandwidth requirements of IoT systems. At the heart of HyLink is a novel technique named parallel Chirp Spread Spectrum modulation, which tunes the number of modulated symbols to adapt bit-rates according to channel conditions. Over strong link connections, HyLink fully exploits the link capability to transmit more symbols and thus transforms good channel SNRs to high link throughput. While for weak links, it conservatively modulates one symbol and concentrates all transmit power onto the symbol to combat poor channels, which can achieve the same performance as legacy LoRa. HyLink addresses a series of technical challenges on encoding and decoding of multiple payloads in a single packet, aiming at amortizing communication overheads in terms of channel access, radio-on power, transmission air-time, etc. We perform extensive experiments to evaluate the effectiveness of HyLink. Evaluations show that HyLink produces up to 10\\times higher bit rates than LoRa when channel SNRs are higher than \\mathrm {5 dB} . HyLink inter-operates with legacy LoRa devices and can support new emerging traffic-intensive IoT applications.}
}


@article{DBLP:journals/ton/ParkCLH24,
	author = {Jinwoo Park and
                  Byungkwon Choi and
                  Chunghan Lee and
                  Dongsu Han},
	title = {Graph Neural Network-Based SLO-Aware Proactive Resource Autoscaling
                  Framework for Microservices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3331--3346},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3393427},
	doi = {10.1109/TNET.2024.3393427},
	timestamp = {Sun, 19 Jan 2025 13:55:33 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ParkCLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microservice is an architectural style widely adopted in various latency-sensitive cloud applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing the resource utilization of microservices. However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. In this paper, we present GRAF, a graph neural network-based SLO-aware proactive resource autoscaling framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to (a) observe/estimate the impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. GRAF also handles a traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler. Moreover, we verify the scalability of GRAF on large-scale deployments, where GRAF saves 21.6% and 25.4% for CPU resources and memory resources, respectively.}
}


@article{DBLP:journals/ton/WangJLM24,
	author = {Sen Wang and
                  Zhongyuan Jiang and
                  Xinghua Li and
                  Jianfeng Ma},
	title = {K-Backup: Load- and TCAM-Aware Multi-Backup Fast Failure Recovery
                  in SDNs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3347--3360},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3386091},
	doi = {10.1109/TNET.2024.3386091},
	timestamp = {Sun, 19 Jan 2025 13:55:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangJLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Proactive Recovery (PR) mechanism in Software-Defined Networking (SDN) provides good failure recovery resilience for the Beyond Fifth-Generation/Sixth-Generation (B5G/6G) delay-sensitive applications. However, PR’s fixed single backup path policy for any flow and fine-grained backup forwarding rule configuration poses severe challenges for post-recovery congestion management and limited Ternary Content Addressable Memory (TCAM) space in SDN switches. To this end, we propose K-backup, a load- and TCAM-aware multi-backup fast failure recovery scheme for SDNs. Firstly, K-backup formulates and solves the congestion-aware multi-backup path planning problem for various failure scenarios, exploiting the inherent load diversity of multi-backup paths to minimize the post-recovery maximum link utilization. Secondly, K-backup aggregates flows sharing the same backup-path-weight pair on a link into a cascading table of Fast-Failover and SELECT groups. Meanwhile, each outputted backup path is labeled, and a corresponding label-matching flow table is configured for each intermediate switch to aggregate all flows on that path. Thirdly, K-backup dynamically adjusts the backup path update period based on the network load stabilization to reduce unnecessary controller overhead. Compared with state-of-the-art, K-backup achieves the lowest controller overhead, the best load balancing performance, the near-fewest TCAM space usage, and the near-shortest recovery time.}
}


@article{DBLP:journals/ton/SinghN24,
	author = {Amardip Kumar Singh and
                  Kim Khoa Nguyen},
	title = {Communication Efficient Compressed and Accelerated Federated Learning
                  in Open {RAN} Intelligent Controllers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3361--3375},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3384839},
	doi = {10.1109/TNET.2024.3384839},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SinghN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The disaggregated and hierarchical architecture of Open Radio Access Network (ORAN) with openness paradigm promises to deliver the ever demanding 5G services. Meanwhile, it also faces new challenges for the efficient deployment of Machine Learning (ML) models. Although ORAN has been designed with built-in Radio Intelligent Controllers (RIC) providing the capability of training ML models, traditional centralized learning methods may be no longer appropriate for the RICs due to privacy issues, computational burden, and communication overhead. Recently, Federated Learning (FL), a powerful distributed ML training, has emerged as a new solution for training models in ORAN systems. 5G use cases such as meeting the network slice Service Level Agreement (SLA) and Key Performance Indicator (KPI) monitoring for the smart radio resource management can greatly benefit from the FL models. However, training FL models efficiently in ORAN system is a challenging issue due to the stringent deadline of ORAN control loops, expensive compute resources, and limited communication bandwidth. Moreover, to deliver Grade of Service (GoS), the trained ML models must converge with acceptable accuracy. In this paper, we propose a second order gradient descent based FL training method named MCORANFed that utilizes compression techniques to minimize the communication cost and yet converges at a faster rate than state-of-the-art FL variants. We formulate a joint optimization problem to minimize the overall resource cost and learning time, and then solve it by the decomposition method. Our experimental results prove that MCORANFed is communication efficient with respect to ORAN system, and outperforms FL methods like MFL, FedAvg, and ORANFed in terms of costs and convergence rate.}
}


@article{DBLP:journals/ton/GongCLC24,
	author = {Xiaowen Gong and
                  Mingyu Chen and
                  Dongsheng Li and
                  Yang Cao},
	title = {Delay-Optimal Distributed Computation Offloading in Wireless Edge
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3376--3391},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3394789},
	doi = {10.1109/TNET.2024.3394789},
	timestamp = {Sun, 19 Jan 2025 13:55:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GongCLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we explore distributed edge computation offloading (DECO) that offloads computation to distributed edge devices connected wirelessly, which perform the offloaded computation in parallel. By integrating edge computing with parallel computing, DECO can substantially reduce the total computation delay. In particular, we study the fundamental problem of minimizing the total completion time of DECO. We show that the time-sharing based communication resource allocation always outperforms the bandwidth-sharing scheme, so that it suffices to focus on the time-sharing based communication scheduling. Based on the time-sharing scheme, we first establish some structural properties of the optimal communication scheduling policy. Then, given these properties, we develop an efficient algorithm that finds the optimal allocation of computation workloads. Next, based on the optimal computation allocation, we characterize the optimal scheduling order of communications, which exhibits an elegant structure: the optimal order is in the non-decreasing order of the ratio between a device’s computation rate and its communication time. Last, based on the optimal computation allocation and communication scheduling, we show that the optimal device selection problem is a submodular minimization problem, so that it can be solved efficiently using some existing methods. We further extend the study to the setting where devices are subject to maximum computation workload constraints, and develop an efficient algorithm that finds the optimal computation workload allocation. Our results provide useful insights for the optimal computation-communication co-design for DECO. We evaluate the theoretical findings using extensive simulations in both practical settings and controlled settings, which demonstrate the performance of DECO in practice and also the efficiency of our proposed schemes and algorithms for DECO.}
}


@article{DBLP:journals/ton/YangLD24,
	author = {Kang Yang and
                  Miaomiao Liu and
                  Wan Du},
	title = {RALoRa: Rateless-Enabled Link Adaptation for LoRa Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3392--3407},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3392342},
	doi = {10.1109/TNET.2024.3392342},
	timestamp = {Sun, 19 Jan 2025 13:55:30 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Both our experiments and previous studies show that LoRa links vary dynamically, which makes data transmission unreliable and consumes much energy of sensor nodes by retransmissions. This paper presents RALoRa, a Rateless-enabled link Adaptation system for LoRa networks. Rateless coding approaches the optimal data rate of a link by continuously transmitting encoded data with an initial data rate. However, LoRa’s modulation, Chirp Spread Spectrum (CSS), introduces unique challenges to rateless-enabled transmissions. With CSS, Spreading Factor (SF) simultaneously determines the initial data rate and the concurrent transmissions of multiple links. This dual role of SF requires the co-design of coding and networking. We thus formulate an optimization problem for allocating network resources (SF, frequency channels, and transmission power) and coding parameters (block size and packet size) to all sensor nodes. A key component of this formulation is a rateless-aware network model that estimates the data transmission results of sensor nodes based on their link quality and transmission setting. Given that the optimization problem for obtaining the best transmission setting of all sensor nodes is NP-complete, a two-stage heuristic algorithm is designed. A Kalman filter-based link quality predictor is developed to capture the link quality variation. We implement RALoRa on commodity LoRa hardware. Extensive experiments on a real testbed show that RALoRa extends the lifetime of LoRaWAN by 66.1%.}
}


@article{DBLP:journals/ton/KuzniarNGH24,
	author = {Carson Kuzniar and
                  Miguel C. Neves and
                  Vladimir Gurevich and
                  Israat Haque},
	title = {PoirIoT: Fingerprinting IoT Devices at Tbps Scale},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3408--3420},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3395278},
	doi = {10.1109/TNET.2024.3395278},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KuzniarNGH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The massive growth in popularity of household IoT devices has brought new capabilities to our lives while also bringing new challenges to network providers. In particular, large numbers of devices have been used to cause disruptions to critical Internet services. Understanding which devices are connected to a network empowers administrators to mitigate threats with target-specific interventions. This information is obtained by analyzing traffic through a process known as device fingerprinting. Current device fingerprinting solutions face major scalability issues in high-speed and high-volume networks, either relying on middleboxes to perform their tasks or targeting a single household. This paper introduces a novel in-network fingerprinting system, PoirIoT, capable of real-time, accurate, and scalable IoT device fingerprinting. Specifically, PoirIoT takes advantage of recent programmable switches and use standard packet metadata, length, and direction information to gain high-throughput and per-packet fingerprinting granularity. We show the effectiveness (100% device detection accuracy) of our solution using a publicly available dataset on a testbed consisting of Intel Tofino switches. Moreover, PoirIoT adds no additional latency to the regular traffic flow and utilizes minimal switch resources (e.g., memory).}
}


@article{DBLP:journals/ton/QiuSGS24,
	author = {Jiaming Qiu and
                  Jiayi Song and
                  Roch Gu{\'{e}}rin and
                  Henry Sariowan},
	title = {On the Benefits of Traffic "Reprofiling" the Multiple Hops
                  Case - Part {I}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3421--3436},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3392030},
	doi = {10.1109/TNET.2024.3392030},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiuSGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers networks where user traffic is regulated through deterministic traffic profiles, e.g., token buckets, and requires cleanrequires guaranteed clean hard delay bounds. The network’s goal is to minimize the resources it needs to meet those requirements clean bounds cleanbounds. The paper explores how reprofiling, i.e., proactively modifying how user traffic enters the network, can be of benefit. Reprofiling produces “smoother” flows but introduces an up-front access delay that forces tighter network delays. The paper explores this trade-off and demonstrates that, unlike what holds in the single-hop case, reprofiling can be of benefit cleanthat, unlike what holds in the single-hop case, reprofiling can be of benefit even when “optimal” clean“optimal” sophisticated clean schedulers are available at each hop cleanat each hop.}
}


@article{DBLP:journals/ton/FangXZYSX24,
	author = {Jin Fang and
                  Hongli Xu and
                  Gongming Zhao and
                  Zhuolong Yu and
                  Bingchen Shen and
                  Liguang Xie},
	title = {Accelerating Distributed Training With Collaborative In-Network Aggregation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3437--3452},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3387948},
	doi = {10.1109/TNET.2024.3387948},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FangXZYSX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surging scale of distributed training (DT) incurs significant communication overhead in datacenters, while a promising solution is in-network aggregation (INA). It leverages programmable switches (e.g., Intel Tofino switches) for gradient aggregation to accelerate DT tasks. Due to switches’ limited on-chip memory size, existing solutions try to design the memory sharing mechanism for INA. This mechanism requires gradients to arrive at switches synchronously, while network dynamics make it common for the asynchronous arrival of gradients, resulting in existing solutions being inefficient (e.g., massive communication overhead). To address this issue, we propose GOAT, the first-of-its-kind work on gradient scheduling with collaborative in-network aggregation, so that switches can efficiently aggregate asynchronously arriving gradients. Specifically, GOAT first partitions the model into a set of sub-models, then decides which sub-model gradients each switch is responsible for aggregating exclusively and to which switch each worker should send its sub-model gradients. To this end, we design an efficient knapsack-based randomized rounding algorithm and formally analyze the approximation performance. We implement GOAT and evaluate its performance on a testbed consisting of 3 Intel Tofino switches and 9 servers. Experimental results show that GOAT can speed up the DT by 1.5 \\times\ncompared to the state-of-the-art solutions.}
}


@article{DBLP:journals/ton/KimJHY24,
	author = {Minsong Kim and
                  Woohyuk Jang and
                  JunNyung Hur and
                  MyungKeun Yoon},
	title = {Relative Frequency-Rank Encoding for Unsupervised Network Anomaly
                  Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3453--3467},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3391396},
	doi = {10.1109/TNET.2024.3391396},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KimJHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network-based anomaly detection plays a pivotal role in cybersecurity. Most detection models are based on unsupervised machine learning to learn such a normal flow pattern of network traffic as the numbers of incoming/outgoing packets, traffic volumes in bytes, duration time, etc., most of which are numerical features. On the contrary, non-numerical features have not been fully utilized yet although they often give a decisive hint to the detection of unseen attacks; for example, rarely observed combinations of IP addresses and port numbers can reveal an uncommon attack attempt. This heuristic has already been used by human experts for decades, but not fully utilized yet by deep learning models. In this paper, we present a new encoding scheme for non-numerical features such as IP addresses and port numbers that might have been mistakenly considered as numerical features. The new encoding scheme first ranks non-numerical features in their frequency order and then evenly places each rank between 0 and 1, which transforms raw data into a form that is easy for machines to understand. The anomaly detection performance is significantly improved when this new encoding scheme is applied to the same deep learning model. For example, a simple autoencoder model with the new encoding scheme achieved the Area Under Receiver Operating Characteristic, AUROC, of 0.99 for the well-known CICIDS2017 dataset while the previous record was 0.91. Experimental results from three different open datasets show that the proposed encoding scheme can significantly enhance the performance of anomaly detection models.}
}


@article{DBLP:journals/ton/LiuWXXLHH24,
	author = {Jianchun Liu and
                  Shilong Wang and
                  Hongli Xu and
                  Yang Xu and
                  Yunming Liao and
                  Jinyang Huang and
                  He Huang},
	title = {Federated Learning With Experience-Driven Model Migration in Heterogeneous
                  Edge Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3468--3484},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3390416},
	doi = {10.1109/TNET.2024.3390416},
	timestamp = {Sun, 06 Oct 2024 21:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuWXXLHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To approach the challenges of non-IID data and limited communication resource raised by the emerging federated learning (FL) in mobile edge computing (MEC), we propose an efficient framework, called FedMigr, which integrates a deep reinforcement learning (DRL) based model migration strategy into the pioneer FL algorithm FedAvg. According to the data distribution and resource budgets, our FedMigr will intelligently guide one client to forward its local model to another client after local updating, before directly sending the local models to the server for global aggregation as in FedAvg. Intuitively, migrating a local model from one client to another is equivalent to training the model over more data from different clients, alleviating the influence of non-IID issue. To this end, we propose an experience-driven method to make proper decisions for model migrations while satisfying the resource constraints. We also prove that FedMigr can help to reduce the parameter divergences between different local models and the global model from a theoretical perspective under the non-IID setting. Extensive experiments on three popular benchmark datasets demonstrate that FedMigr can achieve an average accuracy improvement of around 13%, and reduce bandwidth consumption for global communication by 42% on average, compared with the baselines.}
}


@article{DBLP:journals/ton/GuoQDWFX24,
	author = {Zehua Guo and
                  Li Qi and
                  Songshi Dou and
                  Jiawei Weng and
                  Xiaoyang Fu and
                  Yuanqing Xia},
	title = {Maintaining Control Resiliency for Traffic Engineering in SD-WANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3485--3498},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3393841},
	doi = {10.1109/TNET.2024.3393841},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoQDWFX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN) is introduced to Wide Area Networks (WANs) to facilitate network operations and management. One main advantage of SDN is path programmability, i.e., the ability to change forwarding path of flows by controlling underlying SDN switches to accommodate traffic variations. However, the SDN controllers may experience unexpected failure and thus lose its path programmability. The typical solution is to let active controllers control offline switches, which are controlled by failed controllers, by establishing the remapping between the offline switches and active controllers. However, existing remapping solutions do not consider the impact of controller failure on network performance and cannot exhibit predictable network performance under controller failure. In this paper, we take Traffic Engineering (TE) as a typical network scenario and propose Traffic Engineering-Aware Controller-switcH remapping rEcoveRy named TEACHER. We introduce Traffic-aware Path Programmability (TPP) as a new metric to describe the impact of controller failure on TE and design TEACHER based on this metric to smartly recover offline switches. Simulation results show that TEACHER can increase the overall TPP by up to 83.0% and improves the load balancing performance by up to 58.2% under Sprintlink topology with relatively low computation time, compared with baselines.}
}


@article{DBLP:journals/ton/SongWMWKWLLYMLC24,
	author = {Zhuo Song and
                  Jiejian Wu and
                  Teng Ma and
                  Zhe Wang and
                  Linghe Kong and
                  Zhenzao Wen and
                  Jingxuan Li and
                  Yang Lu and
                  Yong Yang and
                  Tao Ma and
                  Zheng Liu and
                  Guihai Chen},
	title = {Zero+: Monitoring Large-Scale Cloud-Native Infrastructure Using One-Sided
                  {RDMA}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3499--3514},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3394514},
	doi = {10.1109/TNET.2024.3394514},
	timestamp = {Sun, 06 Oct 2024 21:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SongWMWKWLLYMLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud services have shifted from monolithic designs to microservices running on cloud-native infrastructure with monitoring systems to ensure service level agreements (SLAs). However, traditional monitoring systems no longer meet the demands of cloud-native monitoring. In Alibaba’s “double eleven” shopping festival, it is observed that the monitor occupies resources of the monitored infrastructure and even disrupts services. In this paper, we propose a novel monitoring system named Zero+ for cloud-native monitoring. Zero+ achieves zero overhead in collecting raw metrics using one-sided remote direct memory access (RDMA) and remedies network congestion by adopting a receiver-driven flow control scheme. Zero+ also features a priority queue mechanism to meet different quality of service requirements and an efficient batch processing design to relieve CPU occupation. Zero+ has been deployed and evaluated in four different clusters with heterogeneous RDMA NIC devices and architectures in Alibaba Cloud. Results show that Zero+ achieves no CPU occupation at the monitored host and supports 1\\sim 10k\nhosts with 0.1\\sim 1s\nsampling interval using a single thread for network I/O. Zero+ significantly relieves the incast issue and maintains 80\\sim 95\\%\nof bandwidth utilization in several clusters when monitoring 1k\nhosts. Zero+ also ensures services with high priority accomplish collecting metrics earlier than low priority ones by at least 400 \\mu s\nwhen monitoring 1k\nhosts.}
}


@article{DBLP:journals/ton/GuoHNZLS24,
	author = {Xiuzhen Guo and
                  Yuan He and
                  Jing Nan and
                  Jiacheng Zhang and
                  Yunhao Liu and
                  Longfei Shangguan},
	title = {A Low-Power Demodulator for LoRa Backscatter Systems With Frequency-Amplitude
                  Transformation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3515--3527},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3396509},
	doi = {10.1109/TNET.2024.3396509},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoHNZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The radio range of backscatter systems continues growing as new wireless communication primitives are continuously invented. Nevertheless, both the bit error rate and the packet loss rate of backscatter signals increase rapidly with the radio range, thereby necessitating the cooperation between the access point and the backscatter tags through a feedback loop. Unfortunately, the low-power nature of backscatter tags limits their ability to demodulate feedback signals from a remote access point and scales down to such circumstances. This paper presents Saiyan, an ultra-low-power demodulator for long-range LoRa backscatter systems. Saiyan is based on an observation that a frequency-modulated chirp signal can be transformed into an amplitude-modulated signal using a differential circuit. Moreover, we redesign a LoRa backscatter tag which integrates Saiyan and a ring oscillator-based modulator. The LoRa backscatter tag enables re-transmission, rate adaption and channel hopping – three PHY-layer operations that are important to channel efficiency yet unavailable on existing long-range backscatter systems. We prototype Saiyan and the LoRa backscatter tag on two PCB boards and evaluate their performance in different environments. Results show that Saiyan achieves 3.5– 5\\times gain on the demodulation range, compared with state-of-the-art systems. Our ASIC simulation shows that the power consumption of Saiyan and the LoRa backscatter tag are around 93.2~\\mu W and 94.7~\\mu W . Code and hardware schematics can be found at: https://github.com/ZangJac/Saiyan.}
}


@article{DBLP:journals/ton/ColuzziBAL24,
	author = {Massimo Coluzzi and
                  Amos Brocco and
                  Alessandro Antonucci and
                  Tiziano Leidi},
	title = {MementoHash: {A} Stateful, Minimal Memory, Best Performing Consistent
                  Hash Algorithm},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3528--3543},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3393476},
	doi = {10.1109/TNET.2024.3393476},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ColuzziBAL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consistent hashing is used in distributed systems and networking applications to spread data evenly and efficiently across a cluster of nodes. In this paper, we present MementoHash, a novel consistent hashing algorithm that eliminates known limitations of state-of-the-art algorithms while keeping optimal performance and minimal memory usage. We describe the algorithm in detail, provide a pseudo-code implementation, and formally establish its solid theoretical guarantees. To measure the efficacy of MementoHash, we compare its performance, in terms of memory usage and lookup time, to that of state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike JumpHash, MementoHash can handle random failures. Moreover, MementoHash does not require fixing the overall capacity of the cluster (as AnchorHash and DxHash do), allowing it to scale indefinitely. The number of removed nodes affects the performance of all the considered algorithms. Therefore, we conduct experiments considering three different scenarios: stable (no removed nodes), one-shot removals (90% of the nodes removed at once), and incremental removals. We report experimental results that averaged a varying number of nodes from ten to one million. Results indicate that our algorithm shows optimal lookup performance and minimal memory usage in its best-case scenario. It behaves better than AnchorHash and DxHash in its average-case scenario and at least as well as those two algorithms in its worst-case scenario.}
}


@article{DBLP:journals/ton/ChiangLCH24,
	author = {Yun{-}Hsin Chiang and
                  Yi{-}Jheng Lin and
                  Cheng{-}Shang Chang and
                  Y.{-}W. Peter Hong},
	title = {Throughput Analysis for Parallel Decoding of Irregular Repetition
                  Slotted {ALOHA} With Noise},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3544--3558},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3392960},
	doi = {10.1109/TNET.2024.3392960},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChiangLCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to its simplicity and scalability, the Irregular Repetition Slotted ALOHA (IRSA) system that uses the successive interference cancellation (SIC) technique is a promising solution for uncoordinated multiple access of a massive number of Internet-of-Things (IoT) devices. In this paper, we propose two parallel decoding algorithms for IRSA in an additive white Gaussian noise channel. Our first algorithm is limited to SIC-decoupling matrices that correspond to the SIC decoding process in IRSA. For this, we propose a message-passing algorithm to find the optimal SIC-decoupling matrix that can minimize the accumulated noise power when the induced user-slot bipartite graph of an IRSA system is acyclic. This includes the Contention Resolution Diversity Slotted ALOHA (CRDSA) system that sends exactly two copies for each packet as a special case. Our second algorithm extends the first one by finding the optimal decoupling matrix for CRDSA through an optimal combination of two SIC-decoupling matrices. Using a random graph analysis, we derive the throughput for the two parallel decoding algorithms of CRDSA in a threshold-based decoding model. We then conduct various numerical experiments to illustrate the tradeoffs between sequential decoding with a limited number of iterations and parallel decoding with a predefined signal-to-noise ratio (SNR) threshold. Finally, we demonstrate how to extend our parallel decoding scheme to bipartite graphs with cycles.}
}


@article{DBLP:journals/ton/ZhangF24,
	author = {Bibo Zhang and
                  Ilario Filippini},
	title = {Mobility-Aware Resource Allocation for mmWave {IAB} Networks: {A}
                  Multi-Agent Reinforcement Learning Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3559--3574},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3396214},
	doi = {10.1109/TNET.2024.3396214},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {MmWaves have been envisioned as a promising direction to provide Gbps wireless access. However, they are susceptible to high path losses and blockages, which can only be partially mitigated by directional antennas. That makes mmWave networks coverage-limited, thus requiring dense deployments. Integrated access and backhaul (IAB) architectures have emerged as a cost-effective solution for network densification. Resource allocation in mmWave IAB networks must face big challenges originated by heavy temporal dynamics, such as intermittent links caused by user mobility and blockages from moving obstacles. This makes it extremely difficult to find optimal and adaptive solutions. In this article, exploiting the distributed structure of the problem, we propose a Multi-Agent Reinforcement Learning (MARL) framework to optimize user throughput via flow routing and link scheduling in mmWave IAB networks characterized by mobile users and obstacles. The proposed approach implicitly captures the environment dynamics, coordinates the interference, and manages the buffer levels of IAB relay nodes. We design different MARL components, respectively for full-duplex and half-duplex networks. In addition, we propose an online training algorithm, which addresses the feasibility issues of practical systems, especially the communication and coordination among RL agents. Numerical results show the effectiveness of the proposed approach.}
}


@article{DBLP:journals/ton/YuZDLQCC24,
	author = {Hebin Yu and
                  Jiaqi Zheng and
                  Zhuoxuan Du and
                  Yu Liu and
                  Bing Quan and
                  Tao Chen and
                  Guihai Chen},
	title = {When Classic Meets Intelligence: {A} Hybrid Multipath Congestion Control
                  Framework},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3575--3590},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3395356},
	doi = {10.1109/TNET.2024.3395356},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuZDLQCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath TCP (MPTCP) is a burgeoning transport protocol which enables the server to transmit the traffic across multiple network interfaces in parallel. Classic MPTCPs have good friendliness and practicality such as relatively low overhead, but are hard to achieve consistent high-throughput and adaptability, especially for the ability to flexibly balance the congestion among different subpaths. In contrast, learning-based MPTCPs can essentially achieve consistent high-throughput and adaptability, but have poor friendliness and practicality. In this paper, we proposed MPLibra, a combined multipath congestion control framework that can complement the advantages of classic MPTCPs and learning-based MPTCPs together. MPLibra periodically leverages both classic MPTCPs and learning-based MPTCPs to make decisions and select the better one based on real-time network feedbacks. Extensive simulations on NS3 show that MPLibra can achieve good performance and outperform state-of-the-art MPTCPs under different network conditions. MPLibra improves the throughput by 40.5% and reduces the file download time by 29.94% compared with LIA, achieves good friendliness and balances congestion timely. What’s more, on the basis of MPLibra, we propose MPLibra+ which adds a safety module and an optimized packet scheduler and is the upgrade version of MPLibra. MPLibra+ has better ability to cope with untrained network environment and achieve better performance on heterogeneous scenarios compared with MPLibra.}
}


@article{DBLP:journals/ton/ZhaoLDWXZ24,
	author = {Cui Zhao and
                  Zhenjiang Li and
                  Han Ding and
                  Ge Wang and
                  Wei Xi and
                  Jizhong Zhao},
	title = {Enabling Multi-Frequency and Wider-Band {RFID} Sensing Using {COTS}
                  Device},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3591--3605},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3394974},
	doi = {10.1109/TNET.2024.3394974},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoLDWXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RFID shows great potentials to build useful sensing applications. However, current RFID sensing can obtain mainly a single-dimensional sensing measurement from each reader-to-tag query, such as phase, RSS, etc. This is sufficient to fulfill the designs that are bound to the tag’s movement, e.g., the localization of tags. However, it imposes inevitable uncertainty on many sensing tasks relying on the features extracted from the RFID signals. These traditional sensing measurements limit the fidelity of RFID sensing fundamentally and prevent its broader usage in more sophisticated sensing scenarios. This paper presents RF-Wise to push the limit of RFID-based sensing, motivated by an insightful observation to customize RFID signals. RF-Wise can enrich the existing single-dimensional feature measure to a channel state information (CSI)-like measure with up to 150-dimensional samples across different frequencies concurrently. More importantly, RF-Wise is a software solution atop the standard EPC Gen2 protocol without using any extra hardware. It requires only one tag for sensing and works within the ISM band. RF-Wise, so far as we know, is the first system of such a kind. Extensive experiments show that RF-Wise does not impact underlying RFID communications, while by using the features extracted by RF-Wise, applications’ sensing performance can be improved remarkably. The source codes of RF-Wise are available at https://cui-zhao.github.io/RF-WISE/.}
}


@article{DBLP:journals/ton/HanTYYSWG24,
	author = {Lei Han and
                  Chunyu Tu and
                  Zhiwen Yu and
                  Zhiyong Yu and
                  Weihua Shan and
                  Liang Wang and
                  Bin Guo},
	title = {Collaborative Route Planning of UAVs, Workers, and Cars for Crowdsensing
                  in Disaster Response},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3606--3621},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3395493},
	doi = {10.1109/TNET.2024.3395493},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HanTYYSWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficiently obtaining the up-to-date information in the disaster-stricken area is the key to successful disaster response. Unmanned aerial vehicles (UAVs), workers and cars can collaborate to accomplish sensing tasks, such as life detection task in disaster-stricken areas. In this paper, we explicitly address the route planning for a group of agents, including UAVs, workers, and cars, with the goal of maximizing the sensing task completion rate. we propose a MARL-based heterogeneous multi-agent route planning algorithm called MANF-RL-RP. The algorithm has made targeted designs in terms of global-local dual information processing and model structure for heterogeneous multi-agent, making it effectively considers the collaboration among heterogeneous agents and the long-term impact of current decisions. Finally, we conducted detailed experiments based on the rich simulation data. In comparison to the baseline algorithms, namely Greedy-SC-RP and MANF-DNN-RP, MANF-RL-RP has exhibited a significant performance improvement. Compared to MANF-DNN-RP and Greedy-SC-RP, the task completion rate based on MANF-RL-RP increased by an average of 8.82% and 56.8%, respectively.}
}


@article{DBLP:journals/ton/YuBPM24,
	author = {Mingli Yu and
                  Quinn K. Burke and
                  Thomas F. La Porta and
                  Patrick D. McDaniel},
	title = {Stealthy Misreporting Attacks Against Load Balancing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3622--3635},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3396387},
	doi = {10.1109/TNET.2024.3396387},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuBPM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Load balancing in software-defined networks (SDNs) is commonly realized with a centralized architecture. Dynamic load balancing relies on the SDN controller to periodically collect traffic statistics from network switches and make decisions in a timely manner. In this paper, we examine the extent to which an adversary that has compromised a switch can influence the load balancing algorithm by misreporting its own traffic statistics. We design an attack that allows an adversary to perform preliminary reconnaissance, which means learning network traffic distributions and setting attack parameters, and then accurately model and estimate the reward from misreporting while evading detection. Our evaluation offers three insights: 1) network traffic exhibits discernible patterns by reconnaissance; 2) the reconnaissance can be used to design misreporting attacks that can effectively draw unfair proportions of network traffic to the adversary under the guise of honest behavior; and 3) reconnaissance itself can be accelerated by misreporting to launch more targeted attacks.}
}


@article{DBLP:journals/ton/LiGLWCXXW24,
	author = {Jing Li and
                  Song Guo and
                  Weifa Liang and
                  Jie Wu and
                  Quan Chen and
                  Zichuan Xu and
                  Wenzheng Xu and
                  Jianping Wang},
	title = {AoI-Aware, Digital Twin-Empowered IoT Query Services in Mobile Edge
                  Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {4},
	pages = {3636--3650},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3395709},
	doi = {10.1109/TNET.2024.3395709},
	timestamp = {Sun, 08 Sep 2024 16:07:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiGLWCXXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Mobile Edge Computing (MEC) paradigm gives impetus to the vigorous advancement of the Internet of Things (IoT), through provisioning low-latency computing services at network edges. The emerging digital twin technique has been explosively growing in the IoT community, which bridges the gap between physical objects and their digital representations in an MEC network, enabling real-time monitoring and analysis, simulations on the dynamics of systems, accurate predictions on behaviours of objects, and optimization on network resource allocation. In this paper, we consider AoI-aware query services in an MEC network empowered by digital twin technology for diverse IoT applications. We aim to maximize the weighted sum of the accumulative freshness of query results measured by the Age of Information (AoI) and the total query service delay of admitted requests. To this end, we first formulate a novel minimization problem that explores nontrivial trade-offs between the two conflicting optimization objectives: the freshness of query results and service delays, and we show the NP-hardness of the problem. Then, we propose an approximation algorithm with a provable approximation ratio for the problem, at the expense of bounded computing capacity violations. We also develop a heuristic for the problem without any capacity violations. We finally evaluate the performance of the proposed algorithms via simulations. The simulation results demonstrate that the proposed algorithms are promising, and outperform the comparison benchmarks.}
}


@article{DBLP:journals/ton/WangSPLLY24,
	author = {Yuntao Wang and
                  Zhou Su and
                  Yanghe Pan and
                  Tom H. Luan and
                  Ruidong Li and
                  Shui Yu},
	title = {Social-Aware Clustered Federated Learning With Customized Privacy
                  Preservation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3654--3668},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3379439},
	doi = {10.1109/TNET.2024.3379439},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangSPLLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of individuals. As such, SCFL considerably enhances model utility without sacrificing privacy in a low-cost and highly feasible manner. We unfold the design of SCFL in three steps. i) Stable social cluster formation. Considering users’ heterogeneous training samples and data distributions, we formulate the optimal social cluster formation problem as a federation game and devise a fair revenue allocation mechanism to resist free-riders. ii) Differentiated trust-privacy mapping. For the clusters with low mutual trust, we design a customizable privacy preservation mechanism to adaptively sanitize participants’ model updates depending on social trust degrees. iii) Distributed convergence. A distributed two-sided matching algorithm is devised to attain an optimized disjoint partition with Nash-stable convergence. Experiments on Facebook network and MNIST/CIFAR-10 datasets validate that our SCFL can effectively enhance learning utility, improve user payoff, and enforce customizable privacy protection.}
}


@article{DBLP:journals/ton/DingCWRS24,
	author = {Kemi Ding and
                  Yijun Chen and
                  Lei Wang and
                  Xiaoqiang Ren and
                  Guodong Shi},
	title = {Network Learning in Quadratic Games From Best-Response Dynamics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3669--3684},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404509},
	doi = {10.1109/TNET.2024.3404509},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DingCWRS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the capacity of an adversary to learn the underlying interaction network through repeated best response actions in linear-quadratic games. The adversary strategically perturbs the decisions of a set of action-compromised players and observes the sequential decisions of a set of action-leaked players. The central question pertains to whether such an adversary can fully reconstruct or effectively estimate the underlying interaction structure among the players. To begin with, we establish a series of results that characterize the learnability of the interaction graph from the adversary’s perspective by drawing connections between this network learning problem in games and classical system identification theory. Subsequently, taking into account the inherent stability and sparsity constraints inherent in the network interaction structure, we propose a stable and sparse system identification framework for learning the interaction graph based on complete player action observations. Moreover, we present a stable and sparse subspace identification framework for learning the interaction graph when only partially observed player actions are available. Finally, we demonstrate the efficacy of the proposed learning frameworks through numerical examples.}
}


@article{DBLP:journals/ton/GaoMGWJKC24,
	author = {Xiaofeng Gao and
                  Akbar Majidi and
                  Yucen Gao and
                  Guanhao Wu and
                  Nazila Jahanbakhsh and
                  Linghe Kong and
                  Guihai Chen},
	title = {Nous: Drop-Freeness and Duplicate-Freeness for Consistent Updating
                  in {SDN} Multicast Routing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3685--3698},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404967},
	doi = {10.1109/TNET.2024.3404967},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GaoMGWJKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consistent routing updates through Software-Defined Networking (SDN) can be difficult due to the asynchronous and distributed nature of the data plane. Recent studies have achieved consistent unicast routing updates. However, achieving consistent updates with drop-freeness and duplicate-freeness remains a challenge for multicast with fewer known results. This paper proposes a Novel Ordered Update Scheme called Nous, a novel approach that offers a comprehensive solution for consistently updating multicast routing based on SDN. To avoid duplicate entries, Nous configures the inport match field in the forwarding rules. Nous implements a dependency graph to schedule update operations dynamically. It also solves the Replace Operation Tree Migration Problem (ROTMP) using a greedy solution. To compare the greedy solution with the optimal solution, we employ the state-of-the-art mathematical programming solver Gurobi Optimizer 7.5 (for solving the optimization problem), Mininet 2.0, and Floodlight 1.2 (for simulation and comparison) to obtain a near-optimal solution. Simulation results show that using the greedy solution, Nous can usually achieve near-optimal solutions to the ROTMP with an average of fewer than 1.2 rounds and within 10 ms in different scenarios. This makes Nous the first ordered update scheme to guarantee two consistent states simultaneously.}
}


@article{DBLP:journals/ton/WangHX24,
	author = {Qi Wang and
                  Jianhui Huang and
                  Yongjun Xu},
	title = {Scheduling of Real-Time Wireless Flows: {A} Comparative Study of Centralized
                  and Decentralized Reinforcement Learning Approaches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3699--3714},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3405950},
	doi = {10.1109/TNET.2024.3405950},
	timestamp = {Tue, 04 Feb 2025 14:47:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the problem of scheduling real-time wireless flows with general traffic patterns in dynamic network conditions. The main goal is to maximize the fraction of packets to be delivered within their deadlines, which is referred to as timely-throughput. While scheduling algorithms for frame-based traffic models and greedy maximal scheduling methods like LDF have been thoroughly studied, algorithms providing deadline guarantees on packet delivery for general traffic under dynamic network conditions are insufficient. To address this issue, we present a comparative study of two deep reinforcement learning-based scheduling algorithms: RL-Centralized and RL-Decentralized, which are designed to optimize timely-throughput for real-time wireless flows with general traffic patterns in dynamic wireless networks. The RL-Centralized scheduling algorithm formulates the centralized scheduling problem as a Markov Decision Process (MDP) and leverages a Multi-Environments Dueling Double Deep Q-Network (ME-D3QN) structure to adapt to dynamic network conditions. The RL-Decentralized scheduling problem is formulated as a Multi-Agent Markov Decision Process (MMDP) and employs the Node State Consensus Protocol (NSCP) and Lifelong Reinforcement Learning Decentralized Training and Decentralized Execution (LRL-DTDE) structure to accelerate training. Our experimental results indicate that both proposed algorithms can converge quickly and efficiently adapt to dynamic network conditions with better performance than their baseline policies. Finally, test-bed experiments validate simulation results and confirm that the proposed algorithms are practical on resource-limited platforms.}
}


@article{DBLP:journals/ton/LuoBW24,
	author = {Ziyue Luo and
                  Yixin Bao and
                  Chuan Wu},
	title = {Optimizing Task Placement and Online Scheduling for Distributed {GNN}
                  Training Acceleration in Heterogeneous Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3715--3729},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3415089},
	doi = {10.1109/TNET.2024.3415089},
	timestamp = {Sun, 19 Jan 2025 13:55:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoBW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training Graph Neural Networks (GNNs) on large graphs is resource-intensive and time-consuming, mainly due to the large graph data that cannot be fit into the memory of a single machine, but have to be fetched from distributed graph storage and processed on the go. Unlike distributed deep neural network (DNN) training, the bottleneck in distributed GNN training lies largely in large graph data transmission for constructing mini-batches of training samples. Existing solutions often advocate data-computation colocation, and do not work well with limited resources and heterogeneous training devices in heterogeneous clusters. The potentials of strategical task placement and optimal scheduling of data transmission and task execution have not been well explored. This paper designs an efficient algorithm framework for task placement and execution scheduling of distributed GNN training in heterogeneous systems, to better resource utilization, improve execution pipelining, and expedite training completion. Our framework consists of two modules: (i) an online scheduling algorithm that schedules the execution of training tasks, and the data transmission plan; and (ii) an exploratory task placement scheme that decides the placement of each training task. We conduct thorough theoretical analysis, testbed experiments and simulation studies, and observe up to 48% training speed-up with our algorithm as compared to representative baselines in our testbed settings.}
}


@article{DBLP:journals/ton/CongXWZYZXL24,
	author = {Yinchuan Cong and
                  Kun Xie and
                  Jigang Wen and
                  Jiwei Zhang and
                  Yansong Yin and
                  Qingyi Zhang and
                  Gaogang Xie and
                  Wei Liang},
	title = {Per-Packet Traffic Measurement in Storage, Computation and Bandwidth
                  Limited Data Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3730--3742},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404011},
	doi = {10.1109/TNET.2024.3404011},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CongXWZYZXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet level measurement in the data plane provides a microscopic view of the network’s state. Although advances in programmable switches and routers make it possible to measure the Sequence of Packet Lengths and Arrival Times (SPLT) in the data plane, collecting this information remains challenging due to limited storage, processing resources, and bandwidth. To address this issue, we propose MES, which Measures and Encodes Simultaneously the packet length and timestamp when each packet passes through the Network Processor (NP) in the switch/router. We design the packet length compression and timestamp compression algorithms to be lightweight and implement the designed algorithms using simple operations supported by the network processor, while taking into account the computation constraints of the NP. Through extensive experiments on five packet traces, we demonstrate that our MES achieves high precision SPLT measurements (up to 99.82% cosine similarity) while reducing storage and bandwidth overhead by up to 87%. Simulations conducted on the BMV2 P4 software switch demonstrate that our designed SPLT measurement mechanism imposes little impact on network throughput and delay.}
}


@article{DBLP:journals/ton/YuanZQHYCXYHZ24,
	author = {Danfu Yuan and
                  Weizhan Zhang and
                  Yubing Qiu and
                  Haiyu Huang and
                  Mingliang Yang and
                  Peng Chen and
                  Kai Xiao and
                  Hongfei Yan and
                  Yaming He and
                  Yiping Zhang},
	title = {Context-Aware Cross-Layer Congestion Control for Large-Scale Live
                  Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3743--3759},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3397671},
	doi = {10.1109/TNET.2024.3397671},
	timestamp = {Thu, 13 Feb 2025 14:31:48 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuanZQHYCXYHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Live video streaming has come to dominate today’s Internet traffic. Content Delivery Network (CDN) providers, responsible for hosting outsourced live streaming services, are now striving to ensure an enhanced quality of experience (QoE) to meet the ever-increasing user expectations. Existing congestion control (CC) schemes in the kernel, however, suffer from unsatisfactory performance for live video delivery due to disparities in traffic characteristics and differentiated optimization goals between generic traffic and live video traffic. In this paper, we propose XCC, a streaming context-aware CC approach that helps achieve better QoE for the live streaming services from CDN provider. The core of XCC is to adaptively coordinate the transmission strategy and frame rate through a cross-layer feedback framework, responding to the fluctuating traffic dynamics and network conditions in the short term. Further, XCC matches the long-term traffic characteristics (i.e., two-stage delivery mode) by employing a task-specific state transition mechanism as the underlying TCP. XCC has been implemented in the Linux kernel’s TCP stack and media engine and has been fully deployed in Alibaba Cloud’s production service. Evaluation in experimental environments and A/B testing serving tens of millions of sessions demonstrate that XCC is competitive in streaming delay against the most prevalent TCP in today’s Operating Systems, while reducing startup delay by 9.9%, stall time by 36.4%, and stall frequency by 42.5% on average in deployment.}
}


@article{DBLP:journals/ton/LiLLQYW24,
	author = {Chengxin Li and
                  Zhetao Li and
                  Saiqin Long and
                  Pengpeng Qiao and
                  Ye Yuan and
                  Guoren Wang},
	title = {Robust Data Inference and Cost-Effective Cell Selection for Sparse
                  Mobile Crowdsensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3760--3775},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3397309},
	doi = {10.1109/TNET.2024.3397309},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLLQYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse Mobile CrowdSensing (MCS) aims to reduce sensing cost while ensuring high task quality by intelligently selecting small regions for sensing and accurately inferring the remaining areas. Data inference and cell selection are crucial components in Sparse MCS. However, cell division, which is a prerequisite for cell selection, has received insufficient attention. The existing uniform division method disregards the correlation of the sensing area. In addition, the impact of sparse noise on both data inference and cell selection has been ignored, potentially undermining the effectiveness of Sparse MCS. To address these issues, we propose a novel scheme termed Robust data Inference and Cost-Effective cell Selection for Sparse MCS (Rices). Specifically, we first design an adaptive region division strategy that captures the correlation of sensing regions. Subsequently, we tackle the robust data inference problem in the presence of sparse noise by formulating it as a dual-objective optimization. Furthermore, we optimize the cell selection strategy to dynamically adjust the set of sampled cells under the constraints of data inference quality. Extensive experiments on large-scale real-world datesets are conducted to evaluate the proposed scheme. The results demonstrate that Rices can accurately recover missing data with 20% sparse noise and significantly reduce sensing costs compared to baseline models.}
}


@article{DBLP:journals/ton/WangDCLGCZCLDC24,
	author = {Hancheng Wang and
                  Haipeng Dai and
                  Shusen Chen and
                  Meng Li and
                  Rong Gu and
                  Huayi Chai and
                  Jiaqi Zheng and
                  Zhiyuan Chen and
                  Shuaituan Li and
                  Xianjun Deng and
                  Guihai Chen},
	title = {Bamboo Filters: Make Resizing Smooth and Adaptive},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3776--3791},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3403997},
	doi = {10.1109/TNET.2024.3403997},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangDCLGCZCLDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The approximate membership query (AMQ) data structure is a kind of space-efficient probabilistic data structure. It can approximately indicate whether an element exists in a set. The AMQ data structure has been widely used in network measurements, network security, network caching, etc. Resizing is an extensively utilized operation of the AMQ data structure, but it can lead to system performance degradation. We summarize two main problems that lead to such degradation. Specifically, one of them is that the resizing operation can block other operations, while the other one is that the throughput of AMQ structures will deteriorate after multiple resizing operations due to more computation cost. However, existing related work cannot alleviate both of them. Therefore, we propose a novel AMQ data structure called bamboo filters, which can alleviate the two problems simultaneously. Bamboo filters can insert, look up, and delete an element in constant time. They can also dynamically resize in a fine-grained way. Furthermore, we propose space utilization adaptive bamboo filters that adaptively trigger resizing operations according to the space utilization, thereby achieving lower average memory consumption. Experimental results show that our scheme significantly outperforms state-of-the-art work. Especially, bamboo filters achieve 2.12\\times\nlookup throughput of the logarithmic dynamic cuckoo filter.}
}


@article{DBLP:journals/ton/LiuGHC24,
	author = {Boya Liu and
                  Chaojie Gu and
                  Shibo He and
                  Jiming Chen},
	title = {LoPhy: {A} Resilient and Fast Covert Channel Over LoRa {PHY}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3792--3807},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3398814},
	doi = {10.1109/TNET.2024.3398814},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuGHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Covert channel, which can break the logical protections of the computer system and leak confidential or sensitive information, has long been considered a security issue in the network research community. However, recent research has shown that cooperative agents can use the “covert” channel to augment the communication of legitimate applications, rather than by adversaries seeking to compromise computer security. This further broadens the potential applications of covert channels. Despite this, the design and implementation of covert channels in the context of Low Power Wide Area Networks (LPWANs) have not been widely discussed. Current state-of-the-art uses On-off keying (OOK) on LoRa PHY to create a covert channel, but this channel has limited transmission distance and capacity. In this paper, we propose LoPhy, a resilient and fast covert channel over LoRa physical layer (PHY). LoPhy uses the Chirp Spreading Spectrum (CSS) modulation scheme to increase its resilience and explore the trade-off between the covert channel’s capacity and the legitimate channel’s resilience. We implement the proposed covert channel on off-the-shelf devices and software-defined radios and show that LoPhy achieves a 0.57% bit error rate at a distance of 700\\,\\text {m} with slight impact on legitimate channel’s performance. Moreover, we present two applications enabled by LoPhy to demonstrate the potential of LoPhy. Compared with the state-of-the-art, LoPhy brings up to 18\\times reduction of bit errors and 63\\times gain on noise resilience.}
}


@article{DBLP:journals/ton/WangZYMHFZ24,
	author = {Guijuan Wang and
                  Yazhi Zhang and
                  Jiguo Yu and
                  Meijie Ma and
                  Chunqiang Hu and
                  Jianxi Fan and
                  Li Zhang},
	title = {HS-DCell: {A} Highly Scalable DCell-Based Server-Centric Topology
                  for Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3808--3823},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3398628},
	doi = {10.1109/TNET.2024.3398628},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangZYMHFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topology design is vital to the high performance data center networks. Due to the limited scalability, many traditional server-centric data center networks are confronting the updating and upgrading hurdles. To address the issue, this paper proposes a highly scalable DCell-based server-centric data center network topology, called HS-DCell, which can use inexpensive and typical switches and servers with only three network ports to achieve excellent network performance. HS-DCell can accommodate a large number of servers, and its diameter increases linearly with the growth of network levels, which is better than that of most existing server-centric networks. Furthermore, a fault-free routing algorithm and a fault-tolerant routing algorithm are developed based on HS-DCell. Compared with other mainstream server-centric network topologies, the experimental results show that HS-DCell has obvious advantages in many key performance indicators including scalability, fault tolerance, and server port utilization.}
}


@article{DBLP:journals/ton/ShisherSH24,
	author = {Md Kamran Chowdhury Shisher and
                  Yin Sun and
                  I{-}Hong Hou},
	title = {Timely Communications for Remote Inference},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3824--3839},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3408673},
	doi = {10.1109/TNET.2024.3408673},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShisherSH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we analyze the impact of data freshness on remote inference systems, where a pre-trained neural network infers a time-varying target (e.g., the locations of vehicles and pedestrians) based on features (e.g., video frames) observed at a sensing node (e.g., a camera). One might expect that the performance of a remote inference system degrades monotonically as the feature becomes stale. Using an information-theoretic analysis, we show that this is true if the feature and target data sequence can be closely approximated as a Markov chain, whereas it is not true if the data sequence is far from being Markovian. Hence, the inference error is a function of Age of Information (AoI), where the function could be non-monotonic. To minimize the inference error in real-time, we propose a new “selection-from-buffer” model for sending the features, which is more general than the “generate-at-will” model used in earlier studies. In addition, we design low-complexity scheduling policies to improve inference performance. For single-source, single-channel systems, we provide an optimal scheduling policy. In multi-source, multi-channel systems, the scheduling problem becomes a multi-action restless multi-armed bandit problem. For this setting, we design a new scheduling policy by integrating Whittle index-based source selection and duality-based feature selection-from-buffer algorithms. This new scheduling policy is proven to be asymptotically optimal. These scheduling results hold for minimizing general AoI functions (monotonic or non-monotonic). Data-driven evaluations demonstrate the significant advantages of our proposed scheduling policies.}
}


@article{DBLP:journals/ton/LiWW24,
	author = {Shuyang Li and
                  Qiang Wu and
                  Ran Wang},
	title = {Dynamic Discrete Topology Design and Routing for Satellite-Terrestrial
                  Integrated Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3840--3853},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3397613},
	doi = {10.1109/TNET.2024.3397613},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite-terrestrial integrated networks (STNs) are considered a promising architecture for 6G networks due to their ability to provide ubiquitous, high-capacity coverage on a global scale by combining satellite and terrestrial network infrastructures. However, the complex network architecture, time-varying topology, and frequent inter-satellite connection handovers present significant challenges for developing efficient routing and service continuity in STNs. To overcome these challenges, we propose Dyna-STN, a dynamic discrete topology-oriented wide-area routing mechanism in this paper. Dyna-STN utilizes a dynamic discrete topology model to characterize the time-varying topology of satellite networks. Furthermore, a hierarchical framework is established within the management plane to implement Dyna-STN, which comprises a dynamic discrete topology management plane and a routing management plane. A virtual overlay network composed of fixed virtual nodes shields the dynamics of satellite networks, while the open shortest path first protocol (OSPF) is deployed within the virtual overlay network to exchange routing reachability information among virtual nodes. Additionally, Dyna-STN performs dynamic binding and service migration between different satellite entities at specific time slots, thereby maintaining the continuity of virtual node services. Extensive numerical results demonstrate that Dyna-STN outperforms several baseline schemes in terms of routing protocol performance, packet forwarding performance, and service continuity. Furthermore, Dyna-STN maintains stable performance as the network scale increases and supports reliable data transmission among terminal devices in STNs.}
}


@article{DBLP:journals/ton/LiLLZZHTXJ24,
	author = {Ruoyu Li and
                  Qing Li and
                  Tao Lin and
                  Qingsong Zou and
                  Dan Zhao and
                  Yucheng Huang and
                  Gareth Tyson and
                  Guorui Xie and
                  Yong Jiang},
	title = {DeviceRadar: Online IoT Device Fingerprinting in ISPs Using Programmable
                  Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3854--3869},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3398778},
	doi = {10.1109/TNET.2024.3398778},
	timestamp = {Sun, 19 Jan 2025 13:55:29 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLLZZHTXJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device fingerprinting can be used by Internet Service Providers (ISPs) to identify vulnerable IoT devices for early prevention of threats. However, due to the wide deployment of middleboxes in ISP networks, some important data, e.g., 5-tuples and flow statistics, are often obscured, rendering many existing approaches invalid. It is further challenged by the high-speed traffic of hundreds of terabytes per day in ISP networks. This paper proposes DeviceRadar, an online IoT device fingerprinting framework that achieves accurate, real-time processing in ISPs using programmable switches. We innovatively exploit “key packets” as a basis of fingerprints only using packet sizes and directions, which appear periodically while exhibiting differences across different IoT devices. To utilize them, we propose a packet size embedding model to discover the spatial relationships between packets. Meanwhile, we design an algorithm to extract the “key packets” of each device, and propose an approach that jointly considers the spatial relationships and the key packets to produce a neighboring key packet distribution, which can serve as a feature vector for machine learning models for inference. Last, we design a model transformation method and a feature extraction process to deploy the model on a programmable data plane within its constrained arithmetic operations and memory to achieve line-speed processing. Our experiments show that DeviceRadar can achieve state-of-the-art accuracy across 77 IoT devices with 40 Gbps throughput, and requires only 1.3% of the processing time compared to GPU-accelerated approaches.}
}


@article{DBLP:journals/ton/SongHZLZFLWY24,
	author = {Guanglei Song and
                  Lin He and
                  Feiyu Zhu and
                  Jinlei Lin and
                  Wenjian Zhang and
                  Linna Fan and
                  Chenglong Li and
                  Zhiliang Wang and
                  Jiahai Yang},
	title = {AddrMiner: {A} Fast, Efficient, and Comprehensive Global Active IPv6
                  Address Detection System},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3870--3887},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3406508},
	doi = {10.1109/TNET.2024.3406508},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SongHZLZFLWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast Internet-wide scanning is essential for network situational awareness and asset evaluation. However, the vast IPv6 address space makes brute-force scanning infeasible. Despite advancements in state-of-the-art methods, they do not work in seedless regions and suffer low detection efficiency and speed in regions with known active IPv6 addresses (i.e., seed addresses). Moreover, the collected active address list (i.e., IPv6 hitlist) with low coverage cannot truly represent the active IPv6 address landscape of the Internet. This paper introduces AddrMiner, a fast, efficient, and comprehensive global active IPv6 address detection system. We design a systematic active IPv6 address detection strategy that divides the IPv6 space into two detection scenarios based on the presence or absence of seed addresses to discover active IPv6 addresses from scratch and from few to many. In the seedless regions, we present AddrMiner-N, leveraging a multi-level association policy to probe active addresses. It fills the gap of address detection in seedless regions and successfully discovers active addresses in 39,899 BGP prefixes without seed addresses, with a 1.03\\times higher hit rate, 30\\sim 911\\times higher speed, and 2.7\\times broader coverage, compared to existing solutions. In the regions with seed addresses, our method AddrMiner-S dynamically generates target addresses using reinforcement learning. Compared to state-of-the-art methods, AddrMiner-S achieves an impressive 56.3% hit rate and a discovery speed of 839.0/s, which is 1.9\\sim 2153\\times and 1.5\\sim 755\\times of existing works, respectively. Finally, we deploy AddrMiner and discover 2.1B active IPv6 addresses, including 1.7B de-aliased active addresses and 0.4B aliased addresses, through continuous probing for three years.}
}


@article{DBLP:journals/ton/YuanGF24,
	author = {Longzhi Yuan and
                  Wei Gong and
                  Yuguang Fang},
	title = {Native WiFi Backscatter},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3888--3900},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3409081},
	doi = {10.1109/TNET.2024.3409081},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuanGF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi backscatter has attracted intensive attention because the large population of WiFi radios can provide plenty of excitation signals. However, WiFi backscatter communication has imposed unwanted constraints on either exciters or receivers since its inception. In this paper, we present Chameleon, a native WiFi backscatter system, where WiFi tags can generate native WiFi packets using uncontrolled productive WiFi signals as carriers. Our tag-only design requires no particular excitation patterns and no changes in software/hardware on WiFi network interface cards (NICs). The key idea is for the Chameleon tag to demodulate the productive WiFi signal and backscatter it into a full-function packet using on-the-fly modulation. To align tag decoding and modulation with excitation symbols, we design a time synchronization and clock compensation scheme suitable for low-power tags. We prototype WiFi tags using ultra-low-power FPGAs and evaluate them in real-world scenarios where excitations are ambient traffic and backscatter receivers are a wide range of commercial off-the-shelf (COTS) NICs. Comprehensive field studies show that the maximal backscatter throughput of Chameleon is almost 1 Mbps, which is over 125\\times and 1000\\times higher than what WiTAG and FS-Backscatter tags could achieve, respectively. We also show that Chameleon can natively communicate with various COTS WiFi devices on Windows, iOS, and Android platforms. We believe that this design will enable ubiquitous WiFi connectivity for billions of IoT devices via widely available mobile gadgets and existing wireless infrastructure.}
}


@article{DBLP:journals/ton/ChenTYZZ24,
	author = {Wei Chen and
                  Ye Tian and
                  Xin Yu and
                  Bowen Zheng and
                  Xinming Zhang},
	title = {Enhancing Fairness for Approximate Weighted Fair Queueing With a Single
                  Queue},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3901--3915},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3399212},
	doi = {10.1109/TNET.2024.3399212},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenTYZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Weighted fair queueing (WFQ) is an essential strategy for enforcing bandwidth guarantee and isolation in high-speed networks. Unfortunately, implementing the original WFQ packet scheduling algorithm on today’s commodity switch hardware is challenging due to the prohibitive complexity. Approximate WFQ packet schedulers, which work with the cheap and widely available First-In First-Out (FIFO) queues, have been proposed as an alternative in recent years. In this paper, we show that both the ideal and the approximate WFQ packet schedulers are unable to allocate bandwidths to TCP flows fairly, because of the bursty nature of the TCP traffic. Furthermore, we find that the representative approximate WFQ schedulers further degrade the scheduling fairness, due to their excessive packet drops. To address these issues, we present novel approximate WFQ packet scheduling algorithms in this paper. Our initial design, namely SQ-WFQ, imposes the minimum hardware requirement by using one single FIFO queue, and effectively reduces the excessive packet drops. Extended from SQ-WFQ, we propose the SQ-EWFQ packet scheduling algorithm. SQ-EWFQ inherits all the merits of SQ-WFQ, and is adaptive to the bursty TCP traffic by tolerating short-term packet bursts, while enforcing a long-term fairness among the TCP flows. We have implemented our proposed schedulers on commodity hardware programmable switches, and achieve line rate packet scheduling with them. Experiment results from a real-world testbed and large-scale simulations show that SQ-WFQ and SQ-EWFQ outperform the state-of-the-art approximate schedulers regarding the scheduling fairness, and SQ-EWFQ allocates bandwidths to TCP flows more fairly than SQ-WFQ and other existing solutions.}
}


@article{DBLP:journals/ton/LiZHC24,
	author = {Wenxue Li and
                  Chaoliang Zeng and
                  Jinbin Hu and
                  Kai Chen},
	title = {FlowSail: Fine-Grained and Practical Flow Control for Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3916--3928},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3406613},
	doi = {10.1109/TNET.2024.3406613},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As datacenter networks continue to support a wider range of applications and faster link speeds, they face the challenge of managing bursty traffic and transient congestion. End-to-end congestion controls (CCs) find it increasingly difficult to maintain effectiveness due to the inherent feedback delay. To address this issue, per-hop flow control (FC) has gained popularity due to its ability to react promptly to transient congestion. However, existing FC mechanisms either lack fine-grained (i.e., per-flow granularity) control or require an impractical number of queues that exceeds the capabilities of commodity switches. In this paper, we introduce FlowSail, an innovative FC scheme that enables fine-grained control at the per-flow level while requiring a practical number of switch queues, theoretically as few as two. The core of FlowSail is an effective approximation of ideal FC by three key design components: dynamic flow-to-queue mapping, hierarchical congested flow identification, and on-demand isolation. We have implemented a prototype of FlowSail using the programmable P4 switch and conducted extensive testbed experiments and simulations. The results indicate that FlowSail effectively sustains performance with significantly fewer queues compared to existing FC schemes. For instance, FlowSail achieves 4.3\\times lower tail latency under the same number of queues, matches existing FC schemes with 4\\times fewer queues, and holds robust performance with a minimum of 2 queues.}
}


@article{DBLP:journals/ton/HekmatiZSJGK24,
	author = {Arvin Hekmati and
                  Jiahe Zhang and
                  Tamoghna Sarkar and
                  Nishant Jethwa and
                  Eugenio Grippo and
                  Bhaskar Krishnamachari},
	title = {Correlation-Aware Neural Networks for DDoS Attack Detection in IoT
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3929--3944},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3408675},
	doi = {10.1109/TNET.2024.3408675},
	timestamp = {Sun, 19 Jan 2025 13:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HekmatiZSJGK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a comprehensive study on applying machine learning to detect distributed Denial of service (DDoS) attacks using large-scale Internet of Things (IoT) systems. While prior works and existing DDoS attacks have largely focused on individual nodes transmitting packets at a high volume, we investigate more sophisticated futuristic attacks that use large numbers of IoT devices and camouflage their attack by having each node transmit at a volume typical of benign traffic. We introduce new correlation-aware architectures that take into account the correlation of traffic across IoT nodes. We extensively analyze the proposed architectures by evaluating five different neural network models trained on a dataset derived from a 4060-node real-world IoT system. We observe that long short-term memory (LSTM) and a transformer-based model, in conjunction with the architectures that use correlation information of the IoT nodes, provide higher performance (in terms of F1 score and binary accuracy) than the other models and architectures, especially when the attacker camouflages itself by following benign traffic distribution on each transmitting node. For instance, by using the LSTM model, the distributed correlation-aware architecture gives 81% F1 score for the attacker that camouflages their attack with benign traffic as compared to 35% for the architecture that does not use correlation information. We validate the effectiveness of our proposed detection mechanism by implementing it on a real testbed. We also investigate the performance of heuristics for selecting a subset of nodes to share their data for correlation-aware architectures to meet resource constraints.}
}


@article{DBLP:journals/ton/ZhaoLXYZZCLHM24,
	author = {Ziming Zhao and
                  Zhaoxuan Li and
                  Xiaofei Xie and
                  Jiongchi Yu and
                  Fan Zhang and
                  Rui Zhang and
                  Binbin Chen and
                  Xiangyang Luo and
                  Ming Hu and
                  Wenrui Ma},
	title = {{FOSS:} Towards Fine-Grained Unknown Class Detection Against the Open-Set
                  Attack Spectrum With Variable Legitimate Traffic},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3945--3960},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3413789},
	doi = {10.1109/TNET.2024.3413789},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoLXYZZCLHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly-based network intrusion detection systems (NIDSs) are essential for ensuring cybersecurity. However, the security communities realize some limitations when they put most existing proposals into practice. The challenges are mainly concerned with (i) fine-grained unknown attack detection and (ii) ever-changing legitimate traffic adaptation. To tackle these problem, we present three key design norms. The core idea is to construct a model to split the data distribution hyperplane and leverage the concept of isolation, as well as advance the incremental model update. We utilize the isolation tree as the backbone to design our model, named FOSS, to echo back three norms. By analyzing the popular dataset of network intrusion traces, we show that FOSS significantly outperforms the state-of-the-art methods. Further, we perform an initial deployment of FOSS by working with the Internet Service Provider (ISP) to detect distributed denial of service (DDoS) attacks. With real-world tests and manual analysis, we demonstrate the effectiveness of FOSS to identify previously-unseen attacks in a fine-grained manner.}
}


@article{DBLP:journals/ton/WangZLJRYZL24,
	author = {Ziliang Wang and
                  Shiyi Zhu and
                  Jianguo Li and
                  Wei Jiang and
                  K. K. Ramakrishnan and
                  Meng Yan and
                  Xiaohong Zhang and
                  Alex X. Liu},
	title = {DeepScaling: Autoscaling Microservices With Stable {CPU} Utilization
                  for Large Scale Production Cloud Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3961--3976},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3400953},
	doi = {10.1109/TNET.2024.3400953},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangZLJRYZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.}
}


@article{DBLP:journals/ton/LinDH24,
	author = {Qinqi Lin and
                  Lingjie Duan and
                  Jianwei Huang},
	title = {Personalized Pricing Through Strategic User Profiling in Social Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3977--3992},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3410976},
	doi = {10.1109/TNET.2024.3410976},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional user profiling techniques rely on browsing history or purchase records to identify users’ willingness to pay. This enables sellers to offer personalized prices to profiled users while charging only a uniform price to non-profiled users. However, the emergence of privacy-enhancing technologies has caused users to actively avoid on-site data tracking. Today, major online sellers have turned to public platforms such as online social networks to better track users’ profiles from their product-related discussions. This paper presents the first analytical study on how users should best manage their social activities against potential personalized pricing, and how a seller should strategically adjust her pricing scheme to facilitate user profiling in social networks. We formulate a dynamic Bayesian game played between the seller and users under asymmetric information. The key challenge of analyzing this game comes from the double couplings between the seller and the users as well as among the users. Furthermore, the equilibrium analysis needs to ensure consistency between users’ revealed information and the seller’s belief under random user profiling. We address these challenges by alternately applying backward and forward induction, and successfully characterize the unique perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as the accuracy of profiling technology improves, the seller tends to raise the equilibrium uniform price to motivate users’ increased social activities and facilitate user profiling. However, this results in most users being worse off after the informed consent policy is imposed to ensure users’ awareness of data access and profiling practices by potential sellers. This finding suggests that recent regulatory evolution towards enhancing users’ privacy awareness may have unintended consequences of reducing users’ payoffs. Finally, we examine prevalent pricing practices where the seller breaks a pricing promise to personalize final offerings, and show that it only slightly improves the seller’s average revenue while introducing higher variance.}
}


@article{DBLP:journals/ton/LiuZXYWQ24,
	author = {Jiawei Liu and
                  Gongming Zhao and
                  Hongli Xu and
                  Peng Yang and
                  Baoqing Wang and
                  Chunming Qiao},
	title = {Toward a Service Availability-Guaranteed Cloud Through {VM} Placement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {3993--4008},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3401758},
	doi = {10.1109/TNET.2024.3401758},
	timestamp = {Tue, 04 Mar 2025 20:33:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuZXYWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a multi-tenant cloud, the cloud service provider (CSP) leases physical resources to tenants in the form of virtual machines (VMs) with an agreed service level agreement (SLA). As the most important indicator of SLA, we should guarantee the service availability of tenants when placing the VMs. However, previous works about VM placement mainly concentrate on optimizing the cloud resource utilization, but only a few works consider the service availability by measuring the hardware availability. In fact, abnormal tenants can make the corresponding service unavailable by launching network attacks. That is, both the hardware availability and the tenant uncertainty will affect the service availability of VMs on physical machines (PMs). Without considering this factor, the CSP may fail to meet the tenant’s SLA requirements, leading to a reduction in revenue. To solve such a problem, this paper considers the service availability in terms of both the hardware availability and the tenant uncertainty, and studies the service availability-guaranteed VM placement in multi-tenant clouds (SAG-VMP) problem. This problem is very challenging since the service availability actually changes with the tenants served on the PM. To address this issue, we propose a two-phase approach: PM assignment and VM placement. The first phase determines the availability of each PM through a long-term tenant-PM mapping algorithm and the second phase places each VM on a PM that meets the service availability requirement based on a primal-dual online algorithm. Two algorithms with bounded approximation factors are proposed for these two phases, respectively. Both small-scale experiment results and large-scale simulation results show the superior performance of our proposed algorithms compared with other alternatives.}
}


@article{DBLP:journals/ton/LuCYBLHZR24,
	author = {Li Lu and
                  Meng Chen and
                  Jiadi Yu and
                  Zhongjie Ba and
                  Feng Lin and
                  Jinsong Han and
                  Yanmin Zhu and
                  Kui Ren},
	title = {An Imperceptible Eavesdropping Attack on WiFi Sensing Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4009--4024},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3403839},
	doi = {10.1109/TNET.2024.3403839},
	timestamp = {Wed, 06 Nov 2024 22:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuCYBLHZR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed enormous research efforts on WiFi sensing to enable intelligent services of Internet of Things. However, due to the omni-directional broadcasting manner of WiFi signals, the activity semantic underlying the signals can be leaked to adversaries for surveillance, as demonstrated by our previous work. In this paper, we further extend the attack capability of ActListener to impersonation attack, which could eavesdrop on users’ behavioral uniqueness imperceptibly using a WiFi infrastructure in any location of user sensing area. In particular, ActListener detects each human activity and converts the eavesdropped signals to that by legitimate devices based on our proposed signal propagation models. To extract noise-resilient individual behavioral uniqueness from converted CSI of WiFi signals, we further add user identification models into the substitute model set for training the signal pattern calibration generative model. Experimental results demonstrate that ActListener could achieve over 80% accuracy in activity semantics retrieval and impersonation by using the converted signals.}
}


@article{DBLP:journals/ton/ZhengCE24,
	author = {Yilin Zheng and
                  Semih Cayci and
                  Atilla Eryilmaz},
	title = {Fast Online Learning of Vulnerabilities for Networks With Propagating
                  Failures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4025--4039},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3405798},
	doi = {10.1109/TNET.2024.3405798},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhengCE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world networks, we regularly face the effect of propagating failures over networks, for example, rumors spread over social networks, outages spread over power networks, viruses spread over communication and biological networks. Often, these failures spread over a network of agents with unknown and potentially diverse degrees of vulnerabilities to the propagating phenomenon. In this work, we consider a general network model subject to propagating failures and develop provably fast mechanisms for learning the unknown vulnerabilities of the network with minimal cost incurred in the process. We propose an extension to the classic Independent Cascade (IC) model where we incorporate both node and edge failures with non-uniform costs. From an online learning perspective, the goal is to find an optimal policy to control where to start failures and generate samples. Therefore, we formulate a cost minimization problem with Probably-Approximately-Correct (PAC) type guarantees. As a theoretical benchmark, we design a linear programming problem using a proposed joint Bernstein inequality. Then we characterize the performance of randomized policies that use a fixed budget distribution independent of sampling history. Finally, we propose a fast Lyapunov-based online learning policy, for which we give a formal theoretical analysis. The performance of the policy are validated under extensive numerical studies for both synthetic and real-world networks.}
}


@article{DBLP:journals/ton/ZhuCYXZLW24,
	author = {Yuxi Zhu and
                  Hao Chen and
                  Yuan Yang and
                  Mingwei Xu and
                  Yuxuan Zhang and
                  Chenyi Liu and
                  Jianping Wu},
	title = {Fast Software IPv6 Lookup With Neurotrie},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4040--4055},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404599},
	doi = {10.1109/TNET.2024.3404599},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuCYXZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IPv6 has shown notable growth in recent years, imposing the need for high-speed IPv6 lookup. As the forwarding rate of virtual switches continues increasing, software-based IPv6 lookup without using special hardware such as TCAM, GPU, and FPGA is of academic interest and industrial importance. Existing studies achieve fast software IPv4 lookup by reducing the operation number, as well as reducing the memory footprint to benefit from CPU cache. However, in the situation of 128-bit IPv6 addresses, it is challenging to keep both operation numbers and memory footprints small. To address the issue, we propose the Neurotrie data structure, which supports fast lookup and arbitrary strides. Thus, a good balance can be made between trie depth and memory footprint by computing the proper stride for each Neurotrie node. We model the optimal Neurotrie problem which minimizes the depth with limited memory footprint and develop a pseudo-polynomial time baseline algorithm to construct Neurotrie using dynamic programming. To improve the performance and reduce the computation complexity, we develop a deep reinforcement learning-based approach, which leverages a deep neural network to construct Neurotrie efficiently, based on characteristics captured from real IPv6 prefixes. We further refine the data structure called Neurotrie-S and develop an efficient mechanism for routing updates. Experiments on real routing tables show that Neurotrie-S achieves a lookup rate 34% higher than that of state-of-the-art approaches. We implement a Neurotrie-based software switch, and the forwarding rate of Neurotrie-S is about 10% to 345% higher than other algorithms.}
}


@article{DBLP:journals/ton/LiuCYJ24,
	author = {Kanghuai Liu and
                  Lin Chen and
                  Jihong Yu and
                  Ziyue Jia},
	title = {Revisiting {RFID} Missing Tag Identification: Theoretical Foundation
                  and Algorithm Design},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4056--4066},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404471},
	doi = {10.1109/TNET.2024.3404471},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuCYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We revisit the problem of missing tag identification in RFID networks by making three contributions. Firstly, we quantitatively compare and gauge the existing propositions spanning over a decade on missing tag identification. We show that the expected execution time of the best solution in the literature is \\Theta \\left ({{N+\\frac {(1-\\alpha )^{2}(1-\\delta )^{2}}{ \\epsilon ^{2}}}}\\right ) , where \\delta and \\epsilon are parameters quantifying the required identification accuracy, N denotes the number of tags in the system, among which \\alpha N tags are missing. Secondly, we analytically establish the expected execution time lower-bound for any missing tag identification algorithm as \\Theta \\left ({{\\frac {N}{\\log N}+\\frac {(1-\\delta )^{2}(1-\\alpha )^{2}}{\\epsilon ^{2} \\log \\frac {(1-\\delta )(1-\\alpha )}{\\epsilon }}}}\\right ) , thus setting the theoretical performance limit. Thirdly, we develop two novel missing tag identification algorithms with the expected execution time of \\Theta \\left ({{\\frac {\\log \\log N}{\\log N}N+\\frac {(1-\\alpha )^{2}(1-\\delta )^{2}}{ \\epsilon ^{2}}}}\\right ) , reducing the time overhead by a factor of up to \\log N over the best algorithm in the literature. The key technicality in our first algorithm is a novel data structure termed as collision-partition tree (CPT), built on a subset of bits in tag pseudo-IDs, leading to a more balanced tree structure and reducing the time complexity in parsing the entire tree. To further improve time efficiency, our second algorithm integrates multiple CPTs to form a collision-partition forest (CPF), reducing both the number of slots and the quantity of information broadcasting.}
}


@article{DBLP:journals/ton/BanerjeeU24,
	author = {Subhankar Banerjee and
                  Sennur Ulukus},
	title = {The Freshness Game: Timely Communications in the Presence of an Adversary},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4067--4084},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3410228},
	doi = {10.1109/TNET.2024.3410228},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BanerjeeU24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a communication system where a base station (BS) transmits update packets to N users, one user at a time, over a wireless channel. We investigate the age of this status updating system with an adversary that jams the update packets in the downlink. We consider two system models: with diversity and without diversity. In the model without diversity, in each time slot, the BS schedules a user from N users according to a user scheduling algorithm. The constrained adversary blocks at most a given fraction, $\\alpha $ , of the time slots over a horizon of T slots, i.e., it can block at most $\\alpha T$ slots of its choosing out of the total T time slots. We show that if the BS schedules the users with a stationary randomized policy, then the optimal choice for the adversary is to block the user which has the lowest probability of getting scheduled by the BS, at the middle of the time horizon, consecutively for $\\alpha T$ time slots. The interesting consecutive property of the blocked time slots is due to the cumulative nature of the age metric. In the model with diversity, in each time slot, the BS schedules a user from N users and chooses a sub-carrier from $N_{sub}$ sub-carriers to transmit update packets to the scheduled user according to a user scheduling algorithm and a sub-carrier choosing algorithm, respectively. The adversary blocks $\\alpha T$ time slots of its choosing out of T time slots at the sub-carriers of its choosing. We show that for large T, the uniform user scheduling algorithm together with the uniform sub-carrier choosing algorithm is $\\frac {2 N_{sub}}{N_{sub}-1}$ optimal. Next, we investigate the game theoretic equilibrium points of this status updating system. For the model without diversity, we show that a Nash equilibrium does not exist, however, a Stackelberg equilibrium exists when the scheduling algorithm of the BS acts as the leader and the adversary acts as the follower. For the model with diversity, we show that a Nash equilibrium exists and identify the Nash equilibrium. Finally, we extend the model without diversity to the case where the BS can serve multiple users and the adversary can jam multiple users, at a time.}
}


@article{DBLP:journals/ton/ChenQZZSW24,
	author = {Ning Chen and
                  Tie Qiu and
                  Xiaobo Zhou and
                  Songwei Zhang and
                  Weisheng Si and
                  Dapeng Oliver Wu},
	title = {A Distributed Co-Evolutionary Optimization Method With Motif for Large-Scale
                  IoT Robustness},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4085--4098},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3407769},
	doi = {10.1109/TNET.2024.3407769},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenQZZSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast-advancing mobile communication technologies have increased the scale of the Internet of Things (IoT) dramatically. However, this poses a tough challenge to the robustness of IoT networks when the network scale is large. In this paper, we present DAC-Motif, a distributed co-evolutionary method for optimizing network robustness based on network motifs. Unlike centralized evolutionary optimization approaches, DAC-Motif uses the technique of Divide-And-Conquer (DAC) to divide the large-scale IoT topology into partitions and then merge the self-evolving partitions into a global robust topology. This approach leverages both distributed computing and asynchronous communication mechanisms to mitigate premature convergence and reduce time complexity for large-scale IoT topologies. In our evaluation, DAC-Motif achieves three to four orders of magnitude shorter running time and over 10% robustness improvement compared to other centralized evolutionary algorithms under a scale of around 5,000 IoT devices.}
}


@article{DBLP:journals/ton/LiuFCWYJ24,
	author = {Xiaoqing Liu and
                  Jianxi Fan and
                  Baolei Cheng and
                  Yan Wang and
                  Bai Yin and
                  Xiaohua Jia},
	title = {A Family of General Architectures Toward Interconnection Networks
                  and Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4099--4113},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3411021},
	doi = {10.1109/TNET.2024.3411021},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuFCWYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Networks of large scales are an essential component in supercomputing systems as well as in data centers. As the network scale increases, the probability of processor/server failures also inevitably increases. It is therefore a worthwhile undertaking to make efforts reducing, as much as possible, the effect of faulty processors/servers to the entire network. This paper introduces a new class of network architectures, called circulant-based recursive networks (CRNs), and investigates CRN’s diameter, connectivity, and in particular, the fault diagnosability under the two diagnostic models−the PMC and the comparison diagnostic models. CRNs are a generalization of some well-known interconnection networks−hypercube, k-ary n-cube network and the data center network BCube, as well as some other less-known networks. In addition to obtaining its diagnosability properties, the paper also presents a one-to-one (unicast) path construction algorithm named SPath. Based on SPath, we further propose an algorithm FTPath for CRNs finding a fault-tolerant path between any two vertices, provided that the number of faulty vertices is no more than its connectivity minus one. Three parameters−average distance, message density, and cost−are used to assess CRNs’ performance. Experimental comparisons are conducted, and the results indicate that the average path length obtained by the algorithm SPath (resp., FTPath) is shorter than that of the Depth-First Search algorithm (DFS) and is on a par with the Breath-First Search algorithm (BFS).}
}


@article{DBLP:journals/ton/HuHLHW24,
	author = {Jinbin Hu and
                  Yi He and
                  Wangqing Luo and
                  Jiawei Huang and
                  Jin Wang},
	title = {Enhancing Load Balancing With In-Network Recirculation to Prevent
                  Packet Reordering in Lossless Data Centers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4114--4127},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3403671},
	doi = {10.1109/TNET.2024.3403671},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuHLHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many existing load balancing mechanisms work effectively in lossy datacenter networks (DCNs), but they suffer from serious packet reordering in lossless Ethernet DCNs deployed with the hop-by-hop Priority-based Flow Control (PFC). The key reason is that the prior solutions are not able to perceive PFC triggering correctly and in a timely manner when making load balancing decisions. Once the forwarding path pauses transmission due to PFC triggering, the packets allocated on it are blocked, inevitably leading to out-of-order packets and retransmission. In this paper, we present an Reordering-robust Load Balancing (RLB) scheme with PFC prediction in lossless DCNs. At its heart, RLB leverages the derivative of ingress queue length to predict PFC triggering and proactively notifies the upstream switches to choose an appropriate rerouting path or perform packet recirculation to avoid reordering. Furthermore, under switch failure scenarios, RLB adjusts the recirculation threshold adaptively to mitigate the risk of packets over-recirculation. We have implemented RLB in the hardware programmable switch. As a building block for existing load balancing mechanisms, we have integrated RLB into Presto, LetFlow, Hermes and DRILL. The evaluation results show that the RLB-enhanced solutions deliver significant performance by avoiding packet reordering. For example, it reduces the 99^{th}\npercentile flow completion time (FCT) by up to 72%, 67%, 58% and 54% over DRILL, Presto, LetFlow and Hermes, respectively.}
}


@article{DBLP:journals/ton/YangXZZLQ24,
	author = {Peng Yang and
                  Hongli Xu and
                  Gongming Zhao and
                  Qianyu Zhang and
                  Jiawei Liu and
                  Chunming Qiao},
	title = {{ALEPH:} Accelerating Distributed Training With eBPF-Based Hierarchical
                  Gradient Aggregation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4128--4143},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3404999},
	doi = {10.1109/TNET.2024.3404999},
	timestamp = {Tue, 04 Mar 2025 20:33:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangXZZLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed training includes two important operations: gradient transmission and gradient aggregation, which will consume massive bandwidth and computing resources. To achieve efficient distributed training, one must overcome two critical challenges: heterogeneity of bandwidth resources and limitation of computing resources among compute nodes. Existing architectures based on Parameter Server (PS) and All-Reduce (AR) fail to cope with these challenges because the PS will aggregate gradients from all workers and suffers from bandwidth bottlenecks, while AR intends to alleviate bandwidth bottlenecks at the PS, but the workers need to process many gradient packets thus can be overloaded. To address these shortcomings, we design a new distributed training system called ALEPH. In the control plane, ALEPH uses an efficient algorithm to group workers into clusters with different sizes so as to fully utilize heterogeneous bandwidth. We show that the proposed algorithm can achieve a good approximation performance. In the data plane, ALEPH leverages, for the first time, extended Berkeley Packet Filter (eBPF) programs to aggregate and forward gradient packets to reduce computation overhead. We show how to overcome several hurdles in using eBPF for distributed training. We implement ALEPH and evaluate its performance on a small-scale testbed and large-scale simulations. Experimental results show that ALEPH reduces training time by 20%-31% and increases bandwidth utilization by 88% compared with state-of-the-art frameworks.}
}


@article{DBLP:journals/ton/DangXXCL24,
	author = {Fan Dang and
                  Yifan Xu and
                  Rongwu Xu and
                  Xinlei Chen and
                  Yunhao Liu},
	title = {LSync: {A} Universal Timeline-Synchronizing Solution for Live Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4144--4159},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3408147},
	doi = {10.1109/TNET.2024.3408147},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DangXXCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread use of intelligent devices and the development of mobile networks have led to the increasing popularity of live-streaming services worldwide. In addition to video and audio transmissions, a wide range of media content is also sent to audiences, such as player statistics for sports streams and subtitles for live news. However, due to the diverse transmission process between live streams and other media content, synchronizing them has become a significant challenge. Unfortunately, existing commercial solutions are not universal, requiring specific server cloud services or CDNs and limiting users’ free choices of web infrastructures. To address this issue, we propose a lightweight and universal solution called LSync, which inserts a series of audio signals containing metadata into the original audio stream. Based on the embedded metadata, a well-designed timeline-synchronizing solution helps to synchronize the information stream to the live stream. It brings no modifications to the original live broadcast process and thus fits prevalent live broadcast infrastructures. Evaluations show that the proposed solution reduces the signal processing delay to around 5% of an audio buffer length in mobile phones and ensures real-time signal processing. It achieves a channel utilization of more than 150 bps/kHz in a specific configuration, greatly outperforming recent works. Furthermore, the proposed synchronization mechanism reaches a precision of 24.84 ms on average, which matches people’s viewing habits.}
}


@article{DBLP:journals/ton/YangLLWX24,
	author = {Huanqi Yang and
                  Zhenjiang Li and
                  Chengwen Luo and
                  Bo Wei and
                  Weitao Xu},
	title = {InaudibleKey2.0: Deep Learning-Empowered Mobile Device Pairing Protocol
                  Based on Inaudible Acoustic Signals},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4160--4174},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3407783},
	doi = {10.1109/TNET.2024.3407783},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangLLWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing proliferation of Internet-of-Things (IoT) devices in daily life has rendered secure Device-to-Device (D2D) communication increasingly crucial. Achieving secure D2D communication necessitates key agreement between various IoT devices without prior knowledge. Despite existing literature proposing numerous approaches, they exhibit limitations such as low key generation rates and short pairing distances. In this paper, we present InaudibleKey2.0, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey2.0 exploits the acoustic channel frequency response of two legitimate devices as a shared secret for key generation. To significantly enhance performance, InaudibleKey2.0 incorporates novel technologies, including a deep learning-enabled channel prediction model for improved channel reciprocity, a quantization model for increased key generation rates, and a transformer-based reconciliation method for augmented key agreement rates. We conduct comprehensive experiments to evaluate InaudibleKey2.0 in diverse real-world environments. In comparison to state-of-the-art solutions, InaudibleKey2.0 achieves 1.3–9.1 times improvement in key generation rates, 3.2–44 times extension in pairing distances, and 1.2–16 times reduction in information reconciliation counts. Security analysis substantiates that InaudibleKey2.0 is resilient to numerous malicious attacks. Furthermore, we implement InaudibleKey2.0 on modern smartphones and resource-limited IoT devices. The results indicate that it is energy-efficient and can operate on both powerful and resource-limited IoT devices without causing excessive resource consumption.}
}


@article{DBLP:journals/ton/SunLWQZLW24,
	author = {Haifeng Sun and
                  Xingjian Liao and
                  Jingyu Wang and
                  Qi Qi and
                  Zirui Zhuang and
                  Jianxin Liao and
                  Dapeng Oliver Wu},
	title = {Fast and Scalable {ACL} Policy Solving Under Complex Constraints With
                  Graph Neural Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4175--4190},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3409529},
	doi = {10.1109/TNET.2024.3409529},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunLWQZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network operators often need to modify Access Control List (ACL) policies to align with to network upgrades. An essential part of the ACL update task is reachability satisfaction. Previous studies formalize reachability requirements as a set of constraints and then use Boolean Satisfiability (SAT) or Satisfiability Modulo Theories (SMT) solvers to search for solutions. However, as today’s networks grow in size and complexity, the constraints derived from the requirements become increasingly complex, leading to an unacceptable time cost to obtain a correct policy. The sluggish updating of ACL policies can affect the properties of a network, such as connectivity and security. This paper presents a novel approach for fast and scalable ACL policy synthesis under complex constraints. We utilize Graph Neural Networks (GNNs) to learn the relations between nodes and reason the solution that satisfies the update requirements. We further integrate global position encoding into the GNN architecture, which allows for better differentiation of nodes in ACL update tasks. Additionally, an enhanced stochastic local search solver is introduced to address incorrect predictions made by the GNN. Experiments on real-world topologies show that GNN saves up 278\\times time costs compared to advanced SAT/SMT solvers on a 125-node network, and this advantage expands with the network size. Furthermore, our model extrapolates well when faced with different requirements and topologies, demonstrating its ability to handle frequent network upgrades.}
}


@article{DBLP:journals/ton/ChenZSLZHZZLW24,
	author = {Xiang Chen and
                  Wenbin Zhang and
                  Xi Sun and
                  Hongyan Liu and
                  Jianshan Zhang and
                  Qun Huang and
                  Dong Zhang and
                  Haifeng Zhou and
                  Xuan Liu and
                  Chunming Wu},
	title = {Resource-Efficient and Timely Packet Header Vector {(PHV)} Encoding
                  on Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4191--4206},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3413530},
	doi = {10.1109/TNET.2024.3413530},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenZSLZHZZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The programmable switch offers a limited capacity of packet header vector (PHV) words that store packet header fields and metadata fields defined by network functions. However, existing switch compilers employ inefficient strategies of encoding fields on PHV words. Their encoding wastes scarce PHV words and may result in failures when deploying network functions. In this paper, we propose Melody, a new framework that reuses PHV words for as many fields as possible to achieve resource-efficient PHV encoding. Melody offers a field analyzer and an optimization framework. The analyzer identifies which fields can reuse PHV words while preserving the original packet processing logic. The framework integrates analysis results into its encoding to offer the resource-optimal decisions. Also, to achieve timeliness at runtime, it provides a Greedy-based heuristic, which quickly solves PHV encoding and returns near-optimal results. We evaluate Melody with production-scale network functions. Our results show that Melody reduces the consumption of PHV words by up to 85%.}
}


@article{DBLP:journals/ton/RuanZJ24,
	author = {Yichen Ruan and
                  Xiaoxi Zhang and
                  Carlee Joe{-}Wong},
	title = {How Valuable is Your Data? Optimizing Client Recruitment in Federated
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4207--4221},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3422264},
	doi = {10.1109/TNET.2024.3422264},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/RuanZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning allows distributed clients to train a shared machine learning model while preserving user privacy. In this framework, user devices (i.e., clients) perform local iterations of the learning algorithm on their data. These updates are periodically aggregated to form a shared model. Thus, a client represents the bundle of the user data, the device, and the user’s willingness to participate: since participating in federated learning requires clients to expend resources and reveal some information about their data, users may require some form of compensation to contribute to the training process. Recruiting more users generally results in higher accuracy, but slower completion time and higher cost. We propose the first work to theoretically analyze the resulting performance tradeoffs in deciding which clients to recruit for the federated learning algorithm. Our framework accounts for both accuracy (training and testing) and efficiency (completion time and cost) metrics. We provide solutions to this NP-Hard optimization problem and verify the value of client recruitment in experiments on synthetic and real-world data. The results of this work can serve as a guideline for the real-world deployment of federated learning and an initial investigation of the client recruitment problem.}
}


@article{DBLP:journals/ton/KarSLLS24,
	author = {Avik Kar and
                  Rahul Singh and
                  Fang Liu and
                  Xin Liu and
                  Ness B. Shroff},
	title = {Linear Bandits With Side Observations on Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4222--4237},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3422323},
	doi = {10.1109/TNET.2024.3422323},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KarSLLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate linear bandits in a network setting in the presence of side-observations across nodes in order to design recommendation algorithms for users connected via social networks. Users in social networks respond to their friends’ activity and, hence, provide information about each other’s preferences. In our model, when a learning algorithm recommends an article to a user, not only does it observe her response (e.g., an ad click) but also the side-observations, i.e., the response of her neighbors if they were presented with the same article. We model these observation dependencies by a graph \\mathcal {G} in which nodes correspond to users and edges to social links. We derive a problem/instance-dependent lower-bound on the regret of any consistent algorithm. We propose an optimization-based data-driven learning algorithm that utilizes the structure of \\mathcal {G} in order to make recommendations to users and show that it is asymptotically optimal, in the sense that its regret matches the lower-bound as the number of rounds T\\to \\infty . We show that this asymptotically optimal regret is upper-bounded as O\\left ({{|\\chi (\\mathcal {G})|\\log T}}\\right) , where |\\chi (\\mathcal {G})| is the domination number of \\mathcal {G} . In contrast, a naive application of the existing learning algorithms results in O\\left ({{N\\log T}}\\right) regret, where N is the number of users.}
}


@article{DBLP:journals/ton/LiFCYLSDN24,
	author = {Zonghang Li and
                  Wenjiao Feng and
                  Weibo Cai and
                  Hongfang Yu and
                  Long Luo and
                  Gang Sun and
                  Hongyang Du and
                  Dusit Niyato},
	title = {Accelerating Geo-Distributed Machine Learning With Network-Aware Adaptive
                  Tree and Auxiliary Route},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4238--4253},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3412429},
	doi = {10.1109/TNET.2024.3412429},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiFCYLSDN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions. This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks. Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures. However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements. This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers. First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology. Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions. Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions. Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies. Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET.}
}


@article{DBLP:journals/ton/ZhaoYY24,
	author = {Pengxiang Zhao and
                  Jintao You and
                  Xiaoming Yuan},
	title = {Circling Reduction Algorithm for Cloud Edge Traffic Allocation Under
                  the 95th Percentile Billing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4254--4269},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3415649},
	doi = {10.1109/TNET.2024.3415649},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cloud ecosystems, managing bandwidth costs is pivotal for both operational efficiency and service quality. This paper tackles the cloud-edge traffic allocation problem, particularly optimizing for the 95th percentile billing scheme, which is widely employed across various cloud computing scenarios by Internet Service Providers but has yet to be efficiently addressed. We introduce a mathematical model for this issue, confirm its NP-hard complexity, and reformulate it as a mixed-integer programming (MIP). The intricacy of the problem is further magnified by the scale of the cloud ecosystem, involving numerous data centers, client groups, and long billing cycles. Based on a structural analysis of our MIP model, we propose a two-stage solution strategy that retains optimality. We introduce the Circling Reduction Algorithm (CRA), a polynomial-time algorithm based on a rigorously derived lower bound for the objective value, to efficiently determine the binary variables in the first stage, while the remaining linear programming problem in the second stage can be easily resolved. Using the CRA, we develop algorithms for both offline and online traffic allocation scenarios and validate them on real-world datasets from the cloud provider under study. In offline scenarios, our method delivers up to 66.34% cost savings compared to a commercial solver, while also significantly improving computational speed. Additionally, it achieves an average of 14% cost reduction over the current solution of the studied cloud provider. For online scenarios, we achieve an average cost-saving of 8.64% while staying within a 9% gap of the theoretical optimum.}
}


@article{DBLP:journals/ton/LiuCHSWZWLY24,
	author = {Hongyan Liu and
                  Xiang Chen and
                  Qun Huang and
                  Guoqiang Sun and
                  Peiqiao Wang and
                  Dong Zhang and
                  Chunming Wu and
                  Xuan Liu and
                  Qiang Yang},
	title = {Toward Resource-Efficient and High- Performance Program Deployment
                  in Programmable Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4270--4285},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3413388},
	doi = {10.1109/TNET.2024.3413388},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuCHSWZWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable switches allow administrators to customize packet processing behaviors in data plane programs. However, existing solutions for program deployment fail to achieve resource efficiency and high packet processing performance. In this paper, we propose SPEED, a system that provides resource-efficient and high-performance deployment for data plane programs. For resource efficiency, SPEED merges input data plane programs by reducing program redundancy. Then it abstracts the substrate network into an one big switch (OBS), and deploys the merged program on the OBS while minimizing resource usage. For high performance, SPEED searches for the performance-optimal mapping between the OBS and the substrate network with respect to network-wide constraints. It also maintains program logic among different switches via inter-device packet scheduling. We have implemented SPEED on a Barefoot Tofino switch. The evaluation indicates that SPEED achieves resource-efficient and high-performance deployment for real data plane programs.}
}


@article{DBLP:journals/ton/XiongWLS24,
	author = {Guojun Xiong and
                  Shufan Wang and
                  Jian Li and
                  Rahul Singh},
	title = {Whittle Index-Based Q-Learning for Wireless Edge Caching With Linear
                  Function Approximation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4286--4301},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3417351},
	doi = {10.1109/TNET.2024.3417351},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiongWLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of content caching at the wireless edge to serve a set of end users via unreliable wireless channels so as to minimize the average latency experienced by end users due to the constrained wireless edge cache capacity. We formulate this problem as a Markov decision process, or more specifically a restless multi-armed bandit problem, which is provably hard to solve. We begin by investigating a discounted counterpart, and prove that it admits an optimal policy of the threshold-type. We then show that this result also holds for average latency problem. Using this structural result, we establish the indexability of our problem, and employ the Whittle index policy to minimize average latency. Since system parameters such as content request rates and wireless channel conditions are often unknown and time-varying, we further develop a model-free reinforcement learning algorithm dubbed as Q+-Whittle that relies on Whittle index policy. However, Q+-Whittle requires to store the Q-function values for all state-action pairs, the number of which can be extremely large for wireless edge caching. To this end, we approximate the Q-function by a parameterized function class with a much smaller dimension, and further design a Q+-Whittle algorithm with linear function approximation, which is called Q+-Whittle-LFA. We provide a finite-time bound on the mean-square error of Q+-Whittle-LFA. Simulation results using real traces demonstrate that Q+-Whittle-LFA yields excellent empirical performance.}
}


@article{DBLP:journals/ton/LiuZJWZTPH24,
	author = {Kefei Liu and
                  Jiao Zhang and
                  Zhuo Jiang and
                  Haoran Wei and
                  Xiaolong Zhong and
                  Lizhuang Tan and
                  Tian Pan and
                  Tao Huang},
	title = {Diagnosing End-Host Network Bottlenecks in {RDMA} Servers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4302--4316},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3416419},
	doi = {10.1109/TNET.2024.3416419},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuZJWZTPH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In RDMA (Remote Direct Memory Access) networks, end-host networks, including intra-host networks and RNICs (RDMA NIC), were considered robust and have received little attention. However, as the RNIC line rate rapidly increases to multi-hundred gigabits, the intra-host network becomes a potential performance bottleneck for network applications. Intra-host network bottlenecks can result in degraded intra-host bandwidth and increased intra-host latency. In addition, RNIC network problems can result in connection failures and packet drops. Host network problems can severely degrade network performance. However, when host network problems occur, they can hardly be noticed due to the lack of a monitoring system. Furthermore, existing diagnostic mechanisms cannot efficiently diagnose host network problems. In this paper, we analyze the symptom of host network problems based on our long-term troubleshooting experience and propose Hostping, the first monitoring and diagnostic system dedicated to host networks. The core idea of Hostping is to conduct 1) loopback tests between RNICs and endpoints within the host to measure intra-host latency and bandwidth, and 2) mutual probing between RNICs on a host to measure RNIC connectivity. We have deployed Hostping on thousands of servers in our distributed machine learning system. Not only can Hostping detect and diagnose host network problems we already knew in minutes, but it also reveals eight problems we did not notice before.}
}


@article{DBLP:journals/ton/QiuZXHQ24,
	author = {Yuhang Qiu and
                  Gongming Zhao and
                  Hongli Xu and
                  He Huang and
                  Chunming Qiao},
	title = {{PARING:} Joint Task Placement and Routing for Distributed Training
                  With In-Network Aggregation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4317--4332},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3414853},
	doi = {10.1109/TNET.2024.3414853},
	timestamp = {Sun, 19 Jan 2025 13:55:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiuZXHQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increase in both the model size and dataset size of distributed training (DT) tasks, communication between the workers and parameter servers (PSs) in a cluster has become a bottleneck. In-network aggregation (INA) enabled by programmable switches has been proposed as a promising solution to alleviate the communication bottleneck. However, existing works focused on in-network aggregation implementation based on simple DT placement and fixed routing policies, which may lead to a large communication overhead and inefficient use of resources (e.g., storage, computing power and bandwidth). In this paper, we propose PARING, the first-of-its-kind INA approach that jointly optimizes DT task placement and routing in order to reduce traffic volume and minimize communication time. We formulate the problem as a nonlinear multi-objective mixed-integer programming problem, and prove its NP-Hardness. Based on the concept of Steiner trees, an algorithm with bounded approximation factors is proposed for this problem. Large-scale simulations show that our algorithm can reduce communication time by up to 81.0% and traffic volume by up to 19.1% compared to the state-of-the-art algorithms.}
}


@article{DBLP:journals/ton/ZhangQ24,
	author = {Xiaoxue Zhang and
                  Chen Qian},
	title = {Toward Aggregated Payment Channel Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4333--4348},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423000},
	doi = {10.1109/TNET.2024.3423000},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Payment channel networks (PCNs) have been designed and utilized to address the scalability challenge and throughput limitation of blockchains. It provides a high-throughput solution for blockchain-based payment systems. However, such “layer-2” blockchain solutions have their own problems: payment channels require a separate deposit for each channel of two users. Thus it significantly locks funds from users into particular channels without the flexibility of moving these funds across channels. In this paper, we proposed Aggregated Payment Channel Network (APCN), in which flexible funds are used as a per-user basis instead of a per-channel basis. To prevent users from misbehaving such as double-spending, APCN includes mechanisms that make use of hardware trusted execution environments (TEEs) to control funds, balances, and payments. The distributed routing protocol in APCN also addresses the congestion problem to further improve resource utilization. Our prototype implementation and simulation results show that APCN achieves significant improvements on transaction success ratio with low routing latency, compared to even the most advanced PCN routing.}
}


@article{DBLP:journals/ton/QuanES24a,
	author = {Guocong Quan and
                  Atilla Eryilmaz and
                  Ness B. Shroff},
	title = {Minimizing Edge Caching Service Costs Through Regret-Optimal Online
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4349--4364},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3420758},
	doi = {10.1109/TNET.2024.3420758},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QuanES24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge caching has been widely implemented to efficiently serve data requests from end users. Numerous edge caching policies have been proposed to adaptively update the cache contents based on various statistics. One critical statistic is the miss cost, which could measure the latency or the bandwidth/energy consumption to resolve the cache miss. Existing caching policies typically assume that the miss cost for each data item is fixed and known. However, in real systems, they could be random with unknown statistics. A promising approach would be to use online learning to estimate the unknown statistics of these random costs, and make caching decisions adaptively. Unfortunately, conventional learning techniques cannot be directly applied, because the caching problem has additional cache capacity and cache update constraints that are not covered in traditional learning settings. In this work, we resolve these issues by developing a novel edge caching policy that learns uncertain miss costs efficiently, and is shown to be asymptotically optimal. We first derive an asymptotic lower bound on the achievable regret. We then design a Kullback-Leibler lower confidence bound (KL-LCB) based edge caching policy, which adaptively learns the random miss costs by following the “optimism in the face of uncertainty” principle. By employing a novel analysis that accounts for the new constraints and the dynamics of the setting, we prove that the regret of the proposed policy matches the regret lower bound, thus showing asymptotic optimality. Further, via numerical experiments we demonstrate the performance improvements of our policy over natural benchmarks.}
}


@article{DBLP:journals/ton/WangMHCB24,
	author = {Su Wang and
                  Roberto Morabito and
                  Seyyedali Hosseinalipour and
                  Mung Chiang and
                  Christopher G. Brinton},
	title = {Device Sampling and Resource Optimization for Federated Learning in
                  Cooperative Edge Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4365--4381},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423673},
	doi = {10.1109/TNET.2024.3423673},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangMHCB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The conventional federated learning (FedL) architecture distributes machine learning (ML) across worker devices by having them train local models that are periodically aggregated by a server. FedL ignores two important characteristics of contemporary wireless networks, however: (i) the network may contain heterogeneous communication/computation resources, and (ii) there may be significant overlaps in devices’ local data distributions. In this work, we develop a novel optimization methodology that jointly accounts for these factors via intelligent device sampling complemented by device-to-device (D2D) offloading. Our optimization methodology aims to select the best combination of sampled nodes and data offloading configuration to maximize FedL training accuracy while minimizing data processing and D2D communication resource consumption subject to realistic constraints on the network topology and device capabilities. Theoretical analysis of the D2D offloading subproblem leads to new FedL convergence bounds and an efficient sequential convex optimizer. Using these results, we develop a sampling methodology based on graph convolutional networks (GCNs) which learns the relationship between network attributes, sampled nodes, and D2D data offloading to maximize FedL accuracy. Through evaluation on popular datasets and real-world network measurements from our edge testbed, we find that our methodology outperforms popular device sampling methodologies from literature in terms of ML model performance, data processing overhead, and energy consumption.}
}


@article{DBLP:journals/ton/ShaoMTLWND24,
	author = {Chenglong Shao and
                  Osamu Muta and
                  Kazuya Tsukamoto and
                  Wonjun Lee and
                  Xianpeng Wang and
                  Malvin Nkomo and
                  Kapil R. Dandekar},
	title = {Toward Improved Energy Fairness in CSMA-Based LoRaWAN},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4382--4397},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3418913},
	doi = {10.1109/TNET.2024.3418913},
	timestamp = {Sun, 19 Jan 2025 13:55:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShaoMTLWND24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a heterogeneous carrier-sense multiple access (CSMA) protocol named LoHEC as the first research attempt to improve energy fairness when applying CSMA to long-range wide area network (LoRaWAN). LoHEC is enabled by Channel Activity Detection (CAD), a recently introduced carrier-sensing technique to detect LoRaWAN signals even below the noise floor. The design of LoHEC is inspired by the fact that existing CAD-based CSMA proposals are in a homogeneous manner. In other words, they require LoRaWAN end devices to perform identical CAD regardless of the differences of their used network parameter – spreading factor (SF). This causes energy consumption imbalance among end devices since the consumed energy during CAD is significantly affected by SF. By considering the heterogeneity of LoRaWAN in terms of SF, LoHEC requires end devices to perform different numbers of CAD operations with different CAD intervals during channel access. Particularly, the number of needed CADs and CAD interval are determined based on the CAD energy consumption under different SFs. We conduct extensive experiments regarding LoHEC with a practical LoRaWAN testbed including 60 commercial off-the-shelf end devices. Experimental results show that in comparison with the existing solutions, LoHEC can achieve up to 0.85\\times improvement of the energy fairness on average.}
}


@article{DBLP:journals/ton/LinLGSFWY24,
	author = {Jinlei Lin and
                  Chenglong Li and
                  Wenwen Gong and
                  Guanglei Song and
                  Linna Fan and
                  Zhiliang Wang and
                  Jiahai Yang},
	title = {ProbeGeo: {A} Comprehensive Landmark Mining Framework Based on Web
                  Content},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4398--4413},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3422089},
	doi = {10.1109/TNET.2024.3422089},
	timestamp = {Sun, 19 Jan 2025 13:55:23 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinLGSFWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IP geolocation is essential for various location-aware Internet applications. High-quality IP geolocation landmarks play a decisive role in IP geolocation accuracy. However, the previous research works focusing on mining landmarks from the Internet are hampered by limited quantity, poor coverage, and insufficient landmark quality. In this paper, we present a new framework called ProbeGeo to mine high-quality landmarks automatically. We divide landmarks into common landmarks and probe landmarks, providing systematic mining methods based on online retrieval and web content. ProbeGeo expands traditional common landmarks by taking advantage of the exposure of multiple IoT (Internet of Things) devices on the Internet, mining them based on search engines and webpage contents. Common landmarks, consisting of multi-type devices, significantly improve landmark quantity and coverage. Furthermore, ProbeGeo establishes a methodology for acquiring new probe landmarks from Internet VPs (Vantage Points) webpages, extracting geographical locations from heterogeneous webpages and utilizing active probe functions. Probe landmarks enhance landmark quality and functions, bringing new geolocation frameworks and breaking through the geolocation accuracy bottleneck. We develop the ProbeGeo as a continuously running system and conduct real-world experiments to validate its efficacy. Our results show that ProbeGeo can detect 89,849 high-quality landmarks, including 6,874 probe landmarks and 82,975 common landmarks. ProbeGeo landmarks are about 10x more than existing work, distributed in 181 countries and 7,094 cities. ProbeGeo landmarks cover more than 8 types of devices, and more than 60% of them remain stable over one month. Moreover, the landmark accuracy of more than 58% of ProbeGeo landmarks is above street level, which has not been achieved in previous works. ProbeGeo can provide geolocation services with higher landmark accuracy and broader coverage by correlating a large scale of landmarks.}
}


@article{DBLP:journals/ton/LiOZLZC24,
	author = {Rui Li and
                  Tao Ouyang and
                  Liekang Zeng and
                  Guocheng Liao and
                  Zhi Zhou and
                  Xu Chen},
	title = {Online Optimization of {DNN} Inference Network Utility in Collaborative
                  Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4414--4426},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3421356},
	doi = {10.1109/TNET.2024.3421356},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiOZLZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Edge Computing (CEC) is an emerging paradigm that collaborates heterogeneous edge devices as a resource pool to compute DNN inference tasks in proximity such as edge video analytics. Nevertheless, as the key knob to improve network utility in CEC, existing works mainly focus on the workload routing strategies among edge devices with the aim of minimizing the routing cost, remaining an open question for joint workload allocation and routing optimization problem from a system perspective. To this end, this paper presents a holistic, learned optimization for CEC towards maximizing the total network utility in an online manner, even though the utility functions of task input rates are unknown a priori. In particular, we characterize the CEC system in a flow model and formulate an online learning problem in a form of cross-layer optimization. We propose a nested-loop algorithm to solve workload allocation and distributed routing iteratively, using the tools of gradient sampling and online mirror descent. To improve the convergence rate over the nested-loop version, we further devise a single-loop algorithm. Rigorous analysis is provided to show its inherent convexity, efficient convergence, as well as algorithmic optimality. Finally, extensive numerical simulations demonstrate the superior performance of our solutions.}
}


@article{DBLP:journals/ton/HouWZ24,
	author = {Jing Hou and
                  Xuyu Wang and
                  Amy Z. Zeng},
	title = {Inter-Temporal Reward Strategies in the Presence of Strategic Ethical
                  Hackers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4427--4440},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3422922},
	doi = {10.1109/TNET.2024.3422922},
	timestamp = {Sun, 19 Jan 2025 13:55:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HouWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A skyrocketing increase in cyber-attacks significantly elevates the importance of secure software development. Companies launch various bug-bounty programs to reward ethical hackers for identifying potential vulnerabilities in their systems before malicious hackers can exploit them. One of the most difficult decisions in bug-bounty programs is appropriately rewarding ethical hackers. This paper develops a model of an inter-temporal reward strategy with endogenous e-hacker behaviors. We formulate a novel game model to characterize the interactions between a software vendor and multiple heterogeneous ethical hackers. The optimal levels of rewards are discussed under different reward strategies. The impacts of ethical hackers’ strategic bug-hoarding and their competitive and collaborative behaviors on the performance of the program are also evaluated. We demonstrate the effectiveness of the inter-temporal reward mechanism in attracting ethical hackers and encouraging early bug reports. Our results indicate that ignoring the ethical hackers’ strategic behaviors could result in setting inappropriate rewards, which may inadvertently encourage them to hoard bugs for higher rewards. In addition, a more skilled e-hacker is more likely to delay their reporting and less motivated to work collaboratively with other e-hackers. Moreover, the vendor gains more from e-hacker collaboration when it could significantly increase the speed or probability of uncovering difficult-to-detect vulnerabilities.}
}


@article{DBLP:journals/ton/ChenLZHZZLW24,
	author = {Xiang Chen and
                  Hongyan Liu and
                  Wenbin Zhang and
                  Qun Huang and
                  Dong Zhang and
                  Haifeng Zhou and
                  Xuan Liu and
                  Chunming Wu},
	title = {Toward Full-Coverage and Low-Overhead Profiling of Network-Stack Latency},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4441--4455},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3421327},
	doi = {10.1109/TNET.2024.3421327},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLZHZZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern data center networks (DCNs), network-stack processing denotes a large portion of the end-to-end latency of TCP flows. So profiling network-stack latency anomalies has been considered as a crucial part in DCN performance diagnosis and troubleshooting. In particular, such profiling requires full coverage (i.e., profiling every TCP packet) and low overhead (i.e., profiling should avoid high CPU consumption in end-hosts). However, existing solutions rely on system calls or tracepoints in end-hosts to implement network-stack latency profiling, leading to either low coverage or high overhead. We propose Torp, a framework that offers full-coverage and low-overhead profiling of network-stack latency. Our key idea is to offload as much of the profiling from costly system calls or tracepoints to the Torp agent built on eBPF modules, and further to include a Torp handler on the ToR switch to accelerate the remaining profiling operations. Torp efficiently coordinates the ToR switch and the Torp agent on end-hosts to jointly execute the entire latency profiling task. We have implemented Torp on 32\\times 100 Gbps Tofino switches. Testbed experiments indicate that Torp achieves full coverage and orders of magnitude lower host-side overhead compared to other solutions.}
}


@article{DBLP:journals/ton/NguyenLNBSHT24,
	author = {Tung{-}Anh Nguyen and
                  Long Tan Le and
                  Tuan Dung Nguyen and
                  Wei Bao and
                  Suranga Seneviratne and
                  Choong Seon Hong and
                  Nguyen H. Tran},
	title = {Federated {PCA} on Grassmann Manifold for IoT Anomaly Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4456--4471},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423780},
	doi = {10.1109/TNET.2024.3423780},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NguyenLNBSHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of the Internet of Things (IoT) and the rising interconnectedness of devices, network security faces significant challenges, especially from anomalous activities. While traditional machine learning-based intrusion detection systems (ML-IDS) effectively employ supervised learning methods, they possess limitations such as the requirement for labeled data and challenges with high dimensionality. Recent unsupervised ML-IDS approaches such as AutoEncoders and Generative Adversarial Networks (GAN) offer alternative solutions but pose challenges in deployment onto resource-constrained IoT devices and in interpretability. To address these concerns, this paper proposes a novel federated unsupervised anomaly detection framework – FedPCA – that leverages Principal Component Analysis (PCA) and the Alternating Directions Method Multipliers (ADMM) to learn common representations of distributed non-i.i.d. datasets. Building on the FedPCA framework, we propose two algorithms, FedPE in Euclidean space and FedPG on Grassmann manifolds. Our approach enables real-time threat detection and mitigation at the device level, enhancing network resilience while ensuring privacy. Moreover, the proposed algorithms are accompanied by theoretical convergence rates even under a sub-sampling scheme, a novel result. Experimental results on the UNSW-NB15 and TON-IoT datasets show that our proposed methods offer performance in anomaly detection comparable to non-linear baselines, while providing significant improvements in communication and memory efficiency, underscoring their potential for securing IoT networks.}
}


@article{DBLP:journals/ton/WuQLDDC24,
	author = {Tao Wu and
                  Yuben Qu and
                  Chunsheng Liu and
                  Haipeng Dai and
                  Chao Dong and
                  Jiannong Cao},
	title = {Cost-Efficient Federated Learning for Edge Intelligence in Multi-Cell
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4472--4487},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423316},
	doi = {10.1109/TNET.2024.3423316},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WuQLDDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of various mobile devices with massive data and improving computing capacity have prompted the rise of edge artificial intelligence (Edge AI). Without revealing the raw data, federated learning (FL) becomes a promising distributed learning paradigm that caters to the above trend. Nevertheless, due to periodical communication for model aggregation, it would incur inevitable costs in terms of training latency and energy consumption, especially in multi-cell edge networks. Thus motivated, we study the joint edge aggregation and association problem to achieve the cost-efficient FL performance, where the model aggregation over multiple cells just happens at the network edge. After analyzing the NP-hardness with complex coupled variables, we transform it into a set function optimization problem and prove the objective function shows neither submodular nor supermodular property. By decomposing the complex objective function, we reconstruct a substitute function with the supermodularity and the bounded gap. On this basis, we design a two-stage search-based algorithm with theoretical performance guarantee. We further extend to the case of flexible bandwidth allocation and design the decoupled resource allocation algorithm with reduced computation size. Finally, extensive simulations and field experiments based on the testbed are conducted to validate both the effectiveness and near-optimality of our proposed solution.}
}


@article{DBLP:journals/ton/LuoYLX24,
	author = {Shouxi Luo and
                  Xiaoyu Yu and
                  Ke Li and
                  Huanlai Xing},
	title = {Releasing the Power of In-Network Aggregation With Aggregator-Aware
                  Routing Optimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4488--4502},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423380},
	doi = {10.1109/TNET.2024.3423380},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoYLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By offloading partial of the aggregation computation from the logical central parameter servers to network devices like programmable switches, In-Network Aggregation (INA) is a general, effective, and widely used approach to reduce network load thus alleviating the communication bottlenecks suffered by large-scale distributed training. Given the fact that INA would take effects if and only if associated traffic goes through the same in-network aggregator, the key to taking advantage of INA lies in routing control. However, existing proposals fall short in doing so and thus are far from optimal, since they select routes for INA-supported traffic without comprehensively considering the characteristics, limitations, and requirements of the network environment, aggregator hardware, and distributed training jobs. To fill the gap, in this paper, we systematically establish a mathematical model to formulate i) the up-down routing constraints of Clos datacenter networks, ii) the limitations raised by modern programmable switches’ pipeline hardware structure, and iii) the various aggregator-aware routing optimization goals required by distributed training tasks under different parallelism strategies. Based on the model, we develop ARO, an Aggregator-aware Routing Optimization solution for INA-accelerated distributed training applications. To be efficient, ARO involves a suite of search space pruning designs, by using the model’s characteristics, yielding tens of times improvement in the solving time with trivial performance loss. Extensive experiments show that ARO is able to find near-optimal results for large-scale routing optimization in tens of seconds, achieving\n1.8∼4.0×\nhigher throughput than the state-of-the-art solution.}
}


@article{DBLP:journals/ton/MaWWHHG24,
	author = {Zhongyu Ma and
                  Yajing Wang and
                  Zijun Wang and
                  Guangjie Han and
                  Zhanjun Hao and
                  Qun Guo},
	title = {Coalition Formation-Based Sub-Channel Allocation in Full-Duplex-Enabled
                  mmWave {IABN} With {D2D}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4503--4518},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3423775},
	doi = {10.1109/TNET.2024.3423775},
	timestamp = {Wed, 06 Nov 2024 22:18:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaWWHHG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the key techniques for future wireless network is full-duplex-enabled millimeter wave integrated access and backhaul network underlaying device-to-device communication, which is a 3GPP-inspired comprehensive paradigm for higher spectral efficiency and lower latency. However, the multi-user interference (MUI) and residual self-interference (RSI) become the major bottleneck before the commercial application of the system. To this end, we investigate the sub-channel allocation problem for this networking paradigm. To maximize the overall achievable rate under the considerations of MUI and RSI, the sub-channel allocation problem is firstly formulated as an integer nonlinear programming problem, which is intractable to search an optimal solution in polynomial time. Secondly, a coalition formation based sub-channel allocation (CFSA) algorithm is proposed, where the final partition of the sub-channel coalition is iteratively formed by the concurrent link players according to the two defined switching criterions. Thirdly, the properties of the proposed CFSA algorithm are analyzed from the perspectives of Nash stability and uniform convergence. Fourthly, the proposed CFSA algorithm is compared with other reference algorithms through abundant simulations, and superiorities including effectiveness, convergence and sub-optimality of the proposed CFSA algorithm are demonstrated through the kernel indicators.}
}


@article{DBLP:journals/ton/VassBFRR24,
	author = {Bal{\'{a}}zs Vass and
                  Erika R. B{\'{e}}rczi{-}Kov{\'{a}}cs and
                  {\'{A}}d{\'{a}}m Frakn{\'{o}}i and
                  Costin Raiciu and
                  G{\'{a}}bor R{\'{e}}tv{\'{a}}ri},
	title = {Charting the Complexity Landscape of Compiling Packet Programs to
                  Reconfigurable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4519--4534},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3424337},
	doi = {10.1109/TNET.2024.3424337},
	timestamp = {Sun, 19 Jan 2025 13:55:25 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VassBFRR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {P4 is a widely used Domain-specific Language for Programmable Data Planes. A critical step in P4 compilation is finding a feasible and efficient mapping of the high-level P4 source code constructs to the physical resources exposed by the underlying hardware, while meeting data and control flow dependencies in the program. In this paper, we take a new look at the algorithmic aspects of this problem, with the motivation to understand the fundamental theoretical limits and obtain better P4 pipeline embeddings, and to speed up practical P4 compilation times for RMT and dRMT target architectures. We report mixed results: we find that P4 compilation is computationally hard even in a severely relaxed formulation, and there is no polynomial-time approximation of arbitrary precision (unless \\mathcal {P} = \\mathcal {N} \\mathcal {P} ), while the good news is that, despite its inherent complexity, P4 compilation is approximable in linear time with a small constant bound even for the most complex, nearly real-life models.}
}


@article{DBLP:journals/ton/LiaoXXCWQ24,
	author = {Yunming Liao and
                  Yang Xu and
                  Hongli Xu and
                  Min Chen and
                  Lun Wang and
                  Chunming Qiao},
	title = {Asynchronous Decentralized Federated Learning for Heterogeneous Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {5},
	pages = {4535--4550},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3424444},
	doi = {10.1109/TNET.2024.3424444},
	timestamp = {Sun, 19 Jan 2025 13:55:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoXXCWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data generated at the network edge can be processed locally by leveraging the emerging technology of Federated Learning (FL). However, non-IID local data will lead to degradation of model accuracy and the heterogeneity of edge nodes inevitably slows down model training efficiency. Moreover, to avoid the potential communication bottleneck in the parameter-server-based FL, we concentrate on the Decentralized Federated Learning (DFL) that performs distributed model training in Peer-to-Peer (P2P) manner. To address these challenges, we propose an asynchronous DFL system by incorporating neighbor selection and gradient push, termed AsyDFL. Specifically, we require each edge node to push gradients only to a subset of neighbors for resource efficiency. Herein, we first give a theoretical convergence analysis of AsyDFL under the complicated non-IID and heterogeneous scenario, and further design a priority-based algorithm to dynamically select neighbors for each edge node so as to achieve the trade-off between communication cost and model performance. We evaluate the performance of AsyDFL through extensive experiments on a physical platform with 30 NVIDIA Jetson edge devices. Evaluation results show that AsyDFL can reduce the communication cost by 57% and the completion time by about 35% for achieving the same test accuracy, and improve model accuracy by at least 6% under the non-IID scenario, compared to the baselines.}
}


@article{DBLP:journals/ton/GuWDCWBZTHQXDC24,
	author = {Rong Gu and
                  Shulin Wang and
                  Haipeng Dai and
                  Xiaofei Chen and
                  Zhaokang Wang and
                  Wenjie Bao and
                  Jiaqi Zheng and
                  Yaofeng Tu and
                  Yihua Huang and
                  Lianyong Qi and
                  Xiaolong Xu and
                  Wanchun Dou and
                  Guihai Chen},
	title = {Fluid-Shuttle: Efficient Cloud Data Transmission Based on Serverless
                  Computing Compression},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4554--4569},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3402561},
	doi = {10.1109/TNET.2024.3402561},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuWDCWBZTHQXDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, there exists a lot of cross-region data transmission demand on the cloud. It is promising to use serverless computing for data compressing to save the total data size. However, it is challenging to estimate the data transmission time and monetary cost with serverless compression. In addition, minimizing the data transmission cost is non-trivial due to the enormous parameter space. This paper focuses on this problem and makes the following contributions: 1) We propose empirical data transmission time and monetary cost models based on serverless compression. It can also predict compression information, e.g., ratio and speed using chunk sampling and machine learning techniques. 2) For single-task cloud data transmission, we propose two efficient parameter search methods based on Sequential Quadratic Programming (SQP) and Eliminate then Divide and Conquer (EDC) with proven error upper bounds. Besides, we propose a parameter fine-tuning strategy to deal with transmission bandwidth variance. 3) Furthermore, for multi-task scenarios, a parameter search method based on dynamic programming and numerical computation is proposed. We have implemented the system called Fluid-Shuttle, which includes straggler optimization, cache optimization, and the autoscaling decompression mechanism. Finally, we evaluate the performance of Fluid-Shuttle with various workloads and applications on the real-world AWS serverless computing platform. Experimental results show that the proposed approach can improve the parameter search efficiency by over 3\\times\ncompared with the state-of-art methods and achieves better parameter quality. In addition, our approach achieves higher time efficiency and lower monetary cost compared with competing cloud data transmission approaches.}
}


@article{DBLP:journals/ton/ZhuZLM24,
	author = {Shaopeng Zhu and
                  Xiaolong Zheng and
                  Liang Liu and
                  Huadong Ma},
	title = {{FPCA:} Parasitic Coding Authentication for UAVs by {FM} Signals},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4570--4584},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3412958},
	doi = {10.1109/TNET.2024.3412958},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuZLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {De-authentication attack is one of the major threats to Unmanned Aerial Vehicle (UAV) communication, in which the attacker continuously sends de-authentication frames to disconnect the UAV communication link. Existing defense methods are based on authentication by digital passwords or physical channel features. But they suffer from replay attacks or cannot adapt to the UAV mobility. In this paper, instead of enhancing the in-channel authentication, we leverage the ambient broadcasting signal to establish a low-cost additional channel for authentication. Different from methods using another dedicated secure communication channel to perform an independent authentication, we use the ambient FM radio broadcasting channel and couple the two channels by encoding parasitic bits on the host signals of the broadcasting channel, which is called parasitic coding. To further enhance the security, we propose the FM-based Parasitic Coding Authentication (FPCA) that leverages elaborate host signal processing and vector coding to ensure that the attacker cannot decode our authentication even knowing the FM receiving frequency. We implement FPCA on the embedded UAV platform. The extensive experiments show that FPCA can resist replay attacks and brute force searching, achieving reliable continuous authentication for UAVs.}
}


@article{DBLP:journals/ton/ZhouGCLNWH24,
	author = {Mengying Zhou and
                  Tiancheng Guo and
                  Yang Chen and
                  Yupeng Li and
                  Meng Niu and
                  Xin Wang and
                  Pan Hui},
	title = {Polygon: {A} QUIC-Based {CDN} Server Selection System Supporting Multiple
                  Resource Demands},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4585--4599},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3425227},
	doi = {10.1109/TNET.2024.3425227},
	timestamp = {Tue, 11 Feb 2025 16:38:19 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouGCLNWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {CDN is a crucial Internet infrastructure ensuring quick access to Internet content. With the expansion of CDN scenarios, beyond delay, resource types like bandwidth and CPU are also important for CDN performance. Our measurements highlight the distinct impacts of various resource types on different CDN requests. Unfortunately, mainstream CDN server selection schemes only consider a single resource type and are unable to choose the most suitable servers when faced with diverse resource types. To fill this gap, we propose Polygon, a QUIC-powered CDN server selection system that is aware of multiple resource demands. Being an advanced transport layer protocol, QUIC equips Polygon with customizable transport parameters to enable the seamless handling of resource requirements in requests. Its 0-RTT and connection migration mechanisms are also utilized to minimize delays in connection and forwarding. A set of collaborative measurement probes and dispatchers are designed to support Polygon, being responsible for capturing various resource information and forwarding requests to suitable CDN servers. Real-world evaluations on the Google Cloud Platform and extensive simulations demonstrate Polygon’s ability to enhance QoE and optimize resource utilization. The results show up to a 54.8% reduction in job completion time, and resource utilization improvements of 13% in bandwidth and 7% in CPU.}
}


@article{DBLP:journals/ton/SinghSG24,
	author = {Abhiram Singh and
                  Sidharth Sharma and
                  Ashwin Gumaste},
	title = {{VERCEL:} Verification and Rectification of Configuration Errors With
                  Least Squares},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4600--4614},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3422035},
	doi = {10.1109/TNET.2024.3422035},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SinghSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present Vercel, a network verification and automatic fault rectification tool that is based on a computationally tractable, algorithmically expressive, and mathematically aesthetic domain of linear algebra. Vercel works on abstracting out packet headers into standard basis vectors that are used to create a port-specific forwarding matrix \\mathcal {A} , representing a set of packet headers/prefixes that a router forwards along a port. By equating this matrix \\mathcal {A} and a vector b (that represents the set of all headers under consideration), we are able to apply least squares (which produces a column rank agnostic solution) to compute which headers are reachable at the destination. Reachability now simply means evaluating if vector b is in the column space of \\mathcal {A} , which can efficiently be computed using least squares. Further, the use of vector representation and least squares opens new possibilities for understanding network behavior. For example, we are able to map rules, routing policies, what-if scenarios to the fundamental linear algebraic form, \\mathcal {A}x=b , as well as determine how to configure forwarding tables appropriately. We show Vercel is faster than the state-of-art such as NetPlumber, Veriflow, APKeep, AP Verifier, when measured over diverse datasets. Vercel is almost as fast as Deltanet, when rules are verified in batches and provides better scalability, expressiveness and memory efficiency. A key highlight of Vercel is that while evaluating for reachability, the tool can incorporate intents, and transform these into auto-configurable table entries, implying a recommendation/correction system.}
}


@article{DBLP:journals/ton/DaiSWPWKL24,
	author = {Jiongyu Dai and
                  Usama Saeed and
                  Ying Wang and
                  Yanjun Pan and
                  Haining Wang and
                  Kevin T. Kornegay and
                  Lingjia Liu},
	title = {Detection of Overshadowing Attack in 4G and 5G Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4615--4628},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3421371},
	doi = {10.1109/TNET.2024.3421371},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DaiSWPWKL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the promises of current and future cellular networks to increase security, privacy, and robustness, 5G networks are designed to streamline discovery and initiate connections with limited computation and communication costs, leading to the predictability of control channels. This predictability enables signal-level attacks, particularly on unprotected initial access signals. To assess vulnerability in access control and enhance robustness in cellular networks, we present a strategic approach leveraging O-RAN architecture in this paper that detects and classifies signal-level attacks for actionable countermeasure defense. We evaluate attack scenarios of various power levels on both 4G/LTE-Advanced and 5G communication systems. We categorize the types of attack models based on the attack cost: Overshadowing and Jamming. Overshadowing represents low attack power categories with time and frequency synchronization, while Jamming represents un-targeted attacks that cause similar quality-of-service degradation as overshadowing attacks but require high power levels. Our detection strategy relies on supervised machine-learning models, specifically a Reservoir Computing (RC) based supervised learning approach that leverages physical and MAC-layer information for attack detection and classification. We demonstrate the efficacy of our detection strategy through extensive experimental evaluations using the O-RAN platform with software-defined radios (SDRs) and commercial off-the-shelf (COTS) user equipment (UEs). Empirical results show that our method can classify the change in statistics caused by most overshadowing and jamming attacks with more than 95% classification accuracy.}
}


@article{DBLP:journals/ton/LiuF24,
	author = {Qingsong Liu and
                  Zhixuan Fang},
	title = {Online Task Scheduling and Termination With Throughput Constraint},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4629--4643},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3425617},
	doi = {10.1109/TNET.2024.3425617},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the task scheduling scenario where the controller activates one from K task types at each time. Each task induces a random completion time, and a reward is obtained only after the task is completed. The statistics of the completion time and the reward distributions of all task types are unknown to the controller. The controller needs to learn to schedule tasks to maximize the accumulated reward within a given time horizon T. Motivated by the practical scenarios, we require the designed policy to satisfy a system throughput constraint. In addition, we introduce the interruption mechanism to terminate ongoing tasks that last longer than certain deadlines. To address this scheduling problem, we model it as an online learning problem with deadline and throughput constraints. Then, we characterize the optimal offline policy and develop efficient online learning algorithms based on the Lyapunov method. We prove that our online learning algorithm achieves an O(\\sqrt {T}) regret and zero constraint violation. We also conduct simulations to evaluate the performance of our developed learning algorithms.}
}


@article{DBLP:journals/ton/QiaoWY24,
	author = {Yan Qiao and
                  Kui Wu and
                  Xinyu Yuan},
	title = {AutoTomo: Learning-Based Traffic Estimator Incorporating Network Tomography},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4644--4659},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3424446},
	doi = {10.1109/TNET.2024.3424446},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiaoWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating the Traffic Matrix (TM) is a critical yet resource-intensive process in network management. With the advent of deep learning models, we now have the potential to learn the inverse mapping from link loads to origin-destination (OD) flows more efficiently and accurately. However, a significant hurdle is that all current learning-based techniques necessitate a training dataset covering a comprehensive TM for a specific duration. This requirement is often unfeasible in practical scenarios. This paper addresses this complex learning challenge, specifically when dealing with incomplete and biased TM data. Our initial approach involves parameterizing the unidentified flows, thereby transforming this problem of target-deficient learning into an empirical optimization problem that integrates tomography constraints. Following this, we introduce AutoTomo, a learning-based architecture designed to optimize both the inverse mapping and the unexplored flows during the model’s training phase. We also propose an innovative observation selection algorithm, which aids network operators in gathering the most insightful measurements with limited device resources. We evaluate AutoTomo with three public traffic datasets Abilene, GÉANT and Cernet. The results reveal that AutoTomo outperforms five state-of-the-art learning-based TM estimation techniques. With complete training data, AutoTomo enhances the accuracy of the most efficient method by 15%, while it shows an improvement between 30% to 56% with incomplete training data. Furthermore, AutoTomo exhibits rapid testing speed, making it a viable tool for real-time TM estimation.}
}


@article{DBLP:journals/ton/TripathiM24,
	author = {Vishrant Tripathi and
                  Eytan H. Modiano},
	title = {Optimizing Age of Information With Correlated Sources},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4660--4675},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3427658},
	doi = {10.1109/TNET.2024.3427658},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TripathiM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We develop a simple model for the timely monitoring of correlated sources over a wireless network. Using this model, we study how to optimize weighted-sum average Age of Information (AoI) in the presence of correlation. First, we discuss how to find optimal stationary randomized policies and show that they are at-most a factor of two away from optimal policies in general. Then, we develop a Lyapunov drift-based max-weight policy that performs better than randomized policies in practice and show that it is also at-most a factor of two away from optimal. Next, we derive scaling results that show how AoI improves in large networks in the presence of correlation. We also show that for stationary randomized policies, the expression for average AoI is robust to the way in which the correlation structure is modeled. Finally, for the setting where correlation parameters are unknown and time-varying, we develop a heuristic policy that adapts its scheduling decisions by learning the correlation parameters in an online manner. We also provide numerical simulations to support our theoretical results.}
}


@article{DBLP:journals/ton/WangSWCLRXLW24,
	author = {Bo Wang and
                  Muhan Su and
                  Wufan Wang and
                  Kefan Chen and
                  Bingyang Liu and
                  Fengyuan Ren and
                  Mingwei Xu and
                  Jiangchuan Liu and
                  Jianping Wu},
	title = {Enhancing Low Latency Adaptive Live Streaming Through Precise Bandwidth
                  Prediction},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4676--4691},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3426607},
	doi = {10.1109/TNET.2024.3426607},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangSWCLRXLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To ensure high performance for HTTP adaptive streaming (HAS), it is critical to provide accurate prediction of end-to-end network bandwidth. Low Latency Live Streaming (LLLS), which has been gaining popularity, faces even greater challenges in this regard. Unlike Video-on-Demand (VOD) streaming, which only needs long-term bandwidth prediction and can tolerate some prediction errors, LLLS demands precise short-term bandwidth predictions. These challenges are amplified by the fact that short-term bandwidth experiences both large abrupt changes and uncertain fluctuations. Furthermore, obtaining valid bandwidth measurement samples in LLLS poses difficulties due to the on-off traffic pattern. In this work, we present DeeProphet, a system designed to enhance the performance of LLLS by achieving accurate bandwidth prediction. DeeProphet collects valid bandwidth samples by identifying intervals of packet continuous sending leveraging TCP state information, estimates the segment-level bandwidth robustly by filtering out noisy samples, and predicts both significant changes and uncertain fluctuations in future bandwidth by combining both time series and learning-based models. Experimental results demonstrate that DeeProphet effectively enhances the overall Quality of Experience (QoE) by 39.5% to 464.6% compared to state-of-the-art LLLS Adaptive Bitrate (ABR) algorithms.}
}


@article{DBLP:journals/ton/LiuLX24,
	author = {Jianmin Liu and
                  Dan Li and
                  Yongjun Xu},
	title = {Deep Distributional Reinforcement Learning-Based Adaptive Routing
                  With Guaranteed Delay Bounds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4692--4706},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3425652},
	doi = {10.1109/TNET.2024.3425652},
	timestamp = {Tue, 04 Feb 2025 14:47:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time applications that require timely data delivery over wireless multi-hop networks within specified deadlines are growing increasingly. Effective routing protocols that can guarantee real-time QoS are crucial, yet challenging, due to the unpredictable variations in end-to-end delay caused by unreliable wireless channels. In such conditions, the upper bound on the end-to-end delay, i.e., worst-case end-to-end delay, should be guaranteed within the deadline. However, existing routing protocols with guaranteed delay bounds cannot strictly guarantee real-time QoS because they assume that the worst-case end-to-end delay is known and ignore the impact of routing policies on the worst-case end-to-end delay determination. In this paper, we relax this assumption and propose DDRL-ARGB, an Adaptive Routing with Guaranteed delay Bounds using Deep Distributional Reinforcement Learning (DDRL). DDRL-ARGB adopts DDRL to jointly determine the worst-case end-to-end delay and learn routing policies. To accurately determine worst-case end-to-end delay, DDRL-ARGB employs a quantile regression deep Q-network to learn the end-to-end delay cumulative distribution. To guarantee real-time QoS, DDRL-ARGB optimizes routing decisions under the constraint of worst-case end-to-end delay within the deadline. To improve traffic congestion, DDRL-ARGB considers the network congestion status when making routing decisions. Extensive results show that DDRL-ARGB can accurately calculate worst-case end-to-end delay, and can strictly guarantee real-time QoS under a small tolerant violation probability against two state-of-the-art routing protocols.}
}


@article{DBLP:journals/ton/GuoNHKK24,
	author = {Daojing Guo and
                  Khaled Nakhleh and
                  I{-}Hong Hou and
                  Sastry Kompella and
                  Clement Kam},
	title = {AoI, Timely-Throughput, and Beyond: {A} Theory of Second-Order Wireless
                  Network Optimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4707--4721},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3432655},
	doi = {10.1109/TNET.2024.3432655},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoNHKK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces a new theoretical framework for optimizing second-order behaviors of wireless networks. Unlike existing techniques for network utility maximization, which only consider first-order statistics, this framework models every random process by its mean and temporal variance. The inclusion of temporal variance makes this framework well-suited for modeling Markovian fading wireless channels and emerging network performance metrics such as age-of-information (AoI) and timely-throughput. Using this framework, we sharply characterize the second-order capacity region of wireless access networks. We also propose a simple scheduling policy and prove that it can achieve every interior point in the second-order capacity region. To demonstrate the utility of this framework, we apply it to an unsolved network optimization problem where some clients wish to minimize AoI while others wish to maximize timely-throughput. We show that this framework accurately characterizes AoI and timely-throughput. Moreover, it leads to a tractable scheduling policy that outperforms other existing work.}
}


@article{DBLP:journals/ton/BistritzB24,
	author = {Ilai Bistritz and
                  Nicholas Bambos},
	title = {Power Is Knowledge: Distributed and Throughput Optimal Power Control
                  in Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4722--4734},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3444602},
	doi = {10.1109/TNET.2024.3444602},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BistritzB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consider N devices that transmit packets for T time slots, where device n uses transmission power\nP\nn\n(t)\nat time slot t. Independently at each time slot, a packet arrives at device n with probability\nλ\nn\n. The probability of successfully transmitting a packet\nμ\nn\n(P)\nis a function of the transmission powers of all devices\nP\nand the channel gains\n{\ng\nm,n\n}\nbetween them. This function is unknown to the devices that only observe binary reward\nr\nn\n(P)\nof whether the transmission was successful (ACK/NACK). All packets of device n that were not successfully transmitted yet at time slot t wait in a queue\nQ\nn\n(t)\n. The centralized max-weight scheduling (MWS) can stabilize the queues for any feasible\nλ\n(i.e., throughput optimality). However, MWS for power control is intractable even as a centralized algorithm, let alone in a distributed network. We design a distributed yet asymptotically throughput optimal power control for the wireless interference channel, which has long been recognized as a major challenge. Our main observation is that the interference\nI\nn\n(t)=∑\ng\n2\nm,n\nP\nm\n(t)\ncan be leveraged to evaluate the weighted throughput if we add a short pilot signal with power\nP\nm\n∝\nQ\nm\n(t)\nr\nm\n(P)\nafter transmitting the data. Our algorithm requires no explicit communication between the devices and learns to approximate MWS, overcoming its intractable optimization and the unknown throughput functions. We prove that, for large T, our algorithm can achieve any feasible\nλ\n. Numerical experiments show that our algorithm outperforms the state-of-the-art distributed power control, exhibiting better performance than our theoretical bounds.}
}


@article{DBLP:journals/ton/WangSXXMZH24,
	author = {Chen Wang and
                  Mingrui Sha and
                  Wei Xiong and
                  Ning Xie and
                  Rui Mao and
                  Peichang Zhang and
                  Lei Huang},
	title = {Blind Tag-Based Physical-Layer Authentication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4735--4748},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3430980},
	doi = {10.1109/TNET.2024.3430980},
	timestamp = {Sat, 25 Jan 2025 23:34:44 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangSXXMZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In comparison with upper-layer authentication mechanisms, the tag-based Physical-Layer Authentication (PLA) attracts many research interests because of high security and low complexity. This paper mainly concerns two problems in prior tag-based PLA schemes, where the first one is extra overhead and vulnerability due to the reason that the parameter is broadcasted and the other one is the problem of setting the parameter empirically. Therefore, two new tag-based PLA schemes are proposed to address the above limitations. Specifically, a blind tag-based PLA scheme (BTP) is presented to achieve accurate authentication without knowing the tag parameter of the legitimate transmitter, which not only saves the communication overhead but also improves security. Then, an adaptive blind tag-based PLA scheme (ABTP) is further proposed, which adaptively sets the tag parameter according to the wireless channel state to achieve a better balance among robustness, security, and compatibility. Rigorous theoretical analyses are provided for the two proposed schemes and the prior schemes’ performance comparisons are given. The accuracy of the theoretical analyses is verified through simulation results. At last, the advantages and disadvantages of the two proposed schemes are discussed, and suggestions are given according to different scenarios.}
}


@article{DBLP:journals/ton/HuangDLWXWD24,
	author = {Hanlin Huang and
                  Xinle Du and
                  Tong Li and
                  Haiyang Wang and
                  Ke Xu and
                  Mowei Wang and
                  Huichen Dai},
	title = {Re-Architecting Buffer Management in Lossless Ethernet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4749--4764},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3430989},
	doi = {10.1109/TNET.2024.3430989},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangDLWXWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Converged Ethernet employs Priority-based Flow Control (PFC) to provide a lossless network. However, issues caused by PFC, including victim flow, congestion spreading, and deadlock, impede its large-scale deployment in production systems. The fine-grained experimental observations on switch buffer occupancy find that the root cause of these performance problems is a mismatch of sending rates between end-to-end congestion control and hop-by-hop flow control. Resolving this mismatch requires the switch to provide an additional buffer, which is not supported by the classic dynamic threshold (DT) policy in current shared-buffer commercial switches. In this paper, we propose Selective-PFC (SPFC), a practical buffer management scheme that handles such mismatch. Specifically, SPFC incrementally modifies DT by proactively detecting port traffic and adjusting buffer allocation accordingly to trigger PFC PAUSE frames selectively. Extensive case studies demonstrate that SPFC can reduce the number of PFC PAUSEs on non-bursty ports by up to 69.0%, and reduce the average flow completion time by up to 83.5% for large victim flows.}
}


@article{DBLP:journals/ton/XieSCWLL24,
	author = {Liang Xie and
                  Zhou Su and
                  Nan Chen and
                  Yuntao Wang and
                  Yiliang Liu and
                  Ruidong Li},
	title = {A Privacy-Preserving Incentive Scheme for Data Sensing in App-Assisted
                  Mobile Edge Crowdsensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4765--4780},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3431629},
	doi = {10.1109/TNET.2024.3431629},
	timestamp = {Mon, 03 Mar 2025 22:25:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XieSCWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Application (App)-assisted mobile edge crowd- sensing is a promising paradigm, in which Apps are in charge of tagging the location of the sensing tasks as point-of-interest (PoI) to assist the platform in recruiting users to participate in the sensing tasks. However, there exist potential security, incentive, and privacy threats for App-assisted mobile edge crowdsensing (AMECS) due to the presence of malicious Apps, the low-quality shared sensing data, and the vulnerability of wireless communication. Therefore, we propose a differential privacy-based incentive (DPI) scheme for AMECS to provide secure and efficient crowdsensing services while protecting users’ privacy. Specifically, we first propose an App quality management mechanism to correlate the behavior of each App with its quality and then select reliable Apps based on quality thresholds to assist the platform in recruiting users. With the designed mechanism, we further present an auction game-based incentive mechanism to encourage Apps to mark the location of the sensing tasks as PoI. To protect the privacy of users, a privacy-preserving sensing data sharing algorithm is devised based on differential privacy. Further, given the difficulty of obtaining accurate network parameters in practice, a reinforcement learning-based incentive mechanism is designed to encourage users to participate in sensing tasks. Finally, simulation results and security analysis demonstrate that the proposed scheme can effectively improve the utilities of users, ensure the security of the crowdsensing process, and protect the privacy of users.}
}


@article{DBLP:journals/ton/KhamisKR24,
	author = {Julia Khamis and
                  Arad Kotzer and
                  Ori Rottenstreich},
	title = {Topologies for Blockchain Payment Channel Networks: Models and Constructions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4781--4797},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3445274},
	doi = {10.1109/TNET.2024.3445274},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KhamisKR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Payment channel networks (PCNs), also known as off-chain networks, implement a common approach to deal with the scalability problem of blockchain networks. They enable users to execute payments without committing them to the blockchain by relying on predefined payment channels. A pair of users can employ a payment even without a direct channel between them, by routing the payment via payment channels involving other intermediate users. Users, together with the channels, form a graph known as the off-chain network topology. The off-chain topology and the payment characteristics affect network performance such as the average number of intermediate users a payment is routed through or the values of transaction fees. In this paper, we study two basic problems in payment channel network design. First, efficiently mapping users to an off-chain topology with a known structure. Second, constructing a topology with a bounded number of channels that can serve users well with associated payments. We design algorithms for both problems while considering several fundamental topologies. We study topology-related real data statistics of Raiden, the off-chain extension for Ethereum as well as of Lightning, the equivalent off-chain layer of Bitcoin. We conduct experiments to demonstrate the effectiveness of the algorithms for these networks.}
}


@article{DBLP:journals/ton/MohantyGAP24,
	author = {Moonmoon Mohanty and
                  Gaurav Gautam and
                  Vaneet Aggarwal and
                  Parimal Parag},
	title = {Analysis of Fork-Join Scheduling on Heterogeneous Parallel Servers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4798--4809},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3432183},
	doi = {10.1109/TNET.2024.3432183},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MohantyGAP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the (k,k)\nfork-join scheduling scheme on a system of n parallel servers comprising both slow and fast servers. Tasks arriving in the system are divided into k sub-tasks and assigned to a random set of k servers, where each task can be assigned independently to a distinct slow or fast server with selection probability p_{s}\nor 1-p_{s}\n, respectively. Our analysis demonstrates that the joint distribution of the stationary workload across any set of k queues becomes asymptotically independent as the number of servers n grows, with k scaling as o\\left ({{n^{\\frac {1}{4}}}}\\right)\n. Under asymptotic independence, the limiting mean task completion time can be expressed as an integral. However, it is analytically challenging to compute the optimal selection probability p_{s}^{\\ast }\nthat minimizes this integral. To address this, we provide an upper bound on the limiting mean task completion time and identify the selection probability \\hat {p}_{s}\nthat minimizes this bound. We validate that this selection probability \\hat {p}_{s}\nyields a near-optimal performance through numerical experiments.}
}


@article{DBLP:journals/ton/ZhangCXWXG24,
	author = {Yanzhou Zhang and
                  Cailian Chen and
                  Qimin Xu and
                  Shouliang Wang and
                  Lei Xu and
                  Xinping Guan},
	title = {Scalable Scheduling for Industrial Time-Sensitive Networking: {A}
                  Hyper-Flow Graph-Based Scheme},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4810--4825},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3433599},
	doi = {10.1109/TNET.2024.3433599},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangCXWXG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Industrial Time-Sensitive Networking (TSN) provides deterministic mechanisms for real-time and reliable flow transmission. Increasing attention has been paid to efficient scheduling for time-sensitive flows with stringent requirements such as ultra-low latency and jitter. In TSN, the fine-grained traffic shaping protocol, cyclic queuing and forwarding (CQF), eliminates uncertain delay and frame loss via traffic timing in and out of queues. However, it inevitably causes high scheduling complexity. Moreover, complexity is quite sensitive to flow attributes and network scale. The problem stems in part from the lack of an attribute mining mechanism in existing frame-based scheduling. For time-critical industrial networks with large-scale complex flows, a so-called hyper-flow graph based scheduling scheme is proposed to improve the scheduling scalability in terms of schedulability, scheduling efficiency and latency & jitter. The hyper-flow graph is built by aggregating similar flow sets as hyper-flow nodes and designing a hierarchical scheduling framework. The flow attribute-sensitive scheduling information is embedded into the condensed maximal cliques, and reverse maps them precisely to congestion flow portions for re-scheduling. Its parallel scheduling reduces network scale induced complexity. Further, this scheme is designed in its entirety as a comprehensive scheduling algorithm GH2. It improves the three criteria of scalability along a Pareto front. Extensive simulation studies demonstrate its superiority. Notably, GH2 is verified its scheduling stability with a runtime of less than 100 ms for 1000 flows and near 1/190 of the SOTA FITS method for 3000 flows.}
}


@article{DBLP:journals/ton/XiongWLL24,
	author = {Junjie Xiong and
                  Mingkui Wei and
                  Zhuo Lu and
                  Yao Liu},
	title = {Warmonger Attack: {A} Novel Attack Vector in Serverless Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4826--4841},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3437432},
	doi = {10.1109/TNET.2024.3437432},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiongWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We debut the Warmonger attack, a novel attack vector that can cause denial-of-service between a serverless computing platform and an external content server. The Warmonger attack exploits the fact that a serverless computing platform shares the same set of egress IPs among all serverless functions, which belong to different users, to access an external content server. As a result, a malicious user on this platform can purposefully misbehave and cause these egress IPs to be blocked by the content server, resulting in a platform-wide denial of service. To validate the effectiveness of the Warmonger attack, we conducted extensive experiments over several months, collecting and analyzing the egress IP usage patterns of five prominent serverless service providers (SSPs): Amazon Web Service (AWS) Lambda, Google App Engine, Microsoft Azure Functions, Cloudflare Workers, and Alibaba Function Compute. Additionally, we conducted a thorough evaluation of the attacker’s potential actions to compromise an external server and trigger IP blocking. Our findings revealed that certain SSPs employ surprisingly small sets of egress IPs, sometimes as few as four, which are shared among their user base. Furthermore, our research demonstrates that the serverless platform offers ample opportunities for malicious users to engage in well-known disruptive behaviors, ultimately resulting in IP blocking. Our study uncovers a significant security threat within the burgeoning serverless computing platform and sheds light on potential mitigation strategies, such as the detection of malicious serverless functions and the isolation of such entities.}
}


@article{DBLP:journals/ton/ZhangFSWXL24,
	author = {Chuwen Zhang and
                  Yong Feng and
                  Haoyu Song and
                  Ying Wan and
                  Wenquan Xu and
                  Bin Liu},
	title = {{OBMA:} Scalable Route Lookups With Fast and Zero-Interrupt Updates},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4842--4854},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3446689},
	doi = {10.1109/TNET.2024.3446689},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangFSWXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-based IP route lookup is a key component for packet forwarding in Software Defined Networks. Running lookup algorithms on commodity CPUs is flexible and scalable, which shows advantages on cost and power consumption over the hardware-based forwarding engines. However, dynamic network functions and services make route updates more frequent than ever. Existing algorithms often fall short of the incremental update requirements. In this paper, we propose the Overlay BitMap Algorithm (OBMA), which contains several variations, to support extraordinary update performance while maintaining the highest-in-class lookup speed and storage efficiency. Starting from the basic OBMA_B, we develop two variations with different tradeoffs for different application scenarios. OBMA_L supports faster lookups than OBMA_B at a small cost of update speed. OBMA_S achieves better storage efficiency than OBMA_B at a small cost of lookup throughput. We run our algorithms on a commodity CPU and evaluate them with real-world route tables and traces. The experiments show that OBMA achieves the lowest memory footprint, the highest update speed, and over 200 Mpps lookup throughput. Specifically, OBMA_S reduces the memory footprint to 3.98 bytes/prefix which is 25.33% smaller that of the state-of-the-art Poptrie; OBMA_L supports 252.02 Mpps lookup throughput with a single thread, and more than 600 Mpps with multiple parallel threads in a single CPU, significantly outperforming the state-of-the-art Poptrie and SAIL; OBMA_B supports updates at a rate of 14.58M updates/s which is 15 times faster than Poptrie. The tests show that the update process has little interference with the lookup process for OBMA, and achieves zero-interrupt to lookups with multiple threads.}
}


@article{DBLP:journals/ton/HuangP24,
	author = {Zhiming Huang and
                  Jianping Pan},
	title = {Game-Theoretic Bandits for Network Optimization With High-Probability
                  Swap-Regret Upper Bounds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4855--4870},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3444593},
	doi = {10.1109/TNET.2024.3444593},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study a multi-agent bandit problem in an unknown general-sum game repeated for a number of rounds (i.e., learning in a black-box game with bandit feedback), where a set of agents have no information about the underlying game structure and cannot observe each other’s actions and rewards. In each round, each agent needs to play an arm (i.e., action) from a (possibly different) arm set (i.e., action set), and only receives the reward of the played arm that is affected by other agents’ actions. The objective of each agent is to minimize her own cumulative swap regret, where the swap regret is a generic performance measure for online learning algorithms. Many network optimization problems can be cast with the framework of this multi-agent bandit problem, such as wireless medium access control and end-to-end congestion control. We propose an online-mirror-descent-based algorithm and provide near-optimal high-probability swap-regret upper bounds based on refined martingale analyses, which can further bound the expected swap regret instead of the pseudo-regret studied in the literature. Moreover, the high-probability bounds guarantee that correlated equilibria can be achieved in a polynomial number of rounds if the algorithms are played by all agents. To assess the performance of the studied algorithm, we conducted numerical experiments in the context of wireless medium access control, and we performed emulation experiments by implementing the studied algorithms through the Linux Kernel for the end-to-end congestion control.}
}


@article{DBLP:journals/ton/DouQWG24,
	author = {Songshi Dou and
                  Li Qi and
                  Jianye Wang and
                  Zehua Guo},
	title = {{EPIC:} Traffic Engineering-Centric Path Programmability Recovery
                  Under Controller Failures in SD-WANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4871--4884},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3438292},
	doi = {10.1109/TNET.2024.3438292},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DouQWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Wide Area Networks (SD-WANs) offer a promising opportunity to enhance the performance of Traffic Engineering (TE). With the help of Software-Defined Networking (SDN), TE can promptly respond to traffic changes and maintain network performance by leveraging a global network view. One of the key benefits of SDN for TE is path programmability, which is empowered by SDN controllers to enable dynamic adjustments of flows’ forwarding paths. However, controller failures pose new challenges for SD-WANs since path programmability could be decreased due to the increasing number of offline flows, leading to potential TE performance degradation. Existing recovery solutions mainly focus on recovering path programmability for improving unpredictable network performance but cannot guarantee consistently satisfactory TE performance as expected, since path programmability can only indirectly evaluate network performance. In this paper, we propose EPIC to ensure robust TE performance under controller failures. We observe that frequently rerouted flows could greatly influence TE performance. Enlightened by this, EPIC introduces a novel metric called the TE performance-centric ratio to assess the relevance of different path programmability values for TE performance. The key idea of EPIC lies in identifying frequently rerouted flows during TE operations and prioritizing recovery of the path programmability of these flows under controller failures. We formulate an optimization problem to maximize TE performance-centric path programmability and propose an efficient heuristic algorithm to solve this problem. Evaluation results demonstrate that EPIC can improve average load balancing performance by up to 55.6% compared with baselines.}
}


@article{DBLP:journals/ton/LinWLWZ24,
	author = {Rongping Lin and
                  Fan Wang and
                  Shan Luo and
                  Xiong Wang and
                  Moshe Zukerman},
	title = {Time-Efficient Blockchain-Based Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4885--4900},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3436862},
	doi = {10.1109/TNET.2024.3436862},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinWLWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a distributed machine learning method that ensures the privacy and security of participants’ data by avoiding direct data upload to a central node for training. However, the traditional FL typically applies a star structure with cloud servers as the central aggregator for the model parameters from different terminals, leading to problems such as central failure, malicious tampering and malicious participants, resulting in training errors or system crashes. To address these issues, a permissioned blockchain is used to build a secure and reliable data-sharing platform among participating terminals, replacing the central aggregator in the traditional FL called blockchain-based federated learning. However, the block generation method of the blockchain system may introduce significant latency in the federated learning where distributed model parameters upload randomly, resulting in low efficiency of the federated learning. To overcome this, we propose a block generation strategy that groups terminals and generates a block for each group, which minimizes the latency of a single round of federated learning, and an optimal block generation algorithm that considers data distribution, terminal resources, and network resources is provided. The analysis shows that the proposed algorithm can effectively obtain the optimal solution of block generation to minimize the authentication time, and we conduct extensive experiments that demonstrate the time efficiency of the proposed algorithm.}
}


@article{DBLP:journals/ton/EsmatXWLG24,
	author = {Haitham H. Esmat and
                  Xiaohao Xia and
                  Yinxuan Wu and
                  Beatriz Lorenzo and
                  Linke Guo},
	title = {Cross-Technology Federated Matching for Age of Information Minimization
                  in Heterogeneous IoT},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4901--4916},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3436712},
	doi = {10.1109/TNET.2024.3436712},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/EsmatXWLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous Internet of Things (IoT) networks, which operate using various protocols and spectrum bands like WiFi, Bluetooth, Zigbee, and LoRa, bring many opportunities to collaborate and achieve timely data collection. However, several challenges must be addressed due to heterogeneous data patterns, coverage, spectrum bands, and mobility. This paper introduces a cross-technology IoT network architecture design that facilitates collaboration between service providers (SPs) to share their spectrum bands and offload computing tasks from heterogeneous IoT devices using multi-protocol mobile gateways (M-MGs). The objective is to minimize the age of information (AoI) and energy consumption by jointly optimizing collaboration between M-MGs and SPs for bandwidth allocation, relaying, and cross-technology data scheduling. A pricing mechanism is presented to incentivize different levels of collaboration and matching between M-MGs and SPs. Given the uncertainty due to mobility and task requests, we design a cross-technology federated matching algorithm (CT-Fed-Match) based on a multi-agent actor-critic approach in which M-MGs and SPs learn their strategies in a distributed manner. Furthermore, we incorporate federated learning to enhance the convergence of the learning process. The numerical results demonstrate that our CT-Fed-Match-RC algorithm with cross-technology and relaying collaboration reduces the AoI by 30 times and collects 8 times more packets than existing approaches.}
}


@article{DBLP:journals/ton/LiHLLZZJW24,
	author = {Yijun Li and
                  Jiawei Huang and
                  Zhaoyi Li and
                  Jingling Liu and
                  Shengwen Zhou and
                  Tao Zhang and
                  Wanchun Jiang and
                  Jianxin Wang},
	title = {Straggler-Aware Gradient Aggregation for Large-Scale Distributed Deep
                  Learning System},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4917--4930},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3441039},
	doi = {10.1109/TNET.2024.3441039},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiHLLZZJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Neural Network (DNN) is a critical component of a wide range of applications. However, with the rapid growth of the training dataset and model size, communication becomes the bottleneck, resulting in low utilization of computing resources. To accelerate communication, recent works propose to aggregate gradients from multiple workers in the programmable switch to reduce the volume of exchanged data. Unfortunately, since using synchronization transmission to aggregate data, current in-network aggregation designs suffer from the straggler problem, which often occurs in shared clusters due to resource contention. To address this issue, we propose a straggler-aware aggregation transport protocol (SA-ATP), which enables the leading worker to leverage the spare computing and storage resources to help the straggling worker. We implement SA-ATP atop clusters using P4-programmable switches. The evaluation results show that SA-ATP reduces the iteration time by up to 57% and accelerates training by up to 1.8\\times\nin real-world benchmark models.}
}


@article{DBLP:journals/ton/QiaoSCZLCY24,
	author = {Jing Qiao and
                  Shikun Shen and
                  Shuzhen Chen and
                  Xiao Zhang and
                  Tian Lan and
                  Xiuzhen Cheng and
                  Dongxiao Yu},
	title = {De-RPOTA: Decentralized Learning With Resource Adaptation and Privacy
                  Preservation Through Over-the-Air Computation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4931--4943},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3438462},
	doi = {10.1109/TNET.2024.3438462},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiaoSCZLCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose De-RPOTA, a novel algorithm designed for decentralized learning, equipped with mechanisms for resource adaptation and privacy protection through over-the-air computation. We theoretically analyze the combined effects of limited resources and lossy communication on decentralized learning, showing it converges towards a contraction region defined by a scaled errors version. Remarkably, De-RPOTA achieves a convergence rate of \\mathcal {O}\\left ({{\\frac {1}{\\sqrt {nT}}}}\\right) in scenarios devoid of errors, matching the state-of-the-arts. Additionally, we tackle a power control challenge, breaking it down into transmitter and receiver sub-problems to hasten the De-RPOTA algorithm’s convergence. We also offer a quantifiable privacy assurance for our over-the-air computation methodology. Intriguingly, our findings suggest that network noise can actually strengthen the privacy of aggregated information, with over-the-air computation providing extra security for individual updates. Comprehensive experimental validation confirms De-RPOTA’s efficacy in communication resources limited environments. Specifically, the results on the CIFAR-10 dataset reveal nearly 30% reduction in communication costs compared to the state-of-the-arts, all while maintaining similar levels of learning accuracy, even under resource restrictions.}
}


@article{DBLP:journals/ton/YangLDRWWZ24,
	author = {Wei Yang and
                  Chi Lin and
                  Haipeng Dai and
                  Jiankang Ren and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Precise Wireless Charging in Complicated Environments},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4944--4959},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3441113},
	doi = {10.1109/TNET.2024.3441113},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangLDRWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Rechargeable Sensor Networks (WRSNs) have become an important research issue as they can overcome the energy bottleneck problem of wireless sensor networks. However, inaccurate discretization methods and imprecise charging models yield a huge gap between theoretical results and practical applications, making it difficult for wide adoptions. In this paper, we focus on designing a precise charging method for maximizing charging utility when line-of-sight (LOS) and none-line-of-sight (NLOS) charging cases exist in complicated environments. First, we design discretization methods for charging area and charging orientation for precisely constructing the charging model. Then, we develop a novel electromagnetic wave reflection model to describe the signal propagation model in the presence of obstacles. We formalize the mobile charging problem into a submodular function maximization problem which can be solved by a proposed algorithm with an approximation guarantee. Finally, extensive experiments and simulations demonstrate that our schemes outperform comparison algorithms by 32.5% on average in charging utility in complicated environments.}
}


@article{DBLP:journals/ton/HuangXYWCWHHWGTDCY24,
	author = {Chengyuan Huang and
                  Feiyang Xue and
                  Peiwen Yu and
                  Xiaoliang Wang and
                  Yanqing Chen and
                  Tao Wu and
                  Lei Han and
                  Zifa Han and
                  Bingquan Wang and
                  Xiangyu Gong and
                  Chen Tian and
                  Wanchun Dou and
                  Guihai Chen and
                  Hao Yin},
	title = {Minimizing Buffer Utilization for Lossless Inter-DC Links},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4960--4975},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3443600},
	doi = {10.1109/TNET.2024.3443600},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangXYWCWHHWGTDCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RDMA over Converged Ethernet (RoCEv2) has been widely deployed to data centers (DCs) for its better compatibility with Ethernet/IP than Infiniband (IB). As cross-DC applications emerge, they also demand high throughput, low latency, and lossless network for cross-DC data transmission. However, RoCEv2’s underlying lossless mechanism Priority-based Flow Control (PFC) cannot fit into the long-haul transmission scenario and degrades the performance of RoCEv2. PFC is myopic and only considers queue length to pause upstream senders, which leads to large queueing delay. This paper proposes Bifrost, a downstream-driven lossless flow control that supports long distance cross-DC data transmission. Bifrost uses virtual incoming packets, which indicates the upper bound of in-flight packets, together with buffered packets to control the flow rate. It minimizes the buffer space requirement to one-hop bandwidth delay product (BDP) and achieves low one-way latency. Moreover, we extend Bifrost and propose BifrostX, to accommodate the multi-priority queue of the current switch implementation. BifrostX enables flow control for each queue separately while maintaining low buffer reservation, no throughput loss, and no packet loss. Real-world experiments are conducted with prototype switches and 80 kilometers cables. Evaluations demonstrate that compared to PFC, Bifrost reduces average/tail flow completion time (FCT) of inter-DC flows by up to 22.5%/42.0%, respectively. Bifrost is compatible with existing infrastructure and can support distance of thousands of kilometers.}
}


@article{DBLP:journals/ton/HeYFZ24,
	author = {Qiuye He and
                  Edwin Yang and
                  Song Fang and
                  Shangqing Zhao},
	title = {Revisiting Wireless Breath and Crowd Inference Attacks With Defensive
                  Deception},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4976--4988},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3453903},
	doi = {10.1109/TNET.2024.3453903},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeYFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Breathing rates and crowd counting can be used to verify the human presence, especially the former one can disclose a person’s physiological status. Many studies have demonstrated success in applying channel state information (CSI) to estimate the breathing rates of stationary individuals and count the number of people in motion. Due to the invisibility of radio signals, the ubiquitous deployment of wireless infrastructures, and the elimination of the line-of-sight (LOS) requirement, such wireless inference techniques can surreptitiously work and violate user privacy. However, little research has been conducted specifically in mitigating misuse of those techniques. This paper proposes new proactive countermeasures against all existing CSI-based vital signs and crowd counting inference methods. Specifically, we set up ambush locations with carefully designed wireless signals, allowing eavesdroppers to infer a false breathing rate or person count specified by the transmitter. The true breathing rate or person count is thus protected. Experimental results on software-defined radio platforms with 5 participants demonstrate the effectiveness of the proposed defenses. An eavesdropper can be misled into believing any desired breathing rate with an error of less than 1.2 bpm when the user lies on a bed in a bedroom, and 0.9 bpm when the user sits in a chair in an office room. Additionally, our proposed defense mechanisms can deceive an attacker into believing there are moving individuals in an empty room with a 100% success rate, using both Support Vector Machine (SVM) and Decision Tree (DT) classifiers.}
}


@article{DBLP:journals/ton/GargariOPSZA24,
	author = {Amir Ashtari Gargari and
                  Andrea Ortiz and
                  Matteo Pagin and
                  Wanja de Sombre and
                  Michele Zorzi and
                  Arash Asadi},
	title = {Risk-Averse Learning for Reliable mmWave Self-Backhauling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {4989--5003},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3452953},
	doi = {10.1109/TNET.2024.3452953},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GargariOPSZA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless backhauling at millimeter-wave frequencies (mmWave) in static scenarios is a well-established practice in cellular networks. However, highly directional and adaptive beamforming in today’s mmWave systems have opened new possibilities for self-backhauling. Tapping into this potential, 3GPP has standardized Integrated Access and Backhaul (IAB) allowing the same base station to serve both access and backhaul traffic. Although much more cost-effective and flexible, resource allocation and path selection in IAB mmWave networks is a formidable task. To date, prior works have addressed this challenge through a plethora of classic optimization and learning methods, generally optimizing Key Performance Indicators (KPIs) such as throughput, latency, and fairness, and little attention has been paid to the reliability of the KPI. We propose Safehaul, a risk-averse learning-based solution for IAB mmWave networks. In addition to optimizing the average performance, Safehaul ensures reliability by minimizing the losses in the tail of the performance distribution. We develop a novel simulator and show via extensive simulations that Safehaul not only reduces the latency by up to 43.2% compared to the benchmarks, but also exhibits significantly more reliable performance, e.g., 71.4% less variance in latency.}
}


@article{DBLP:journals/ton/FanWLGLLLZYCC24,
	author = {Zhuochen Fan and
                  Xiangyuan Wang and
                  Xiaodong Li and
                  Jiarui Guo and
                  Wenrui Liu and
                  Haoyu Li and
                  Sheng Long and
                  Zheng Zhong and
                  Tong Yang and
                  Xuebin Chen and
                  Bin Cui},
	title = {SteadySketch: {A} High-Performance Algorithm for Finding Steady Flows
                  in Data Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5004--5019},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3444488},
	doi = {10.1109/TNET.2024.3444488},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FanWLGLLLZYCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study steady flows in data streams, which refers to the flows whose arrival rate is always non-zero and around a fixed value for several consecutive time windows. To find steady flows in real time, we propose a novel sketch-based algorithm, SteadySketch, aiming to accurately report steady flows with limited memory. To the best of our knowledge, this is the first work to define and find steady flows in data streams. The key novelty of SteadySketch is our proposed reborn technique, which reduces the memory requirement by 75%. Our theoretical proofs show that the negative impact of the reborn technique is small. Experimental results show that, compared with the two comparison schemes, SteadySketch improves the Precision Rate (PR) by around 79.5% and 82.8%, and reduces the Average Relative Error (ARE) by around 905.9\\times and 657.9\\times , respectively. Finally, we provide three concrete cases: cache prefetch, Redis and P4 implementation. As we will demonstrate, SteadySketch can effectively improve the cache hit ratio while achieving satisfying performance on both Redis and Tofino switches. All related codes of SteadySketch are available at GitHub.}
}


@article{DBLP:journals/ton/GuoHZLS24,
	author = {Xiuzhen Guo and
                  Yuan He and
                  Jiacheng Zhang and
                  Yunhao Liu and
                  Longfei Shangguan},
	title = {Towards Programmable Backscatter Radio Design for Heterogeneous Wireless
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5020--5032},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3454095},
	doi = {10.1109/TNET.2024.3454095},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoHZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents RF-Transformer, a unified backscatter radio hardware abstraction that allows a low-power IoT device to directly communicate with heterogeneous wireless receivers. Unlike existing backscatter systems that are tailored to a specific wireless communication protocol, RF-Transformer provides a programmable interface to the micro-controller, allowing IoT devices to synthesize different types of protocol-compliant backscatter signals in the PHY layer. By leveraging the nonlinear characteristics of the negative impedance, RF-Transformer also achieves a cross-frequency backscatter design that enables IoT devices in harmonic frequency bands to communicate with each other. We implement a PCB prototype of RF-Transformer on 2.4 GHz ISM band and conduct extensive experiments. We leverage the software defined platform USRP to transmit the carrier signal and receive the backscatter signal to verify the efficacy of our design. Our extensive field studies show that RF-Transformer achieves 23.8 Mbps, 247.1 Kbps, 986.5 Kbps, and 27.3 Kbps throughput when generating standard Wi-Fi, ZigBee, Bluetooth, and LoRa signals.}
}


@article{DBLP:journals/ton/GuLYWZLX24,
	author = {Huayue Gu and
                  Zhouyu Li and
                  Ruozhou Yu and
                  Xiaojian Wang and
                  Fangtong Zhou and
                  Jianqing Liu and
                  Guoliang Xue},
	title = {{FENDI:} Toward High-Fidelity Entanglement Distribution in the Quantum
                  Internet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5033--5048},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3450271},
	doi = {10.1109/TNET.2024.3450271},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuLYWZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A quantum network distributes quantum entanglements between remote nodes, and is key to many applications in secure communication, quantum sensing and distributed quantum computing. This paper explores the fundamental trade-off between the throughput and the quality of entanglement distribution in a multi-hop quantum repeater network. Compared to existing work which aims to heuristically maximize the entanglement distribution rate (EDR) and/or entanglement fidelity, our goal is to characterize the maximum achievable worst-case fidelity, while satisfying a bound on the maximum achievable expected EDR between an arbitrary pair of quantum nodes. This characterization will provide fundamental bounds on the achievable performance region of a quantum network, which can assist with the design of quantum network topology, protocols and applications. However, the task is highly non-trivial and is NP-hard as we shall prove. Our main contribution is a fully polynomial-time approximation scheme to approximate the achievable worst-case fidelity subject to a strict expected EDR bound, combining an optimal fidelity-agnostic EDR-maximizing formulation and a worst-case isotropic noise model. The EDR and fidelity guarantees can be implemented by a post-selection-and-storage protocol with quantum memories. By developing a discrete-time quantum network simulator, we conduct simulations to show the characterized performance region (the approximate Pareto frontier) of a network, and demonstrate that the designed protocol can achieve the performance region while existing protocols exhibit a substantial gap.}
}


@article{DBLP:journals/ton/YangCXHLLSL24,
	author = {Jiayu Yang and
                  Yuxin Chen and
                  Kaiping Xue and
                  Jiangping Han and
                  Jian Li and
                  Ruidong Li and
                  Qibin Sun and
                  Jun Lu},
	title = {Adaptive Multi-Source Multi-Path Congestion Control for Named Data
                  Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5049--5064},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3447467},
	doi = {10.1109/TNET.2024.3447467},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangCXHLLSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named Data Networking (NDN), with a receiver-driven connectionless communication paradigm, naturally supports content delivery from multiple sources via multiple paths. In a dynamic environment, sources and paths may change unexpectedly and are uncontrollable for consumer, which requires flexible rate control and real-time multi-path management, still lacking investigations. To address this issue, we propose an Adaptive Multi-source Multi-path Congestion Control (AMM-CC) scheme based on online learning. AMM-CC explores source/path distribution with continuous micro-experiments and abstracts the empirically experienced performance by meticulously designed two-level utility functions. Specifically, AMM-CC enables each consumer to optimize a local transmission-level utility function that fuses multi-source characteristics, including congestion level and source weights. Then, a sub-gradient descent method is designed to adjust transmission rate adaptively and achieve fine-grained control. Moreover, AMM-CC coordinates consumer with the forwarding module to ensure efficient and on-time multi-path management. It enables consumer to determine congestion gap among multiple paths by a path-level utility that sensitively captures changes and congestion on each path. Then, consumer further notifies the forwarding module in achieving precise traffic transferring. We conducted comprehensive evaluations in dynamic scenario with various content distribution using the NDN simulator, ndnSIM. The evaluation results demonstrate that AMM-CC can adapt to flexible content acquisition from multi-sources and significantly improve bandwidth utilization of multi-path compared with state-of-the-art schemes.}
}


@article{DBLP:journals/ton/LiLQWZLMWLG24,
	author = {Jianfeng Li and
                  Zheng Lin and
                  Jian Qu and
                  Shuohan Wu and
                  Hao Zhou and
                  Yangyang Liu and
                  Xiaobo Ma and
                  Ting Wang and
                  Xiapu Luo and
                  Xiaohong Guan},
	title = {Robust App Fingerprinting Over the Air},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5065--5080},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3448621},
	doi = {10.1109/TNET.2024.3448621},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLQWZLMWLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile apps have significantly transformed various aspects of modern life, leading to growing concerns about privacy risks. Despite widespread encrypted communication, app fingerprinting (AF) attacks threaten user privacy substantially. However, existing AF attacks, when targeted at wireless traffic, face four fundamental challenges, namely 1) sample inseparability; 2) app multiplexing; 3) signal attenuation; and 4) open-world recognition. In this paper, we advance a novel AF attack, dubbed PacketPrint, to recognize app user activities over the air in an open-world setting. We introduce two novel models, i.e., sequential XGBoost and hierarchical bag-of-words model, to tackle sample inseparability and enhance robustness against noise packets arising from app multiplexing. We also propose the environment-aware model enhancement to bolster PacketPrint’s robustness in handling packet loss at the sniffer caused by signal attenuation. We conduct extensive experiments to evaluate the proposed attack in a series of challenging scenarios, including 1) open-world setting; 2) simultaneous use of different apps; 3) severe packet loss at the sniffer; and 4) cross-dataset recognition. The experimental results show that PacketPrint can accurately recognize app user activities. It achieves the average F1-score 0.947 for open-world app recognition and the average F1-score 0.959 for in-app user action recognition.}
}


@article{DBLP:journals/ton/XuLZ24,
	author = {Zichen Xu and
                  Ziye Lu and
                  Zuqing Zhu},
	title = {Information-Sensitive In-Band Network Telemetry in P4-Based Programmable
                  Data Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5081--5096},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3448244},
	doi = {10.1109/TNET.2024.3448244},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of programmable data plane (PDP), in-band network telemetry (INT) has become a promising network monitoring technique to visualize network operations in a fine-grained and real-time way. In this work, to better balance the tradeoff between INT overheads and monitoring accuracy, we design and optimize an information-sensitive INT system (namely, P4InfoSen-INT), which makes each PDP switch decide locally whether and what type(s) of telemetry data should be inserted in a packet based on the “information content” of the data, and implement it in P4-based PDP switches. We first realize the basic principle of P4InfoSen-INT with P4 programs. Then, we propose algorithms to estimate the information content of telemetry data accurately in a dynamic network and optimize the tradeoff between INT overheads and monitoring accuracy. Finally, we further optimize the implementation of P4InfoSen-INT by proposing table merging to reduce stage occupation in each switch. Experimental results verify that our proposed P4InfoSen-INT can balance the tradeoff between INT overheads and monitoring accuracy better than existing benchmarks.}
}


@article{DBLP:journals/ton/LiLPZWC24,
	author = {Fuliang Li and
                  Minglong Li and
                  Yunhang Pu and
                  Yuxin Zhang and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {{XNV:} Explainable Network Verification},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5097--5111},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3456124},
	doi = {10.1109/TNET.2024.3456124},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLPZWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network verification has recently made strides, focusing on the satisfiability of configurations and policies or the performance and versatility of their methods. However, they generally ignore explainability, which is the ability to explain why a network violates or satisfies a certain forwarding policy. In this paper, we propose an explainable network verification framework XNV, which uses a novel interpretable fault analysis method to construct an effective explainable network verifier using knowledge graph (KG). XNV provides appropriate explanations to help operators understand the verification results, improving the transparency and trustworthiness of the verification system. First, XNV uses the KG as an intermediate representation of the configuration semantic level, storing the configuration semantics and routing protocol states. Then, XNV constructs human-logical fault trees for policies and implements root-cause analysis of policy violations based on KG queries and minimum cut set matching. Experiments and case evaluations show that our system provides good interpretability while balancing performance, accelerated understanding, and handling of misconfigurations.}
}


@article{DBLP:journals/ton/ShiYWHSHZL24,
	author = {Jianzhi Shi and
                  Bo Yi and
                  Xing{-}Wei Wang and
                  Min Huang and
                  Yang Song and
                  Qiang He and
                  Chao Zeng and
                  Keqin Li},
	title = {JointCloud Resource Market Competition: {A} Game-Theoretic Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5112--5127},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3450098},
	doi = {10.1109/TNET.2024.3450098},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiYWHSHZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The current global economy is undergoing a transformative phase, emphasizing collaboration among multiple competing entities rather than monopolization. Economic globalization is accelerating the adoption of globalized cloud services, and in line with this trend, cloud 2.0 introduces the concept of “cloud cooperation”. JointCloud, as a novel computing model for Cloud 2.0, advocates for the establishment of an evolving cloud ecosystem. However, a critical challenge arises due to the lack of direct incentives for a cloud to join the JointCloud ecosystem, leading to uncertainty regarding the rationale for the existence of the JointCloud ecosystem. To address this ambiguity, we draw inspiration from supply chain competition and formulate the market dynamics of resources within the JointCloud ecosystem. Our focus is particularly on the analysis of data resource trade within the JointCloud market. To comprehensively analyze the JointCloud market, we propose a market game that examines the competition among clouds within the ecosystem. We theoretically prove that a Nash Equilibrium always exists under the JointCloud market. Subsequently, we conduct an in-depth analysis of the profits of cloud resource manufacturers and cloud resource retailers as the number of clouds varies within the JointCloud ecosystem. Based on our analysis, we further explore the incentives for a cloud to participate in the JointCloud ecosystem. We then evaluate the performance of the proposed market game through extensive experiments, illustrating how process variables and profits change with the market size. The experiments demonstrate that the trends of various variables are aligned with our analysis obtained from the market game. Compared with the Cournot model, our proposed model captures the market power of both manufacturers and retailers, resulting in a model that closely mirrors real market dynamics. Our findings provide valuable insights into the cloud market within Cloud 2.0, offering guidance for stakeholders navigating the evolving landscape of cloud cooperation and competition.}
}


@article{DBLP:journals/ton/DingYXHLSL24,
	author = {Kunpeng Ding and
                  Jiayu Yang and
                  Kaiping Xue and
                  Jiangping Han and
                  Jian Li and
                  Qibin Sun and
                  Jun Lu},
	title = {{SLP:} {A} Secure and Lightweight Scheme Against Content Poisoning
                  Attacks in Named Data Networking Based on Probing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5128--5143},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3451231},
	doi = {10.1109/TNET.2024.3451231},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DingYXHLSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named Data Networking (NDN) stands out as a promising Information Centric Networking architecture capable of facilitating large-scale content distribution through in-network caching and location-independent data access. However, attackers can easily inject poisoned content into the network, called content poisoning attacks, which leads to a substantial deterioration in user experience and transmission efficiency. In existing schemes, routers fail to determine the contamination source of received poisoned content, leading to the inability to accurately identify attacker nodes. Besides, attackers’ dynamic behaviors and network instability could disrupt identification results. In this paper, we propose a Secure and Lightweight scheme against content poisoning attacks based on Probing (SLP), where a proactive and reliable probing protocol is designed to identify adversaries quickly and precisely. In SLP, a router sends specifically chosen interest packets to probe a suspicious node, so that the returned corresponding content can straightly reflect its trustworthiness without other nodes’ interference. In addition, a hypothesis testing algorithm is developed to analyze the returned content, which can exclude the impact of transmission errors and adapt to dynamic attackers. Moreover, we utilize users’ feedback to avoid unnecessary probing costs on unaffected routers, with its reliability guaranteed by an efficient cuckoo-filter-based feedback validation mechanism. Security analysis shows that SLP achieves resistance against content poisoning attacks and malicious feedback. The experimental results demonstrate that SLP makes users hardly be affected by attacks and brings in only slight overhead.}
}


@article{DBLP:journals/ton/TripathiM24a,
	author = {Vishrant Tripathi and
                  Eytan H. Modiano},
	title = {A Whittle Index Approach to Minimizing Functions of Age of Information},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5144--5158},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3452006},
	doi = {10.1109/TNET.2024.3452006},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TripathiM24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a setting where multiple active sources send real-time updates over a single-hop wireless broadcast network to a monitoring station. Our goal is to design a scheduling policy that minimizes the time-average of general non-decreasing cost functions of Age of Information. We use a Whittle index based approach to find low complexity scheduling policies that have good performance. We prove that for a system with two sources, having possibly different cost functions and reliable channels, the Whittle index policy is exactly optimal. We derive structural properties of an optimal policy, that suggest that the performance of the Whittle index policy may be close to optimal in general. These results might also be of independent interest in the study of restless multi-armed bandit problems with similar underlying structure. We further establish that minimizing monitoring error for linear time-invariant systems and symmetric Markov chains is equivalent to minimizing appropriately chosen monotone functions of Age of Information. Finally, we provide simulations comparing the Whittle index policy with optimal scheduling policies found using dynamic programming, which support our results.}
}


@article{DBLP:journals/ton/ZhuW24,
	author = {Juan Zhu and
                  Shaowei Wang},
	title = {QoS-Guaranteed Resource Allocation in Mobile Communications: {A} Stochastic
                  Network Calculus Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5159--5171},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3458922},
	doi = {10.1109/TNET.2024.3458922},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deterministic mobile networks are essential for advanced applications that demand strict quality of service (QoS) assurances under limited resource availability. Though network slicing can optimize average performance metrics to offer best-effort services, it often fails to meet the high-reliability requirements of deterministic communication scenarios. In this paper, we introduce a novel QoS-guaranteed inter-slice radio resource allocation scheme for mobile networks to deliver deterministic services over the long term. First, we develop an analytical martingale-based stochastic network calculus framework, which yields stochastic bounds for transmission delays and queue backlogs across various traffic arrival patterns. These bounds produce robust interval estimations that guide resource allocation decisions, effectively addressing channel variability and long-tail QoS effects. Then, an efficient resource allocation algorithm is proposed to approach the derived performance bounds while ensuring fairness across different radio slices with diverse QoS needs. The framework also incorporates an adaptive traffic predictor, enabling our algorithm to track and respond to network dynamics. Numerical results demonstrate that our proposed scheme achieves a promising trade-off between resource utilization and QoS guarantees.}
}


@article{DBLP:journals/ton/JiaFZZL24,
	author = {Riheng Jia and
                  Qiyong Fu and
                  Zhonglong Zheng and
                  Guanglin Zhang and
                  Minglu Li},
	title = {Energy and Time Trade-Off Optimization for Multi-UAV Enabled Data
                  Collection of IoT Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5172--5187},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3450489},
	doi = {10.1109/TNET.2024.3450489},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JiaFZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we study the problem of dispatching multiple unmanned aerial vehicles (UAVs) for data collection in internet of things (IoT), where each UAV departs from its start point, visits some IoT devices for data collection and returns to its destination point. Considering the UAV’s limited onboard energy and the time required to collect data from all IoT devices, it is essential to appropriately assign the data collection task for each UAV, such that none of the dispatched UAVs consumes excessive energy and the maximum task completion time among all UAVs is minimized. To optimize those two conflicting objectives, we focus on minimizing the maximum task completion time and the maximum energy consumption among all UAVs, by jointly designing the flight trajectory, hovering positions for data collection and flight speed of each UAV. We formulate this problem as a multi-objective optimization problem with the aim of obtaining a set of Pareto-optimal solutions in terms of time or energy dominance. Due to the NP-hardness and complexity of the formulated problem, we propose a multi-strategy multi-objective ant colony optimization algorithm (MSMOACO), which is developed based on a constrained ant colony optimization algorithm with a fitnessguided mutation strategy and an adaptive hovering strategy being delicately incorporated, to solve the problem. To accommodate the practical scenario, we also design a novel geometry-based collision avoidance strategy to reduce the possibility of collisions among UAVs. Extensive evaluations validate the effectiveness and superiority of the proposed MSMOACO, compared with previous approaches.}
}


@article{DBLP:journals/ton/DengZLLX24,
	author = {Qingyong Deng and
                  Qinghua Zuo and
                  Zhetao Li and
                  Haolin Liu and
                  Yong Xie},
	title = {Blockchain-Based Reputation Privacy Preserving for Quality-Aware Worker
                  Recruitment Scheme in {MCS}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5188--5203},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3453056},
	doi = {10.1109/TNET.2024.3453056},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DengZLLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsourcing (MCS) has become a novel paradigm for enabling data collection by worker recruitment, and the reputation plays a crucial role in achieving high-quality data. Although identity, data, and bid privacy preserving have been thoroughly investigated with the advance of blockchain technology, existing literature barely focuses on reputation privacy, which prevents malicious workers from submitting false data that could affect truth discovery for data requester. Therefore, we propose a Blockchain-Based Reputation Privacy Preserving for Quality-Aware Worker Recruitment Scheme (BRPP-QWR). First, we design a lightweight privacy preserving scheme for the whole life cycle of the worker’s reputation, which adopts sub-address retrieval technique combined with Pedersen Commitment and Compact Linkable Spontaneous Anonymous Group (CLSAG) signature to enable fast and anonymous verification of the reputation update process. Subsequently, to tackle the unknown worker recruitment problem, we propose a Reputation, Selfishness, and Quality-based Multi-Armed Bandit (RSQ-MAB) learning algorithm to select reliable and high-quality workers. Lastly, we implement a prototype system on Hyperledger Fabric to evaluate the performance of the reputation management scheme. The results indicate that the execution latency for the reputation score verification and retrieval latency can be reduced by an average of 6.30%–56.90% compared with ARMS-MCS. In addition, experimental results on both real and synthetic datasets show that the proposed RSQ-MAB algorithm achieves an increase of at least 20.05% in regard to the data requester’s total revenue and a decrease of at least 48.55% and 3.18% in regret and Multi-round Average Error (MAE), respectively, compared with other benchmark methods.}
}


@article{DBLP:journals/ton/MichelB24,
	author = {Fran{\c{c}}ois Michel and
                  Olivier Bonaventure},
	title = {{QUIRL:} Flexible {QUIC} Loss Recovery for Low Latency Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5204--5215},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3453759},
	doi = {10.1109/TNET.2024.3453759},
	timestamp = {Fri, 14 Feb 2025 20:50:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MichelB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A growing number of Internet applications require low latency. Unfortunately, most of these applications cannot use the rich features of the QUIC protocol since it only uses retransmissions to cope with packet losses. We propose, implement and evaluate QUIRL, a revisit of the QUIC loss recovery mechanism. QUIRL relies on Forward Erasure Correction (FEC) only if it is needed by the application’s latency requirements and uses classical retransmissions otherwise. We implement QUIRL and evaluate its performance for real-time video and HTTP/3. Compared to previous works adding FEC to QUIC, QUIRL is the first to be evaluated with and obtain significant performance improvements for popular applications over real lossy networks. Our evaluation shows that for video QUIRL improves the video quality while meeting strict delay requirements. For HTTP/3 transfers, QUIRL efficiently reduces the tail latency when packet losses occur without causing harm when there are no losses. We confirm these results using emulation over a wide ranges of bandwidth, delays and loss scenarios. We release our QUIRL implementation to encourage other researchers and industry to explore in more details the use of FEC in QUIC.}
}


@article{DBLP:journals/ton/ArrigoniPB24,
	author = {Viviana Arrigoni and
                  Matteo Prata and
                  Novella Bartolini},
	title = {Recovering Critical Service After Large-Scale Failures With Bayesian
                  Network Tomography},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5216--5231},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3454478},
	doi = {10.1109/TNET.2024.3454478},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ArrigoniPB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive failures in communication networks result from natural disasters, heavy blackouts, and military and cyber attacks. After these events, an adequate network recovery plan is key to ensuring emergency-critical service restoration and preventing intolerable downtime and performance degradation. We tackle the problem of minimizing the time and number of interventions to sufficiently restore the communication network to support emergency services after large-scale failures. We propose Proton (Progressive RecOvery and Tomography-based mONitoring), an efficient algorithm for progressive recovery of emergency services. Unlike previous work, assuming centralized routing and complete network observability, Proton addresses the more realistic scenario in which the network relies on the existing routing protocols, and knowledge of the network state is partial and uncertain. Proton relies on Network Tomography for monitoring and acquiring information about the state of nodes and links. Simulation results on real topologies show that our algorithm outperforms previous solutions in terms of cumulative routed flow, repair costs and recovery time in static and dynamic failure scenarios.}
}


@article{DBLP:journals/ton/LiaoQZZCXL24a,
	author = {Zhengyu Liao and
                  Shiyou Qian and
                  Zhonglong Zheng and
                  Jiange Zhang and
                  Jian Cao and
                  Guangtao Xue and
                  Minglu Li},
	title = {DBTable: Leveraging Discriminative Bitsets for High-Performance Packet
                  Classification},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5232--5246},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3452780},
	doi = {10.1109/TNET.2024.3452780},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoQZZCXL24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet classification, as a crucial function of networks, has been extensively investigated. In recent years, the rapid advancement of software-defined networking (SDN) has introduced new demands for packet classification, particularly in supporting dynamic rule updates and fast lookup. This paper presents a novel structure called DBTable for efficient packet classification to achieve high overall performance. DBTable integrates the strengths of conventional packet classification methods and neural network concepts. Within DBTable, a straightforward indexing scheme is proposed to eliminate rule replication, thereby ensuring high update performance. Additionally, we propose an iterative method for generating a discriminative bitset (DBS) to evenly partition rules. By utilizing the DBS, rules can be efficiently mapped in a hash table, thus achieving exceptional lookup performance. Moreover, DBTable incorporates a hybrid structure to further optimize the worst-case lookup performance, primarily caused by data skewness. The experiment results on 12 256k rulesets show that, compared to seven state-of-the-art schemes, DBTable achieves an overall lookup speed improvement ranging from 1.53x to 7.29x, while maintaining the fastest update speed.}
}


@article{DBLP:journals/ton/JinYLTWZC24,
	author = {Meng Jin and
                  Shun Yao and
                  Kexin Li and
                  Xiaohua Tian and
                  Xinbing Wang and
                  Chenghu Zhou and
                  Xinde Cao},
	title = {Fine-Grained {UHF} {RFID} Localization for Robotics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5247--5262},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3457696},
	doi = {10.1109/TNET.2024.3457696},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JinYLTWZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We in this paper present TiSee, an RFID-based sensing system that supports miniature robots to perform agile tasks in everyday environments. TiSee’s unique capability is that it uses a single arbitrarily-deployed antenna to locate a target with sub-cm-level accuracy and identify its orientation to within few degrees. Compared with existing solutions which rely on either antenna arrays or multiple RFID readers, TiSee is cheap, compact, and applicable to miniature robots. The idea of TiSee is to stick an RFID tag on the robot (or its gripper) and use it as a moving “antenna” to locate the tags on the target. The core of this design is a novel technique which can build a “channel” between two commercial RFID tags. Such an inter-tag channel is proved to be highly sensitive to the change in inter-tag distance and is resistant to multipath. By leveraging this channel and the mobility of the robot, we emulate an antenna array and use it for fine-grained localization and orientation estimation. Our experiments show that TiSee achieves a median accuracy of 9.5mm and 3.1° in 3D localization and orientation estimation. TiSee brings an eye-in-hand “camera” to miniature robots, supporting them to perform agile tasks in dark, cluttered, and occluded settings.}
}


@article{DBLP:journals/ton/LiCMXLYX24,
	author = {Jiang Li and
                  Jiahao Cao and
                  Zili Meng and
                  Renjie Xie and
                  Qi Li and
                  Yuan Yang and
                  Mingwei Xu},
	title = {RoLL+: Real-Time and Accurate Route Leak Locating With {AS} Triplet
                  Features at Scale},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5263--5278},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3458943},
	doi = {10.1109/TNET.2024.3458943},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiCMXLYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Border Gateway Protocol (BGP) is the only inter-domain routing protocol that plays an important role on the Internet. However, BGP suffers from route leaks, which can cause serious security threats. To mitigate the effects of route leaks, accurate and timely route leak locating is of great importance. Prior studies leverage AS business relationships to locate route leaks in real time. However, they fail to achieve high locating accuracy. Recent studies apply machine learning to accurately detect route leaks from statistical features of massive BGP messages. Nevertheless, they have high detection latency and cannot further locate route leaks. In this paper, we propose a real-time and accurate route leak locating system named RoLL+. It leverages distinctive AS triplet features to accurately locate AS triplets with route leaks from each BGP message in real time. Considering that RoLL+ may receive a substantial volume of BGP update messages per second, we integrate a cache-like design and a lazy update mechanism into the system to effectively identify route leaks at scale. Our experimental results on real-world BGP route leak data demonstrate that it can achieve 92% locating accuracy with less than 1 ms locating latency. Furthermore, the results show that RoLL+ can process over 7,000 AS triplets per second, meeting real-world throughput requirements.}
}


@article{DBLP:journals/ton/ParkBK24,
	author = {Soohyun Park and
                  Hankyul Baek and
                  Joongheon Kim},
	title = {Spatio-Temporal Multi-Metaverse Dynamic Streaming for Hybrid Quantum-Classical
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5279--5294},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3453067},
	doi = {10.1109/TNET.2024.3453067},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ParkBK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {According to the challenges related to the limited availability of quantum bits (qubits) in the era of noisy intermediate-scale quantum (NISQ), the immediate replacement of all components in existing network architectures with quantum computing devices may not be practical. As a result, implementing a hybrid quantum-classical system is regarded as one of effective strategies. In hybrid quantum-classical systems, quantum computing devices can be used for computation-intensive applications, such as massive scheduling in dynamic environments. Furthermore, one of most popular network applications is advanced social media services such as metaverse. Accordingly, this paper proposes an advanced multi-metaverse dynamic streaming algorithm in hybrid quantum-classical systems. For this purpose, the proposed algorithm consists of three stages. For the first stage, three-dimensional (3D) point cloud data gathering should be conducted using spatially scheduled observing devices from physical-spaces for constructing virtual multiple meta-spaces in metaverse server. This is for massive scheduling over dynamic situations, i.e., quantum multi-agent reinforcement learning-based scheduling is utilized for scheduling dimension reduction into a logarithmic-scale. For the second stage, a temporal low-delay metaverse server’s processor scheduler is designed for region-popularity-aware multiple virtual meta-spaces rendering contents allocation via modified bin-packing with hard real-time constraints. Lastly, a novel dynamic dynamic streaming algorithm is proposed for high-quality, differentiated, and stabilized meta-spaces rendering contents delivery to individual users via Lyapunov optimization theory. Our performance evaluation results verify that the proposed spatio-temporal algorithm outperforms benchmarks in various aspects over hybrid quantum-classical systems.}
}


@article{DBLP:journals/ton/WeiFGHW24,
	author = {Xinliang Wei and
                  Lei Fan and
                  Yuanxiong Guo and
                  Zhu Han and
                  Yu Wang},
	title = {Entanglement From Sky: Optimizing Satellite-Based Entanglement Distribution
                  for Quantum Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5295--5309},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3456789},
	doi = {10.1109/TNET.2024.3456789},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WeiFGHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancement of satellite-based quantum networks shows promise in transforming global communication infrastructure by establishing a secure and reliable quantum Internet. These networks use optical signals from satellites to ground stations to distribute high-fidelity quantum entanglements over long distances, overcoming the limitations of traditional terrestrial systems. However, the complexity of satellite-based entanglement distribution and terrestrial quantum swapping in the integrated network requires joint optimization with satellite assignment, resource allocation, and path selection. To address this challenge, we introduce a hybrid quantum-classical algorithm to solve the optimization problem by leveraging the strengths of both quantum and classical computing. The original problem is decomposed into a master problem and several subproblems using Dantzig-Wolfe decomposition and linearization techniques. Through experiments, this study demonstrates the effectiveness and reliability of the proposed methods in optimizing large-scale networks and managing qubit usage compared to the classical optimization techniques. The findings provide valuable insights for designing and implementing satellite-based entanglement distribution in quantum networks, paving the way for a secure global quantum communication infrastructure.}
}


@article{DBLP:journals/ton/LingLLDCX24,
	author = {Sitong Ling and
                  Zhuotao Liu and
                  Qi Li and
                  Xinle Du and
                  Jing Chen and
                  Ke Xu},
	title = {Stable Byzantine Fault Tolerance in Wide Area Networks With Unreliable
                  Links},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5310--5325},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3461872},
	doi = {10.1109/TNET.2024.3461872},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LingLLDCX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing demand for blockchain technology in various industry sectors, there has been a growing interest in the Byzantine Fault Tolerance (BFT) consensus that is the backbone of most of these blockchains. However, many state-of-the-art algorithms that require reliable connections can only offer limited throughput in wide-area networks (WANs), where participants are connected over long distances and may experience unpredictable network failures. The partially-connected BFTs are designed for unreliable and highly dynamic networks yet impose exponential communication complexity. This paper proposes Stable Byzantine Fault Tolerance (SBFT), a BFT communication abstraction that can sustain high throughput and low latency in WAN. SBFT separates the leader from consensus in pipelined BFT consensus and uses an adaptive consensus mechanism to resist dynamic faulty links, maintaining consensus efficiency when network connectivity is high while adapting to dynamic networks with low connectivity. We implemented a prototype of SBFT and tested it on the WAN. The results demonstrate that SBFT has a throughput similar to HotStuff in a fault-free environment but can reduce about 80% of consensus latency. Besides, SBFT retains 40% of the original throughput when the link failure probability is 0.4, while the baseline HotStuff retains less than 40% when the link failure probability is only 0.1.}
}


@article{DBLP:journals/ton/MizrahiKRC24,
	author = {Avi Mizrahi and
                  Noam Koren and
                  Ori Rottenstreich and
                  Yuval Cassuto},
	title = {Traffic-Aware Merkle Trees for Shortening Blockchain Transaction Proofs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5326--5340},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3466245},
	doi = {10.1109/TNET.2024.3466245},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MizrahiKRC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Merkle trees play a crucial role in blockchain networks in organizing network state. They allow proving a particular value of an entry in the state to a node that maintains only the root of the Merkle trees, a hash-based signature computed over the data in a hierarchical manner. Verification of particular state entries is crucial in reaching a consensus on the execution of a block where state information is required in the processing of its transactions. For instance, a payment transaction should be based on the balance of the two involved accounts. The proof length affects the network communication and is typically logarithmic in the state size. In this paper, we take advantage of typical transaction characteristics for better organizing Merkle trees to improve blockchain network performance. We focus on the common transaction processing where Merkle proofs are jointly provided for multiple accounts. We first provide lower bounds for the communication cost that are based on the distribution of accounts involved in the transactions. We then describe algorithms that consider traffic patterns for significantly reducing it. The algorithms are inspired by various coding methods such as Huffman coding, partition and weight balancing. We also generalize our approach towards the encoding of smart contract transactions that involve an arbitrary number of accounts. Likewise, we rely on real blockchain data to show the savings allowed by our approach. The experimental evaluation is based on transactions from the Ethereum network and demonstrates cost reduction for both payment transactions and smart contract transactions.}
}


@article{DBLP:journals/ton/WangLZML24,
	author = {Zhiyuan Wang and
                  Xin Lai and
                  Shan Zhang and
                  Qingkai Meng and
                  Hongbin Luo},
	title = {Enabling Byzantine Fault Tolerance in Access Authentication for Mega-Constellations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5341--5355},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3463609},
	doi = {10.1109/TNET.2024.3463609},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLZML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Earth-Orbit (LEO) satellite constellations are becoming the necessary infrastructure in the future. However, the secure operation of LEO constellations is faced with severe risks. Specifically, LEO satellites are constantly orbiting and their channel interfaces are open. The adversary in hostile regions can leverage the global footprint to inject malicious traffic via access satellites. That is, LEO satellites are susceptible to physical and cyber attacks. Therefore, access authentication regarding terrestrial users (TUs) is crucial to ensure the secure operation of LEO constellations. The traditional on-orbit authentication frameworks usually presume that satellites are reliable and mutually trusted, thus one could rely on access satellites to perform authentication. In practice, however, physical and cyber attacks could bring down the satellites (causing fail-stop fault) or even hijack the satellites (causing Byzantine fault). This fact requires that the access authentication framework installed on LEO constellations should be fault-tolerant. In this paper, we aim to achieve Byzantine fault tolerance in access authentication for LEO satellite networks by properly integrating PBFT consensus protocol with traditional on-orbit authentication. Based on the topology characteristics of LEO constellations, we analytically derive the consensus probability, authentication accuracy, and communication overhead under PBFT-based authentication. To reduce the communication overhead, we propose to partition the constellation into multiple consensus groups, and devise a hierarchical PBFT (HPBFT) protocol. Simulation results based on Starlink Shell-I constellation indicate that HPBFT-based authentication could reduce the communication overhead (by an order of magnitude) and maintain almost the same authentication accuracy compared to PBFT-based authentication.}
}


@article{DBLP:journals/ton/TianZLY24,
	author = {Xiang Tian and
                  Baoxian Zhang and
                  Cheng Li and
                  Jiguo Yu},
	title = {Distributed Stable Multi-Source Dynamic Broadcasting for Wireless
                  Multi-Hop Networks Under SINR-Based Adversarial Channel Jamming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5356--5371},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3470649},
	doi = {10.1109/TNET.2024.3470649},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TianZLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Disseminating continuous packet flows injected at multiple location-random source nodes to all network nodes, known as the multi-source dynamic global broadcast problem, is a fundamental building block for wireless multi-hop networks to run smoothly and efficiently. Previous studies on dynamic global broadcast all assume reliable communications. However, in realistic wireless networks, there exist unpredictable transmission failures caused by the randomized signal interference from uncorrelated wireless networks sharing the same spectrum or even malicious attackers. In this paper, by integrating the Signal-to-Interference-plus-Noise-Ratio (SINR) model, multi-channel communication mode, and randomized malicious channel jamming controlled by an adaptive adversary, we present an SINR-based adversarial channel jamming model to capture the unpredictable transmission failures in a wireless multi-hop network. We first propose a distributed Jamming-resilient Multi-source Static Broadcast (JMSB) algorithm based on random channel selection and message transmissions for multi-hop wireless networks under the above SINR-based adversarial channel jamming model. We then propose a distributed stable Jamming-resilient Multi-source Dynamic Broadcast (JMDB) algorithm which iterates JMSB repeatedly and efficiently in a two-stage manner. We derive the maximum supportable broadcast throughput of JMDB under the stability guarantee, i.e., the expected boundedness on the queue length of each network node and expected broadcast latency for each injected packet. Simulation results shows the stability and throughput efficiency of our proposed JMDB algorithm.}
}


@article{DBLP:journals/ton/LuoZSYN24,
	author = {Haoxiang Luo and
                  Qianqian Zhang and
                  Gang Sun and
                  Hongfang Yu and
                  Dusit Niyato},
	title = {Symbiotic Blockchain Consensus: Cognitive Backscatter Communications-Enabled
                  Wireless Blockchain Consensus},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5372--5387},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3462539},
	doi = {10.1109/TNET.2024.3462539},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoZSYN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The wireless blockchain network (WBN) concept, born from the blockchain deployed in wireless networks, has appealed to many network scenarios. Blockchain consensus mechanisms (CMs) are key to enabling nodes in a wireless network to achieve consistency without any trusted entity. However, consensus reliability will be seriously affected by the instability of communication links in wireless networks. Meanwhile, it is difficult for nodes in wireless scenarios to obtain a timely energy supply. Energy-intensive blockchain functions can quickly drain the power of nodes, thus degrading consensus performance. Fortunately, a symbiotic radio (SR) system enabled by cognitive backscatter communications can solve the above problems. In SR, the secondary transmitter (STx) transmits messages over the radio frequency (RF) signal emitted from a primary transmitter (PTx) with extremely low energy consumption, and the STx can provide multipath gain to the PTx in return. Such an approach is useful for almost all vote-based CMs, such as the Practical Byzantine Fault-tolerant (PBFT)-like and the RAFT-like CMs. This paper proposes symbiotic blockchain consensus (SBC) by transforming 6 PBFT-like and 4 RAFT-like state-of-the-art (SOTA) CMs to demonstrate universality. These new CMs will benefit from mutualistic transmission relationships in SR, making full use of the limited spectrum resources in WBN. Simulation results show that SBC can increase the consensus success rate of PBFT-like and RAFT- like by 54.1% and 5.8%, respectively, and reduce energy consumption by 9.2% and 23.7%, respectively.}
}


@article{DBLP:journals/ton/XingZ24,
	author = {Wei Xing and
                  Xudong Zhao},
	title = {Transmission Scheduling for Remote State Estimation in CPSs With Two-Hop
                  Networks Subject to DoS Attacks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5388--5398},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3463186},
	doi = {10.1109/TNET.2024.3463186},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XingZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a dynamic zero-sum game is formulated to describe the power decision-making process of the sensor/relay and the DoS attacker in cyber-physical systems. The sensor and the relay cooperate with each other to transmit the state estimation to the remote estimator, when the attacker, on the contrary, aims to disturb the wireless communication channels strategically for deterioration of the system performance but can do this taking into account its limited energy. Different from conventional battery-powered nodes, the sensor and the relay can harvest energy from the external environment and store it in their batteries for data transmission. We model the external environment state as a Markov chain to overcome the randomness of the harvested energy. In addition, to tackle the computation complexity of the Nash equilibrium (NE), we restrict our attention to a special case, i.e., the DoS attacker can only launch interference on one of the two communication channels over an infinite time horizon, and provide the corresponding NE strategy of the game using the Markov decision process and the multi-agent reinforcement learning algorithm. Finally, simulation examples are given to illustrate the theoretical findings of the paper.}
}


@article{DBLP:journals/ton/ZhaoYWLZS24,
	author = {Sen Zhao and
                  Shouguo Yang and
                  Zhen Wang and
                  Yongji Liu and
                  Hongsong Zhu and
                  Limin Sun},
	title = {Crafting Binary Protocol Reversing via Deep Learning With Knowledge-Driven
                  Augmentation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5399--5414},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3468350},
	doi = {10.1109/TNET.2024.3468350},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoYWLZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Protocol reverse engineering (PRE) serves as an instrumental tool in various security research, such as protocol fuzzing and intrusion detection. Its primary objective lies in uncovering the format, semantics, and behavior of an unknown protocol without prior information. This paper presents DL-ProS2, a deep learning-based approach for binary protocol reversing, focusing on format segmentation and semantic inference from network traffic. Our approach is underpinned by highlighting the effectiveness of multi-scale features within the network traffic for identifying various types of fields and semantics. Based on this, DL-ProS2 employs a comprehensive end-to-end model that integrates U-Net, siamese network, and BiLSTM-CRF, which enables the effective analysis of unknown protocol traffic to extract the field boundaries and semantics. Meanwhile, to address the issue of limited data diversity and coverage, we implement an innovative knowledge-driven traffic simulation technique. This method harnesses the ChatGPT to extract protocol knowledge from publicly available protocol documents, such as RFCs, as the foundational rules for the simulation. Empirical results substantiate the efficacy of our approach, demonstrating precision rates exceeding 0.95 and recall rates surpassing 0.97 for partially unknown protocol format segmentation and semantic inference. It also retains effectiveness in the inference of completely unknown protocols, with average precision and recall rates of 0.69 and 0.62 for format segmentation, and 0.43 and 0.47 for semantic inference, respectively.}
}


@article{DBLP:journals/ton/GaoXLZY24,
	author = {Ronghao Gao and
                  Yunlai Xu and
                  Han Li and
                  Qinyu Zhang and
                  Zhihua Yang},
	title = {Semantic-Aware Jointed Coding and Routing Design in Large-Scale Satellite
                  Networks: {A} Deep Learning Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5415--5429},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3464540},
	doi = {10.1109/TNET.2024.3464540},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GaoXLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In large-scale satellite networks, data delivery confronts obvious challenges such as high loss rate and long propagation delay leading to low Packet Delivery Ratio (PDR) and huge delivery latency over intermittent Inter-Satellite Links (ISLs), making the current routing algorithms exploiting typical Automatic Repeat reQuest (ARQ) mechanisms extremely inefficient and even incapable. To address this issue, in this paper, we propose a semantic-aware coding and routing joint mechanism called Semantic Adaptive Coding and Routing (SACR) by considering both the semantic correlations in the context-dependent data and the link status knowledge. In particular, the proposed SACR achieves excellent error-tolerant and routing-agile capabilities by an elaborately interactive design consisting of a customized routing-aware Semantic Adaptive Coding Hybrid ARQ (SAC-HARQ) mechanism and a Semantic Coding-based Routing Mechanism (SCRM). The simulation results indicate that the proposed SACR mechanism performs better in reducing the average delivery latency and improving the effective throughput compared with typical routing mechanisms such as Open Shortest Path First (OSPF) routing, Deep Q-Networks based Intelligent Routing (DQN-IR), and Real-Time Hop-by-hop routing (RTHop), integrating with typical semantic coding methods, i.e., Deep Learning-based Joint Channel-Source Coding (DL-JSCC), Deep learning-based Semantic Communication system (DeepSC), and Semantic Coding HARQ (SCHARQ), respectively.}
}


@article{DBLP:journals/ton/Zhang24,
	author = {Peng Zhang},
	title = {Combining Capacity and Length: Finding Connectivity Bottleneck in
                  a Layered Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5430--5439},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3466522},
	doi = {10.1109/TNET.2024.3466522},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Zhang24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computer networks are often multi-layered. For simplicity, let us focus on two-layered networks with logical layer and physical layer. Such a network can be modeled as a labeled graph G = (V, E) with a label set L = \\{\\ell _{1}, \\ell _{2}, {\\dots }, \\ell _{q} \\} , in which each edge (denotes logical connection) e \\in E has a label (denotes physical link) \\ell (e) from L. The key issue is that different edges may have the same label. In the weighted minimum Label s-t Cut problem, we are given a labeled graph G=(V,E) with label set L, where each label \\ell has a nonnegative weight w_{\\ell } , a source s \\in V and a sink t \\in V . The problem asks to find a minimum weight label subset L' (called a label s-t cut) such that the removal of all edges with labels in L' disconnects s and t. Label s-t cut depicts the connectivity bottleneck of a layered network. It is a natural generalization of the edge connectivity of a graph. In this paper, we provide an approximation algorithm for the weighted Label s-t Cut problem with ratio O(n^{2/3}) , where n is the number of vertices. This is the first approximation algorithm for the problem whose ratio is given in terms of n. The key point of the algorithm is a mechanism to interpret label weight on an edge as both its length (as in the Shortest s-t Path problem) and capacity (as in the Min s-t Cut problem). Experiments on random graphs show that the algorithm has also good practical performance.}
}


@article{DBLP:journals/ton/SunLGLX24,
	author = {Yuchen Sun and
                  Lailong Luo and
                  Deke Guo and
                  Li Liu and
                  Junjie Xie},
	title = {KMSharing: The Framework and Space Abstraction for Efficient Data
                  Sharing at the Network Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5440--5458},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3465844},
	doi = {10.1109/TNET.2024.3465844},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunLGLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge storage promises to be crucial for edge computing infrastructure, which enables users to access data within a low delay from widespread storage nodes at the network edge. The key challenge is how to integrate massive geographically distributed weak edge nodes to form an efficient storage system, enabling users to launch data operations from any node or retrieve the desired data across the entire distributed system. To address this data-sharing problem, researchers from both the traditional peer-to-peer (P2P) overlay networking and emerging edge computing fields have proposed some decentralized indexing mechanisms. However, existing studies lack insightful descriptions and analyses about the nature of the data-sharing problem at the network edge. It motivates us to rethink the edge data-sharing framework and provide the problem reformulation for analyzing the limitations of existing schemes. We reveal that the existing data-sharing schemes fail in complex network topologies which can be regarded as high-dimensional network spaces beyond the representation of low-dimensional Euclidean spaces or other existing hash spaces. A better space abstraction is an urgent need to alleviate the performance degradation due to the dimensional mismatch between network spaces and virtual spaces. To fill this gap, this paper proposes the Kautz metric space, a novel space abstraction extended from Kautz graphs, where the coordinates and the metric are defined as Kautz strings and Kautz distances (i.e., the shortest distances in undirected Kautz graphs), respectively. We design a dynamic programming algorithm to directly compute the Kautz distances. Then, we propose KMSharing, an efficient edge data-sharing scheme: both nodes and data are represented in a Kautz metric space, where the Kautz distance of any two Kautz strings reflects the network delay of the corresponding nodes. The workflow of KMSharing consists of three core components: the virtual address allocation represents edge nodes in the Kautz metric space; the data-to-node mapping ensures the uniqueness of target nodes; and forwarding table construction ensures the data delivery. Theoretical analyses confirm that KMSharing ideally achieves $\\mathcal {O}\\left ({{ \\tau }}\\right)$ network delays, $\\mathcal {O}\\left ({{ \\log N }}\\right)$ overlay hops, and $\\mathcal {O}\\left ({{ 1 }}\\right)$ forwarding entries in an N-node edge system with the network radius $\\tau $ , while the successive ensuring data delivery. Its worst-case network delay $\\mathcal {O}\\left ({{ \\tau \\log N }}\\right)$ is also much better than ${\\mathcal {O}\\left ({{ \\tau N^{\\alpha } }}\\right)},\\alpha \\mathrm {\\in }(0,1)$ , the worst case of the baselines using Euclidean spaces. Evaluation on various network topologies also shows that our KMSharing effectively reduces network delays and indexing costs than existing data-sharing schemes.}
}


@article{DBLP:journals/ton/ShenYZDL24,
	author = {Dian Shen and
                  Bin Yang and
                  Junxue Zhang and
                  Fang Dong and
                  John C. S. Lui},
	title = {eMPTCP: {A} Framework to Fully Extend Multipath {TCP}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5459--5474},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3469396},
	doi = {10.1109/TNET.2024.3469396},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShenYZDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {MPTCP provides the basic multipath support for network applications to deliver high throughput and robust communication. However, the original MPTCP is designed with limited extensibility. Various research works have tried to extend MPTCP to attain better performance or richer functionalities. These existing approaches either modify the kernel implementation of MPTCP, which involve considerable engineering efforts and may accidentally introduce safety issues, or control MPTCP via userspace tools, which suffer from restricted functionality support. To address this issue, we propose eMPTCP, an easy-to-use framework to fully extend MPTCP without safety risks. Internally, eMPTCP has a modular and pluggable model which allows operators to specify a comprehensive MPTCP extension as a chain of sub-policies. eMPTCP further enforces the policies through packet header manipulations. To ensure safety, eMPTCP is implemented using eBPF. Despite the stringent constraints of eBPF, we show that it is possible to implement an elaborated framework for a fully extensible MPTCP. Through verifying MPTCP in a number of real-world cases and extensive experiments, we show that eMPTCP is able to support a wide range of MPTCP extensions, while the overhead of eMPTCP operations in the kernel is in the scale of nanosecond, and the extra processing time accounts for only about 0.63% of flows’ transmission time.}
}


@article{DBLP:journals/ton/WangZCSLLZCC24,
	author = {Kun Wang and
                  Chengcheng Zhao and
                  Jinpei Chu and
                  Yiping Shi and
                  Jianyuan Lu and
                  Biao Lyu and
                  Shunmin Zhu and
                  Peng Cheng and
                  Jiming Chen},
	title = {LFVeri: Network Configuration Verification for Virtual Private Cloud
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5475--5490},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3469386},
	doi = {10.1109/TNET.2024.3469386},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangZCSLLZCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Virtual Private Cloud (VPC) service enables users to configure shared resources within public clouds on demand, providing isolation between users. However, configuring the VPC network is a complex and error-prone task, and misconfiguration has been the leading cause of cloud network security issues. The large number of complex network components and configurations makes it difficult to perform scalable, efficient, and accurate fault verification of the network behavior. To address this issue, we design a comprehensive and automated fault diagnosis and localization tool, called LFVeri, which is built upon an innovative modular network model that accurately captures the logic functions of real components within VPC networks, and propose eleven functions to verify network reachability and security requirements. We conduct performance testing of LFVeri on various datasets and compared it with other verification tools. The experiments show that LFVeri outperforms in modeling and analyzing real VPC scenarios while also possessing the fastest verification speed. It can model and analyze large VPC networks with tens of thousands of components and millions of configuration rules in less than half an hour.}
}


@article{DBLP:journals/ton/GaoMHSWDC24,
	author = {Guoju Gao and
                  Tianyu Ma and
                  He Huang and
                  Yu{-}E Sun and
                  Haibo Wang and
                  Yang Du and
                  Shigang Chen},
	title = {Scout Sketch+: Finding Both Promising and Damping Items Simultaneously
                  in Data Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5491--5506},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3469196},
	doi = {10.1109/TNET.2024.3469196},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GaoMHSWDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data stream processing holds great potential value in lots of practical application scenarios. This paper studies two new but important patterns for items in data streams, called promising and damping items. The promising items mean that the frequencies of an item in multiple continuous time windows show an upward trend overall, while a slight decrease in some of these windows is allowed. In contrast to promising items exhibiting an increasing trend, the definition of damping items indicates a decreasing trend. Many applications can benefit from the property of promising or damping items, e.g., monitoring latent attacks in computer networks, pre-adjusting bandwidth allocation in communication channels, detecting potential hot events/news, or finding topics that gradually lose momentum in social networks. We first introduce how to accurately find promising items in data streams in real-time under limited memory space. To this end, we propose a novel structure named Scout Sketch, which consists of Filter and Finder. Filter is devised based on the Bloom filter to eliminate the ungratified items with less memory overload; Finder records some necessary information about the potential items and detects the promising items at the end of each time window, where we propose some tailor-made detection operations. We then enhance Scout Sketch (called Scout Sketch+) to adaptively detect both types of promising and damping items simultaneously. Finally, we conducted extensive experiments on four real-world datasets, which show that the F1 Score and throughput of Scout Sketch(+) are about 2.02 and 5.61 times that of the compared solutions. All source codes are available at Github (https://github.com/Aoohhh/ScoutSketch).}
}


@article{DBLP:journals/ton/LiYYL24,
	author = {Ruixiang Li and
                  Xiaoyun Yuan and
                  Meijuan Yin and
                  Xiangyang Luo},
	title = {Mobile {IP} Geolocation Based on District Anchor Without Cooperation
                  of Users or Internet Service Providers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5507--5523},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3471335},
	doi = {10.1109/TNET.2024.3471335},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile IP geolocation aims to obtain a mobile device’s geographic location by IP. This technology is widely used in preventing financial risk, investigating cybercrime, and delivering targeted information. Currently, there are three types of IP geolocation: based on cooperation, querying in database, or network measurement. However, since restricted cooperation, low-reliability databases, and unresponsive mobile IPs, existing technologies are hard to geolocate fine-grained location of mobile IP. In this paper, we propose the concept of district anchor, and propose a non-cooperative mobile IP geolocation scheme, including three parts: acquiring district anchors by clustering, evaluating the reliability of district anchors, and geolocating mobile IPs. We also give implemented approach of this scheme. Instead of using existing clustering algorithms treating IPs and geolocations in no particular order, we propose two-stages clustering algorithm (IPG2C) to acquire district anchors, and establish reliability evaluation mechanism by IP distribution and spatial distribution of cluster. Eventually, using obtained reliable district anchors, we use “subnet geolocation” strategy to geolocate mobile IPs. The experimental results in 10 cities show that: 1) our scheme can be used to geolocate mobile IPs without cooperation; 2) the mean geolocation error is 12.47km, where precision of 56.67% of mobile IPs is street-level and minimum error is only 13m; 3) that the mean geolocation error of the anchor-based method is smaller than that of the landmark-based method; 4) compared with 13 clustering algorithms (e.g., K-Means++, Mean Shift, DBSCAN, and GMM), mean geolocation error using IPG2C’s district anchors is reduced by 26.62%~50.77%.}
}


@article{DBLP:journals/ton/SongHCLFWWY24,
	author = {Guanglei Song and
                  Lin He and
                  Tao Chen and
                  Jinlei Lin and
                  Linna Fan and
                  Kun Wen and
                  Zhiliang Wang and
                  Jiahai Yang},
	title = {PMap: Reinforcement Learning-Based Internet-Wide Port Scanning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {32},
	number = {6},
	pages = {5524--5538},
	year = {2024},
	url = {https://doi.org/10.1109/TNET.2024.3491314},
	doi = {10.1109/TNET.2024.3491314},
	timestamp = {Sat, 25 Jan 2025 23:34:45 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SongHCLFWWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet-wide scanning is a commonly used research technique in various network surveys, such as measuring service deployment and security vulnerabilities. However, these network surveys are limited to the given port set, not comprehensively obtaining the real network landscape, and even misleading survey conclusions. In this work, we introduce PMap, a port scanning tool that efficiently discovers the most open ports from all 65K ports in the whole network. PMap uses the correlation of ports to build an open port correlation graph of each network, using a reinforcement learning framework to update the correlation graph based on feedback results and dynamically adjust the order of port scanning. Compared to current port scanning methods, PMap performs better on hit rate, coverage, and intrusiveness. Our experiments over real networks show that PMap can find 90% open ports by only scanning 125 ports (90%@125) to each address, which is 99.3% less than the state-of-the-art port scanning methods. It reduces the number of scanned ports to decrease the intrusive nature of port scanning. In addition, PMap is highly parallel and lightweight. It scans 500 networks in parallel, achieving a port recommendation rate of up to 18 million per second, consuming only 7GB of memory. PMap is the first effective practice for scanning open ports using reinforcement learning. It bridges the gap of existing scanning tools and effectively supports subsequent service discovery and security research.}
}
