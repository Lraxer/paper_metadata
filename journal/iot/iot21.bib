@article{DBLP:journals/iot/ShirvaniM23,
	author = {Mirsaeid Hosseini Shirvani and
                  Mohammad Masdari},
	title = {A survey study on trust-based security in Internet of Things: Challenges
                  and issues},
	journal = {Internet Things},
	volume = {21},
	pages = {100640},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100640},
	doi = {10.1016/J.IOT.2022.100640},
	timestamp = {Tue, 18 Apr 2023 16:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ShirvaniM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is an emerging communication technology which connects all of the network-enabled devises in all of the world. It is being pervasively utilized in the variety industrial applications such as infrastructure monitoring, utilities, controlling, mission-critical, and security-sensitive applications. Similar to other conventional networks, the IoT applications are also vulnerable to the different kinds of security breaches, threats, and attacks. To mitigate such related problems, various cryptographic, trust-based approaches, and models have been presented in the literature. To address the issue, this paper presents a subjective survey study on IoT trust security and challenges to investigate solutions provided for trust management & handling in the IoT context. The subjective survey is designed based on the commonalities and differences of the presented state-of-the-arts in literature. Then, it classifies the studied state-of-the-art schemes that are based on the trust handling method and summarizes their main contributions and properties. A comparison on the various published papers of these trust-based schemes is tabulated based on the relevant concepts and evaluation metrics derived from literature. Therefore, the paper highlights potentials, existing gaps, and the future research direction in the trust management of the IoT context for further processing and improvement.}
}


@article{DBLP:journals/iot/NayakS23,
	author = {Padmalaya Nayak and
                  G. Swapna},
	title = {Security issues in IoT applications using certificateless aggregate
                  signcryption schemes: An overview},
	journal = {Internet Things},
	volume = {21},
	pages = {100641},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100641},
	doi = {10.1016/J.IOT.2022.100641},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/NayakS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Security is one of the major concerns in IoT networks which consists of a huge number of objects that have given an alarm with the risen number of threats in our day-to-day life. Mobile IoT users are dynamically categorized into multiple social groups based on their velocity, directions, and accelerations. In each group, all IoT nodes broadcast their collected content bundles in a short range of communication and share a dynamically updated group key negotiated by all of them. To make the whole IoT infrastructure secure, efficient, robust cryptographic techniques are developed and required to handle data authentication, data privacy, confidentiality, integrity, etc. Furthermore, with the large-scale connectivity and group key management policy, cryptographic processes like Signcryption techniques used for authentication and confidentiality increase the overhead complexity of the network resources. To reduce this overhead, numerous research has been conducted on Certificateless Aggregate Signcyption Schemes (CLASC) to reduce the time complexity, computational and communication overheads. Keeping the effectiveness of CLASC in mind, an attempt has been made to provide a detailed review of the security-related challenges and issues in IoT applications using CLASC hoping that it will provide a good insight to the current researchers, practitioners, and relevant stakeholders to carry out further research.}
}


@article{DBLP:journals/iot/Herabad23,
	author = {Mohammadsadeq Garshasbi Herabad},
	title = {Communication-efficient semi-synchronous hierarchical federated learning
                  with balanced training in heterogeneous IoT edge environments},
	journal = {Internet Things},
	volume = {21},
	pages = {100642},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100642},
	doi = {10.1016/J.IOT.2022.100642},
	timestamp = {Tue, 18 Apr 2023 16:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/iot/Herabad23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) aims to train a globally shared model by employing local data samples generated by data sources. The inherent heterogeneity of IoT environments, in terms of hardware, network technology, network condition, and power supply, imposes challenges to original FL associated with model convergence speed, model accuracy, balanced training, and communication costs. In this paper, we propose a Hierarchical FL (HFL) framework based on semi-synchronous communications to address heterogeneity issues of original FL in intelligent IoT Edge environments. In particular, the proposed approach runs the aggregation process of local models in a hierarchal manner to generate a global model and uses semi-synchronous communications to reduce communication costs. Moreover, a weighted aggregation method is introduced to overcome the unbalanced training problem of original FL in heterogeneous environments. To evaluate the performance of the proposed framework, we implement a heterogeneous IoT use case related to anomaly detection in a milling machining and design a prediction-based anomaly detection method based on Convolutional Neural Network (CNN), which is trained based on proposed Semi-synchronous HFL (SHFL). The results of implementations demonstrate the effectiveness of the proposed approach in comparison to fully synchronous and fully asynchronous FL approaches.}
}


@article{DBLP:journals/iot/SinhaCSSD23,
	author = {Aparna Sinha and
                  Deepraj Chowdhury and
                  Sandeep Sharma and
                  Yashasva Raj Sherke and
                  Debanjan Das},
	title = {nCare: Fault-aware edge intelligence for rendering viable sensor nodes},
	journal = {Internet Things},
	volume = {21},
	pages = {100643},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100643},
	doi = {10.1016/J.IOT.2022.100643},
	timestamp = {Mon, 01 May 2023 13:02:02 +0200},
	biburl = {https://dblp.org/rec/journals/iot/SinhaCSSD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In remote sensor nodes, energy harvesters are used with rechargeable batteries as the power source. As these batteries undergo capacitance reduction, they degrade with each charge–discharge cycle. Most of the existing researches do not consider edge-based computation for battery health prediction. This paper proposes a smart, accurate, and reliable data-driven architecture, nCare, to predict the battery’s health at low power and low computation costs. Three models are used for health prediction, two Gaussian Process Regression (GPR) models deployed at the sensor node — one to predict the State of Health (SoH) of the battery for the first few cycles and the second to predict the Remaining Useful Life (RUL) of battery. An AutoKeras model deployed at the gateway where it will be trained using the first 200 cycles. Then weight matrix and layer information will be sent to sensor nodes to replicate the model without training to predict SoH after 200 cycles. Two main parameters, battery temperature and elapsed time (ET), are used to predict the SoH, which is further used for RUL prediction. Furthermore, to prove the efficiency of the RUL prediction method, the calculated Absolute Error (AE), Relative Mean Square Error (RMSE), and Relative error (RE) are 0.019, 0.142, and 0.005, respectively for a battery of 730 cycles. This method helps the sensor node acquire intelligence that will be battery specific. Blockchain is used for maintaining the data integrity of the architecture. nCare thus ensures an effective and accurate solution for predicting the RUL of a battery.}
}


@article{DBLP:journals/iot/AhnLAK23,
	author = {Jungmo Ahn and
                  Youngki Lee and
                  Jeongseob Ahn and
                  JeongGil Ko},
	title = {Server load and network-aware adaptive deep learning inference offloading
                  for edge platforms},
	journal = {Internet Things},
	volume = {21},
	pages = {100644},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100644},
	doi = {10.1016/J.IOT.2022.100644},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/AhnLAK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work presents DIAMOND, a deep neural network computation offloading scheme consisting of a lightweight client-to-server latency profiling component combined with a server inference time estimation module to accurately assess the expected latency of a deep learning model inference. Latency predictions for both the network and server are comprehensively used to make dynamic (partial) model offloading decisions at the client in run-time. Compared to previous work, DIAMOND targets to minimize network latency estimation overhead and considers the concurrent processing nature of state-of-the-art deep learning inference server designs. Our extensive evaluations with an NVIDIA Jetson Nano client connected to an NVIDIA Triton server shows that DIAMOND completes inference operations with noticeably reduced computational/energy overhead and latency compared to previously proposed model offloading approaches. Furthermore, our results show that DIAMOND well-adapts to practical server load and network dynamics.}
}


@article{DBLP:journals/iot/FawzyMB23,
	author = {Dina Fawzy and
                  Sherin M. Moussa and
                  Nagwa L. Badr},
	title = {An IoT-based resource utilization framework using data fusion for
                  smart environments},
	journal = {Internet Things},
	volume = {21},
	pages = {100645},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100645},
	doi = {10.1016/J.IOT.2022.100645},
	timestamp = {Sat, 13 May 2023 01:06:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/FawzyMB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, many communities are emerging towards smart environments, requiring the communication and collaboration of diverse Internet-of-Things (IoT) devices. A smart environment exploits the use of IoT technology to share and process data among such devices for a better living. However, this comes with additional costs, such as the exponential growth of IoT devices, the heterogeneity of IoT use cases, and the new complex features encountered by IoT data, which complicate their processing and analysis using the traditional techniques. This causes a dramatic performance degradation of the used processing resources, which directly affects the overall efficiency and performance of IoT-based systems. Although different studies have presented resource utilization approaches for IoT systems, but they were not evaluated from different resource utilization perspectives. Besides, no efforts have been directed to investigate their effectiveness to process the unprecedented IoT data features that inevitably impact the accuracy and efficiency of resource utilization. In this paper, the Triple Phases Resource Utilized Data Fusion (TPRUDF) framework is proposed as the first IoT-based cost-aware resource utilization using data fusion. It exclusively considers different IoT data features by employing three phases of data fusion: (1) data in – data out, (2) data in – feature out, and (3) feature in – decision out. TPRUDF fuses the raw IoT data by maintaining the complex IoT data features, independent of the IoT domain or the computing model, using the spatiotemporal data fusion (STDF) IoT-based data fusion approach. TPRUDF then fuses the uncorrelated data features via the Principal Component Analysis. Finally, it employs two different resource utilization techniques: (1) Genetic Algorithms and (2) Particle Swarm Optimization, fusing their results using the voting logic fusion technique. A public edge-computing simulator is used to evaluate TPRUDF via three real-world smart cities datasets. The experimental results of the proposed TPRUDF framework indicate that it: (1) achieves an average accuracy level of resource utilization equal to 91%, (2) increases the resource utilization throughput by an average of 40% and eventually minimizes the processing delay, (3) boosts the resource utilization availability by 60%, and (4) decreases the energy consumption by 35%.}
}


@article{DBLP:journals/iot/ParkPCK23,
	author = {Junhyun Park and
                  Junyoung O. Park and
                  Jaehyuk Choi and
                  Ted Taekyong Kwon},
	title = {\emph{D-SCAN}: Toward collaborative multi-radio coexistence in mobile
                  devices via deep learning},
	journal = {Internet Things},
	volume = {21},
	pages = {100646},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100646},
	doi = {10.1016/J.IOT.2022.100646},
	timestamp = {Mon, 28 Aug 2023 21:41:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ParkPCK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the demand for efficient and reliable wireless connectivity continues to increase, mobile Internet-of-Things devices equipped with multiple heterogeneous radios including Wi-Fi and Bluetooth have become prevalent. However, collocated Wi-Fi and Bluetooth operate in the same 2.4 GHz industrial, scientific, and medical band and interfere with each other internally and externally. Although current devices avoid internal cross-technology interference by enabling either Wi-Fi or Bluetooth to transmit packets at any specific time slot in a time-division multiplexing manner, the dissonance with external interference mitigation schemes can result in severe performance degradation. In this paper, we present D-SCAN, a novel collaborative coexistence mechanism. D-SCAN infers nearby Wi-Fi information efficiently using a collocated Bluetooth radio, thereby offsetting the overhead of key Wi-Fi functions and preventing collisions between Wi-Fi and Bluetooth. To this end, D-SCAN adopts a data-driven approach that captures the unique temporal and spectral features of Wi-Fi signals from Bluetooth spectrum measurements by leveraging deep neural networks. A D-SCAN prototype in real-world experiments reduces the latency and energy consumption of legacy Wi-Fi scanning by 23% and 45%, respectively. It also promotes the agile interference avoidance of Bluetooth that coexists with Wi-Fi on a single device. Thus, D-SCAN demonstrates efficiency in resource management and effectiveness in mitigating cross-technology interferences.}
}


@article{DBLP:journals/iot/Herrero23,
	author = {Rolando Herrero},
	title = {Mechanism for IPv6 adaptation in LoRa topologies},
	journal = {Internet Things},
	volume = {21},
	pages = {100647},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100647},
	doi = {10.1016/J.IOT.2022.100647},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/Herrero23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the main requirements of an IoT solution is the support of end-to-end IPv6 connectivity. Most Wireless Personal Area (WPAN) physical and link layers like Bluetooth Low Energy (BLE), ITU-T G.9959 and IEEE 802.15.4 support adaptation mechanisms that, based on the IPv6 over Low Power Wireless Personal Area Networks (6LoWPAN) protocol, enable IPv6. On the other hand, Low Power Wide Area Network (LPWAN) technologies like LoRa and SigFox are not typically designed to support access-side IPv6 and rely on gateways for translation and overall cloud connectivity. Lack of end-to-end IPv6 connectivity implies that many of these LPWAN technologies do not fully comply with the IoT paradigm preventing the direct interaction between devices of different physical technologies. In this paper we present an adaptation mechanism that enables end-to-end IPv6 support in an LPWAN scenario that relies on a LoRa physical layer. The scheme is analyzed, modeled and implemented by means of an experimental framework that serves to examine the efficiency of the proposed solution.}
}


@article{DBLP:journals/iot/CaiSSGD23,
	author = {Zhi Cai and
                  Yuyu Shu and
                  Xing Su and
                  Limin Guo and
                  Zhiming Ding},
	title = {A traffic data interpolation method for IoT sensors based on spatio-temporal
                  dependence},
	journal = {Internet Things},
	volume = {21},
	pages = {100648},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100648},
	doi = {10.1016/J.IOT.2022.100648},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/CaiSSGD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Missing traffic data collected by IoT sensors is a common issue. Having complete traffic data can help people with their studies and work in real world. A spatio-temporal enhanced\nk\nnearest neighbor (ST-KNN) method is proposed in this paper to interpolate missing traffic data according to its corresponding spatio-temporal dependence. The proposed method is improved in three aspects: initially, localized data are involved in the computation, the distance metric formula is re-designed secondly, and the data regression model is improved. We conducted our experimental evaluations on publicly available real dataset, and the results are compared to those from existing state-of-the-art models. The results of our experiments show that the method proposed in this paper can effectively improve traffic data interpolation accuracy.}
}


@article{DBLP:journals/iot/ZhuXCTDZHMCYYI23,
	author = {Dingju Zhu and
                  Lianzi Xie and
                  Bingxu Chen and
                  Jianbin Tan and
                  Renfeng Deng and
                  Yongzhi Zheng and
                  Qi Hu and
                  Rashed Mustafa and
                  Wanshan Chen and
                  Shuai Yi and
                  Kai{-}Leung Yung and
                  Andrew W. H. Ip},
	title = {Knowledge graph and deep learning based pest detection and identification
                  system for fruit quality},
	journal = {Internet Things},
	volume = {21},
	pages = {100649},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100649},
	doi = {10.1016/J.IOT.2022.100649},
	timestamp = {Sat, 13 May 2023 01:06:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ZhuXCTDZHMCYYI23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fruit usually plays a vital role in people's daily life. Many kinds of fruits are rich in vitamins and trace elements, which have high edible value. Pests and diseases are a considerable problem in the process of fruit planting. The quality and quantity of fruit can be effectively improved by the detection and preventing pests and diseases. However, suppose in the process of fruit growth, it is always necessary to manually identify and detect pests and diseases. In that case, it will inevitably consume a lot of workforce and material resources. Therefore, it is advisable to have an automated system to save unnecessary time and effort. This article introduces the detection and identification system of pests and diseases based on Raspberry Pi to identify and detect the pests and diseases of fruit such as Longan and lychee. Firstly, we constructed a knowledge graph of pests and diseases related to lychee and longan. Then, we used the Raspberry Pi to control the camera to capture the pests and diseases images. Next, the system processed and recognized the images captured by the camera. Finally, the Bluetooth speaker broadcasted the results in real-time. We constructed the knowledge graph through data collection, information extraction, knowledge fusion and storage. We trained the vgg-16 model, which achieves 94.9% accuracy in the pests identification task, and we deployed it on a Raspberry Pi.}
}


@article{DBLP:journals/iot/YuXH23,
	author = {Sheng Yu and
                  Li Xie and
                  Qilei Huang},
	title = {Inception convolutional vision transformers for plant disease identification},
	journal = {Internet Things},
	volume = {21},
	pages = {100650},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100650},
	doi = {10.1016/J.IOT.2022.100650},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/YuXH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Plant disease has a considerable influence on the safety of grain output and quality. Therefore, it is crucial to detect and diagnose plant diseases. Most plant diseases are reflected in plant leaves, and accurate identification of plant diseases require specialized knowledge. Generally, it is difficult for farmers to accurately identify plant disease. Consequently, accurate and timely automatic recognition of plant disease is extremely needed in the area of precision agricultural. To solving this challenge, many classical computer vision and deep learning-based research models have been proposed. Due to the successful performance, deep learning became the preferred approach for plant disease identification. We design a novel transformer block by use of transformer architecture to model long-range features, and of soft split token embedding to capture local information from surrounding pixels and patches, in this paper. Furthermore, inception architecture and cross channel feature learning can improve the information richness, which is especially beneficial to fine-grained feature learning. The proposed model obtains higher accuracy than previous convolution and vision transformer-based models, achieves 99.94% accuracy on VillagePlant, 99.22% accuracy on ibean, 86.89% accuracy on AI2018, and 77.54% accuracy on PlantDoc. The experiment results show its preponderance over the existing models.}
}


@article{DBLP:journals/iot/DreibholzM23,
	author = {Thomas Dreibholz and
                  Somnath Mazumdar},
	title = {Towards a lightweight task scheduling framework for cloud and edge
                  platform},
	journal = {Internet Things},
	volume = {21},
	pages = {100651},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100651},
	doi = {10.1016/J.IOT.2022.100651},
	timestamp = {Sat, 13 May 2023 01:06:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/DreibholzM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile devices are becoming ubiquitous in our daily lives, but they have limited computational capacity. Thanks to the advancement in the network infrastructure, task offloading from resource-constrained devices to the near edge and the cloud becomes possible and advantageous. Complete task offloading is now possible to almost limitless computing resources of public cloud platforms. Generally, the edge computing resources support latency-sensitive applications with limited computing resources, while the cloud supports latency-tolerant applications. This paper proposes one lightweight task-scheduling framework from cloud service provider perspective, for applications using both cloud and edge platforms. Here, the challenge is using edge and cloud resources efficiently when necessary. Such decisions have to be made quickly, with a small management overhead. Our framework aims at solving two research questions. They are: (i) How to distribute tasks to the edge resource pools and multi-clouds? (ii) How to manage these resource pools effectively with low overheads? To answer these two questions, we examine the performance of our proposed framework based on Reliable Server Pooling (RSerPool). We have shown via simulations that RSerPool, with the correct usage and configuration of pool member selection policies, can accomplish the cloud/edge setup resource selection task with a small overhead.}
}


@article{DBLP:journals/iot/SulimanKMODA23,
	author = {Ahmed Suliman and
                  Maha Kadadha and
                  Rabeb Mizouni and
                  Hadi Otrok and
                  Ernesto Damiani and
                  Mahmoud Al{-}Qutayri},
	title = {Blockcheck: {A} consortium blockchain-based conformance checking framework
                  for business processes},
	journal = {Internet Things},
	volume = {21},
	pages = {100652},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100652},
	doi = {10.1016/J.IOT.2022.100652},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/SulimanKMODA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the problem of distributed, multi-perspective conformance checking for Business Process Model and Notation (BPMN) is addressed. Traditionally, conformance checking has been performed centrally by a trusted entity, however that may not be applicable in the case of collaborative processes between multiple organizations. Consortium Blockchain has been adopted to overcome the need for a trusted entity and foster transparency, accountability, and workflow. Existing blockchain-based conformance checking frameworks focus on translating BPMN models into smart contracts, however they do not support complex BPMN constructs, which limits their applicability in real-life collaborations. In this paper, distributed, multi-perspective conformance checking framework is proposed. This framework supports complex BPMN constructs such as Timers or Boundary processes running on a Consortium blockchain. The proposed framework includes: (1) A process to extract Process Conformance Rules from BPMN models, and (2) A blockchain-based technique to perform checking of the extracted rules to determine process conformance. Furthermore, it can be used by businesses in both after-the-fact checking or during process execution in a transparent and trusted manner. Evaluation of the framework implemented over Quorum conducted with real-life logs proves the effectiveness of the proposed framework in terms of detecting various violation types in a traceable and transparent manner.}
}


@article{DBLP:journals/iot/PerezSZL23,
	author = {Alfredo J. Perez and
                  Farhan Siddiqui and
                  Sherali Zeadally and
                  Derek Lane},
	title = {A review of IoT systems to enable independence for the elderly and
                  disabled individuals},
	journal = {Internet Things},
	volume = {21},
	pages = {100653},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100653},
	doi = {10.1016/J.IOT.2022.100653},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/PerezSZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed an increase in human life expectancy fueled by significant improvements in infrastructure, healthcare, and economies across the globe. Longer life spans have altered the world demographics resulting in a larger senior population compared with previous years. This trend has created the need for providing additional care and assistive services to support the aging individuals. Innovative assistance techniques are especially necessary for elderly people who live on their own in their homes. Simultaneously, an explosive growth in IoT gadgets such as sensors and actuators have accelerated the development of smart homes which comprise various types of IoT systems that provide increased convenience to people with degenerating physical and cognitive abilities. Common examples of IoT systems that are being integrated into smart homes include home automation systems, home activity detectors, wearable sensor technologies for remote health management and so on. We identify the common needs of aging and impaired individuals and then we review several IoT applications that can provide the required support. We further discuss some of the challenges that must be addressed to make these IoT systems more practical and reliable for everyday use.}
}


@article{DBLP:journals/iot/ShakilKPLLHB23,
	author = {Ali Shakil and
                  Mohammad Ali Khalighi and
                  Pierre Pudlo and
                  Cyril Leclerc and
                  Dominique Laplace and
                  Fran{\c{c}}ois Hamon and
                  Alexandre Boudonne},
	title = {Outlier detection in non-stationary time series applied to sewer network
                  monitoring},
	journal = {Internet Things},
	volume = {21},
	pages = {100654},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100654},
	doi = {10.1016/J.IOT.2022.100654},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ShakilKPLLHB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the case of data processing for a sewer infrastructure where water drains are equipped with waste-level sensors, which frequently send the related data to a data processing unit. In order to understand the dynamics of waste accumulation within the whole drain network, the collected data should first be pre-processed by removing the unreliable (or, in other words, noisy) measurements. As we show, the evolution of the waste inside a drain can be modeled by a non-stationary discontinuous time series model. Due to the chaotic aspect of the waste and the hostile conditions under which the sensor should operate, the observed time series can include outliers in the form of peaks, which should be removed from the raw data prior to any data processing. This paper proposes an efficient data cleaning algorithm that makes a good compromise between computational complexity and performance. This latter is evaluated in terms of the probabilities of peak detection (i.e., detecting actual outliers) and false detection (i.e., incorrectly denoting measurements as outliers). A trade-off between these two criteria should be made by setting appropriately the detection threshold (which, in the proposed method, does not depend on the mean or variance of the data). For instance, for a threshold of 2.5, the algorithm provides a correct outlier detection probability of 0.85 and a false detection probability of\n. The efficiency of the proposed algorithm is demonstrated by applying it to real measurement data.}
}


@article{DBLP:journals/iot/DibalOZNOSM23,
	author = {Peter Yusuf Dibal and
                  Elizabeth N. Onwuka and
                  Suleiman Zubair and
                  Emmanuel I. Nwankwo and
                  S. A. Okoh and
                  Bala Alhaji Salihu and
                  H. B. Mustaphab},
	title = {Processor power and energy consumption estimation techniques in IoT
                  applications: {A} review},
	journal = {Internet Things},
	volume = {21},
	pages = {100655},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100655},
	doi = {10.1016/J.IOT.2022.100655},
	timestamp = {Sat, 03 Jun 2023 13:54:55 +0200},
	biburl = {https://dblp.org/rec/journals/iot/DibalOZNOSM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The energy efficiency of IoT nodes remains the dominant factor for effective IoT solutions that will meet the challenges of the 21st century, especially in the drive towards a carbon-neutral world through net-zero targets. Microprocessors/microcontrollers are devices that perform entire operations of IoT devices. Therefore, the power and energy consumption of these processors directly reflects the power consumed by the IoT devices they drive. An accurate estimation of the power and energy consumption of the processors is vital for the development of energy-efficient IoT solutions because IoT devices are designed to operate in remote locations for long periods without human intervention. It is against this backdrop that this paper which is expected to serve as a guide for researches and IoT node/application developers in selecting the best technique for an IoT use-case, presents a review of processor power and energy consumption estimation techniques starting from the lowest level of abstraction to the highest level of abstraction. The review involves a detailed discussion of estimation technique methodologies for an abstraction level, and where applicable, generalized methodologies which cover the most approach used for an abstraction level are covered. The existence of overlaps and the impact of processor duty cycles on the techniques were discussed. A comparison of the strengths and weaknesses of each technique was made, from where register-transfer level and instruction level techniques are shown to be resilient against errors that occur from poor input signal conditioning. Future directions for the development of estimation techniques are also presented as recommendation.}
}


@article{DBLP:journals/iot/AbusittaCWHFA23,
	author = {Adel Abusitta and
                  Glaucio H. S. Carvalho and
                  Omar Abdel Wahab and
                  Talal Halabi and
                  Benjamin C. M. Fung and
                  Saja Al{-}Mamoori},
	title = {Deep learning-enabled anomaly detection for IoT systems},
	journal = {Internet Things},
	volume = {21},
	pages = {100656},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100656},
	doi = {10.1016/J.IOT.2022.100656},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/AbusittaCWHFA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) systems have become an intrinsic technology in various industries and government services. Unfortunately, IoT devices and networks are known to be highly vulnerable to security attacks that target data integrity and service availability. Moreover, the heterogeneity of the data collected from various IoT devices, together with the disturbances incurred within the IoT system, render the detection of anomalous behavior and compromised nodes more challenging compared to traditional Information Technology (IT) networks. As a result, there is a pressing need for effective and reliable anomaly detection to identify malicious data to guarantee that they will not be used in IoT-driven decision support systems. In this paper, we propose a deep learning-powered anomaly detection for IoT that can learn and capture robust and useful features, which cannot be significantly affected by unstable environments. These features are then used by the classifier to enhance the accuracy of detecting malicious IoT data. More specifically, the proposed deep learning model is designed based on a denoising autoencoder, which is adopted to obtain features that are robust against the heterogeneous environment of IoT. Experimental results based on real-life IoT datasets show the effectiveness of the proposed framework in terms of enhancing the accuracy of detecting malicious data compared to the state-of-the-art IoT-based anomaly detection models.}
}


@article{DBLP:journals/iot/PutraPZKL23,
	author = {Made Adi Paramartha Putra and
                  Adinda Riztia Putri and
                  Ahmad Zainudin and
                  Dong{-}Seong Kim and
                  Jae{-}Min Lee},
	title = {{ACS:} Accuracy-based client selection mechanism for federated industrial
                  IoT},
	journal = {Internet Things},
	volume = {21},
	pages = {100657},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100657},
	doi = {10.1016/J.IOT.2022.100657},
	timestamp = {Sat, 30 Sep 2023 10:16:54 +0200},
	biburl = {https://dblp.org/rec/journals/iot/PutraPZKL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study proposes secure federated learning (FL)-based architecture for the industrial internet of things (IIoT) with a novel client selection mechanism to enhance the learning performance. In order to secure the FL architecture and ensure that available clients are trustworthy, a certificate authority (CA) is adopted. In traditional FL, an aggregation technique known as federated averaging (FedAvg) is utilized to collect local model parameters by selecting a random subset of clients for the training process. However, the random selection may lead to uncertainty and negatively influence the overall FL performance. Moreover, state-of-the-art studies on client selection mainly rely on client’s additional information, which raises a privacy issue. Therefore, a novel client selection mechanism based on client evaluation accuracy called ACS is introduced in this work to improve FL performance while preserving client privacy. Unlike other client selection methods, ACS relies only on the updated local parameter, which is evaluated in the FL server. The proposed ACS considers the highest-performing clients to fasten the convergence time in the FL. Based on the extensive performance evaluation performed in this work using MNIST and F-MNIST datasets with non-independent identically distributed (non-IID) conditions, the adoption of ACS successfully improved the overall performance of FL in terms of accuracy and F1-score with an average of 4.62%. Furthermore, comparative analysis shows that the proposed ACS can achieve specific accuracy with 2.29% lower communication rounds and stable performance compared to other client selection mechanisms.}
}


@article{DBLP:journals/iot/SharmaDE23,
	author = {Rashmi Priya Sharma and
                  Ramesh Dharavath and
                  Damodar Reddy Edla},
	title = {IoFT-FIS: Internet of farm things based prediction for crop pest infestation
                  using optimized fuzzy inference system},
	journal = {Internet Things},
	volume = {21},
	pages = {100658},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100658},
	doi = {10.1016/J.IOT.2022.100658},
	timestamp = {Mon, 05 Feb 2024 20:22:24 +0100},
	biburl = {https://dblp.org/rec/journals/iot/SharmaDE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advanced farming techniques help to know the appropriate environmental conditions, soil quality, water and fertilizer needs, and crop monitoring during each plant’s growth phase, resulting in higher yields. Also, IoT-compatible crop monitoring and data collection systems can help identify crop diseases and the breeding status of pathogenic pests. Extracting data from samples collected from the complete crop growth cycle acknowledged a plausible relationship between weather parameters, crop yield, and insect reproduction. Data analysis done on the data collected through an IoT monitoring system helped determine the environmental factors supporting the higher pest breeding conditions. The proposed fuzzy inference system’s knowledge base is designed using these weather parameters. The multi-objective evolutionary algorithm uses fuzzy rules to find suitable cropping window and low pest breeding conditions. This proposal identifies crop-sowing windows based on fuzzy logic with maximum crop yield and minimum pest growth by deploying IEEE 802.15.4 wireless IoT-enabled sensor network monitoring infrastructure in medium grass vegetation. Experiments are being done on rice and Sugarcane crops. The experiments were conducted in the agriculture field of Gwalior, Madhya Pradesh, India. The soil moisture, rainfall, temperature, etc., data were collected using the wireless sensor network deployed in the field. Fuzzy logic-based identification of appropriate planting seasons by IoT application development services helps farmers prevent the development of pests and proactively take precautions to achieve maximum yields.}
}


@article{DBLP:journals/iot/KoutayniRS23,
	author = {Mhd Rashed Al Koutayni and
                  Gerd Reis and
                  Didier Stricker},
	title = {DeepEdgeSoC: End-to-end deep learning framework for edge IoT devices},
	journal = {Internet Things},
	volume = {21},
	pages = {100665},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100665},
	doi = {10.1016/J.IOT.2022.100665},
	timestamp = {Sat, 13 May 2023 01:06:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/KoutayniRS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The acceleration of deep neural networks (DNNs) on edge devices is gaining significant importance in various application domains. General purpose graphics processing units (GPGPUs) are typically used to explore, train and evaluate DNNs because they offer higher processing and computational capability compared to CPUs. However, this comes at the cost of increased power consumption required by these devices for operation, which prevents efficient deployment of networks on edge devices. In the Internet of Things (IoT) domain, Field programmable gate arrays (FPGAs) are considered a powerful alternative since their flexible architecture can run the DNNs with much less energy. The enormous amount of effort and time required for the entire end-to-end edge-aware deployment urged us to develop DeepEdgeSoc, an integrated framework for deep learning (DL) design and acceleration. DeepEdgeSoc is an overarching framework under which DNNs can be built. DeepGUI, a visual drag-and-drop DNN design environment, plays an important role in accelerating the network design phase. In DeepEdgeSoc, the networks can be quantized and compressed to suite the underlying edge devices in terms of size and energy. DeepEdgeSoc goes beyond the software level by converting the networks to appropriate FPGA implementations that can be directly synthesized and integrated within a System-on-Chip (SoC).}
}


@article{DBLP:journals/iot/OmenaSPV23,
	author = {R{\^{o}}mulo A. L. V. de Omena and
                  Danilo F. S. Santos and
                  Angelo Perkusich and
                  Dalton C{\'{e}}zane Gomes Valadares},
	title = {Two-tier {MPC} architecture for AGVs navigation assisted by edge computing
                  in an industrial scenario},
	journal = {Internet Things},
	volume = {21},
	pages = {100666},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100666},
	doi = {10.1016/J.IOT.2022.100666},
	timestamp = {Tue, 18 Apr 2023 16:41:52 +0200},
	biburl = {https://dblp.org/rec/journals/iot/OmenaSPV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of mobile networks may be a bridge to move industrial control to edge computing. Embedded controllers physically installed in the industrial process can be replaced by software remotely executed as a service. In this context, Automated-Guided Vehicles (AGVs) are industrial agents that can benefit from this scenario by offloading computing-intensive tasks remotely to the edge. Additionally, an AGV control system based on remote software promotes flexibility, an essential requirement for Industry 4.0. Considering the remote control of AGV, the mobile robot may traverse areas where the signal is degraded, increasing the risks of collisions and accidents due to connection failures. Previous works suggest the adoption of Model Predictive Control (MPC) to control mobile robots in the occurrence of delays and packet losses. In this article, we propose a two-tier architecture of MPCs, one executed at the edge to plan the trajectory of multiple AGVs globally and the other executed individually in each AGV to keep it on the planned track. In simulations performed in an edge computing environment using a robot simulator, the vehicles follow the planned trajectory even with network degradation. In our simulated scenarios with delays and packet loss, predicted actions planned by the MPC on the edge avoided collisions between the vehicles.}
}


@article{DBLP:journals/iot/IftikharATCXGU23,
	author = {Sundas Iftikhar and
                  Mirza Mohammad Mufleh Ahmad and
                  Shreshth Tuli and
                  Deepraj Chowdhury and
                  Minxian Xu and
                  Sukhpal Singh Gill and
                  Steve Uhlig},
	title = {HunterPlus: {AI} based energy-efficient task scheduling for cloud-fog
                  computing environments},
	journal = {Internet Things},
	volume = {21},
	pages = {100667},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100667},
	doi = {10.1016/J.IOT.2022.100667},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/IftikharATCXGU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud computing is a mainstay of modern technology, offering cost-effective and scalable solutions to a variety of different problems. The massive shift of organization resource needs from local systems to cloud-based systems has greatly increased the costs incurred by cloud providers in expanding, maintaining, and supplying server, storage, network, and processing hardware. Due to the large scale at which cloud providers operate, even small performance degradation issues can cause energy or resource usage costs to rise dramatically. One way in which cloud providers may improve cost reduction is by reducing energy consumption. The use of intelligent task-scheduling algorithms to allocate user-deployed jobs to servers can reduce the amount of energy consumed. Conventional task scheduling algorithms involve both heuristic and metaheuristic methods. Recently, the application of Artificial Intelligence (AI) to optimize task scheduling has seen significant progress, including the Gated Graph Convolution Network (GGCN). This paper proposes a new approach called HunterPlus which examine the effect of extending the GGCN’s Gated Recurrent Unit to a Bidirectional Gated Recurrent Unit. The paper also studies the utilization of Convolutional Neural Networks (CNNs) in optimizing cloud–fog task scheduling. Experimental results show that the CNN scheduler outperforms the GGCN-based models in both energy consumption per task and job completion rate metrics by at least 17 and 10.4 percent, respectively.}
}


@article{DBLP:journals/iot/SotengaDK23,
	author = {Prosper Zanu Sotenga and
                  Karim Djouani and
                  Anish Mathew Kurien},
	title = {A virtual network model for gateway media access control virtualisation
                  in Large Scale Internet of Things},
	journal = {Internet Things},
	volume = {21},
	pages = {100668},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100668},
	doi = {10.1016/J.IOT.2022.100668},
	timestamp = {Tue, 13 Jun 2023 23:08:55 +0200},
	biburl = {https://dblp.org/rec/journals/iot/SotengaDK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent approaches, technologies and standards have been developed to address scalability in very large IoT networks with an emphasis on the Media Access Control (MAC) component. One of the best strategies to manage media access for a large number of devices is to provide concurrent media access. This can be achieved through the concept of Radio Frequency (RF) virtualisation. However, to support such strategies in a Large Scale Internet of Things (LS-IoT) network, the Gateway’s (GW) MAC functions need to equally manage transmissions concurrently otherwise congestion is imminent due to the resource constraints of the GW node. In this work, a virtual media access approach using virtual Access Groups (vAG) for supporting the scalability of Large-Scale IoT (LS-IoT) networks over a virtual network framework is proposed. The virtual network and the impact on the MAC throughput are modelled analytically using the Queuing Network Analyser (QNA) method and the unsaturated throughput model for the IEEE 802.11ah standard. The impact of managing large devices using vAGs for a resource-constrained Gateway MAC is also modelled and the results are compared and analysed. The results obtained suggest that the throughput can be improved for a large number of devices based on vAGs when the GW MAC is virtualised using the proposed framework as opposed to using a dedicated localised GW MAC component.}
}


@article{DBLP:journals/iot/ChaudharySVS23,
	author = {Dharminder Chaudhary and
                  Tanmay Soni and
                  Kondeti Lakshmi Vasudev and
                  Kashif Saleem},
	title = {A modified lightweight authenticated key agreement protocol for Internet
                  of Drones},
	journal = {Internet Things},
	volume = {21},
	pages = {100669},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100669},
	doi = {10.1016/J.IOT.2022.100669},
	timestamp = {Mon, 01 May 2023 13:02:02 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ChaudharySVS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones can be embedded in the system Internet of Drones can make a military observation of a region, a transport system embedded with sensors. Drones played a highly important role in different fields and brought great convenience to production and lifestyle. But we know all the data collected by sensors embedded in drones suffer from security challenges and privacy issues. There are a lot of authenticated key agreement protocols for the security of transmitted data through drones. We have analyzed a recent protocol, “A lightweight authentication and key agreement scheme for Internet of Drones” by Zhang et al. in the year 2020. We have found that this protocol is not secure against stolen smart card attacks, and the control server stores extra data. We have tried to address security issues of password guessing, anonymity, user/server impersonation, insider attack, and stolen smart card attacks, and man in the middle attack. We have proposed a modified lightweight authenticated key agreement protocol for Internet of Drones. We have proved its security in the random oracle model. We have done a performance analysis of the proposed protocol with relevant protocols that ensures its efficiency and security as well.}
}


@article{DBLP:journals/iot/TekinAAUG23,
	author = {Nazli Tekin and
                  Abbas Acar and
                  Ahmet Aris and
                  A. Selcuk Uluagac and
                  Vehbi Cagri Gungor},
	title = {Energy consumption of on-device machine learning models for IoT intrusion
                  detection},
	journal = {Internet Things},
	volume = {21},
	pages = {100670},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100670},
	doi = {10.1016/J.IOT.2022.100670},
	timestamp = {Mon, 01 May 2023 13:02:02 +0200},
	biburl = {https://dblp.org/rec/journals/iot/TekinAAUG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Smart Home Systems (SHSs) have gained enormous popularity with the rapid development of the Internet of Things (IoT) technologies. Besides offering many tangible benefits, SHSs are vulnerable to attacks that lead to security and privacy concerns for SHS users. Machine learning (ML)-based Intrusion Detection Systems (IDS) are proposed to address such concerns. Conventionally, ML models are trained and tested on computationally powerful platforms such as cloud services. Nevertheless, the data shared with the cloud is vulnerable to privacy attacks and causes latency, which decreases the performance of real-time applications like intrusion detection systems. Therefore, on-device ML models, in which the user data is kept locally, have emerged as promising solutions to ensure the security and privacy of the data for real-time applications. However, performing ML tasks requires high energy consumption. To the best of our knowledge, no study has been conducted to analyze the energy consumption of ML-based IDS. Therefore, in this paper, we perform a comparative analysis of on-device ML algorithms in terms of energy consumption for IoT intrusion detection applications. For a thorough analysis, we study the training and inference phases separately. For training, we compare the cloud computing-based ML, edge computing-based ML, and IoT device-based ML approaches. For the inference, we evaluate the TinyML approach to run the ML algorithms on tiny IoT devices such as Micro Controller Units (MCUs). Comparative performance evaluations show that deploying the Decision Tree (DT) algorithm on-device gives better results in terms of training time, inference time, and power consumption.}
}


@article{DBLP:journals/iot/ChandrappaRAS23,
	author = {Varun Yarehalli Chandrappa and
                  Biplob Ray and
                  Nanjappa Ashwatha and
                  Pramod Shrestha},
	title = {Spatiotemporal modeling to predict soil moisture for sustainable smart
                  irrigation},
	journal = {Internet Things},
	volume = {21},
	pages = {100671},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100671},
	doi = {10.1016/J.IOT.2022.100671},
	timestamp = {Tue, 18 Apr 2023 16:41:52 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ChandrappaRAS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we have proposed spatiotemporal soil moisture modeling to improve understanding of soil moisture and variability for sustainable irrigation practices. Increasing population growth and climatic changes have intensified the need to implement effective measures to conserve water in irrigation practices. Effective irrigation in parkland influences the overall plant growth processes including the final appearance of the plants. Furthermore, over-irrigation and poor water management in parks and gardens lead to wastage of water which may result in seepage, runoff, and leaching of nutrients into nearby streams. Conversely, under-irrigation results in reduced plant growth and unappealing appearance. Thus, appropriate irrigation management is required to maintain a lush green landscape. Due to large variability in soil properties, environmental conditions and landscape features, the soil moisture level might not be uniformly distributed within a given landscape. Hence, it is essential to understand the soil moisture distribution in the field. There are few existing soil moisture modeling exist but these modelings have not considered multiple vertical depth and time domain. Using Internet of Things(IoT) enabled Cyber–physical system and machine learning techniques, this paper has presented a soil moisture modeling for multiple vertical soil depth for a robust understanding of soil’s and plants need for sustainable sprinkling of the water. The proposed model has used both statistical and deep learning techniques to understand moisture variability in both vertical depth and time. The proposed model has used Correlation Index (CI) to understand multi depth moisture variability on the influence of soil types and local weather parameters, such as precipitation and temperature. To predict moisture data in multi depth, the model has evaluated the use of both statistical machine learning models, such as Support vector regression(SVR) and linear regression (LR), and deep learning models, such as Long Short-Term Memory (LSTM) using seasonal and non-seasonal dataset. The experiment has revealed closely related variability pattern between wind speed and soil moisture in multi depth whereas soil type shows a loosely related variability pattern with moisture in higher depth. With these diverse datasets, the proposed machine learning model has achieved almost 90% success rate to predict moisture content in higher depth using data of lower depth to reduce cumbersome sensors array deployment in higher depth for live moisture visualization. This proposed multiple depth and time domain-based model can enable a smart water dispersing system to conserve water and make a positive contribution to a sustainable future.}
}


@article{DBLP:journals/iot/RaniGHS23,
	author = {Shalli Rani and
                  Divya Gupta and
                  Norbert Herencsar and
                  Gautam Srivastava},
	title = {Blockchain-enabled cooperative computing strategy for resource sharing
                  in fog networks},
	journal = {Internet Things},
	volume = {21},
	pages = {100672},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100672},
	doi = {10.1016/J.IOT.2022.100672},
	timestamp = {Mon, 28 Aug 2023 21:41:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/RaniGHS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Several heterogeneous network designs have been developed to fulfill user experience needs, driven by widely growing applications needing large data processing, such as Industry 4.0. Fog computing is one of them since it allows for the efficient integration and exploitation of ubiquitous computing resources. Willingness and service billing concerns become important in fog computing situations for computing resource sharing. The present fog systems, on the other hand, are vulnerable to malicious attacks. The integration of blockchain technology into the fog computing environment is characterized by successfully allowing consensus in an untrustworthy environment. However, creating public blockchains requires a lot of processing power, which can quickly deplete the computational resources of IoT-enabled Smart Machines (SM) in Industry 4.0. This study suggests partitioning the fog system into fog clusters (FC), with fog nodes (FN) in each cluster sharing the same access control list (ACL) that is protected by a public blockchain. Therefore, Blockchain-enabled Resource Sharing inside SM in Fog networks (B-RSSF), a novel architecture for computing resource sharing in fog networks is proposed. B-RSSF has specified physical architecture, design principles, and inner workings. To make full and effective use of ubiquitous computing resources, fog computing’s wireless features and blockchain technology are tightly integrated. In addition, an enhanced PoW consensus mechanism for reaching consensus in an untrustworthy fog environment has been discussed. Finally, the experimental results have been conducted to evaluate the performance of the proposed scheme over other existing schemes in a fog-based environment.}
}


@article{DBLP:journals/iot/AnguloCP23,
	author = {Cecilio Angulo and
                  Alejandro Chac{\'{o}}n and
                  Pere Ponsa},
	title = {Towards a cognitive assistant supporting human operators in the Artificial
                  Intelligence of Things},
	journal = {Internet Things},
	volume = {21},
	pages = {100673},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100673},
	doi = {10.1016/J.IOT.2022.100673},
	timestamp = {Sat, 30 Sep 2023 10:16:53 +0200},
	biburl = {https://dblp.org/rec/journals/iot/AnguloCP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) systems are becoming increasingly complex due to heterogeneity of devices and requirements for real-time processing and decision making. In this context, Artificial Intelligence (AI) technologies provide powerful capabilities for endowing IoT devices with intelligent services, leading to the so-called Artificial Intelligence of Things (AIoT). The operator is in the middle of this complexity, trying to understand the situation and make effective real-time decisions. Hence, human factors, especially cognitive ones, are a major issue to be addressed. The human cognitive part must be framed together with intelligent artefacts, requiring a systematic approach in the domain of joint cognitive systems. New software development methods in the form of assistants and wizards are necessary to help operators to be context-aware and reduce their technical workload regarding coding or computer-oriented skills, focusing on the task or service at hand. Building on previous research on the role of the human worker in an AIoT environment, this article analyses the described situation in terms of human cyber–physical systems, with the aim of proposing a conceptual framework for these assistance systems at the cognitive level. Two illustrative examples are described to validate the effectiveness of the proposed framework in collaborative tasks.}
}


@article{DBLP:journals/iot/IftikharGSXATDWGCGKACVRDU23,
	author = {Sundas Iftikhar and
                  Sukhpal Singh Gill and
                  Chenghao Song and
                  Minxian Xu and
                  Mohammad Sadegh Aslanpour and
                  Adel Nadjaran Toosi and
                  Junhui Du and
                  Huaming Wu and
                  Shreya Ghosh and
                  Deepraj Chowdhury and
                  Muhammed Golec and
                  Mohit Kumar and
                  Ahmed M. Abdelmoniem and
                  F{\'{e}}lix Cuadrado and
                  Blesson Varghese and
                  Omer F. Rana and
                  Schahram Dustdar and
                  Steve Uhlig},
	title = {AI-based fog and edge computing: {A} systematic review, taxonomy and
                  future directions},
	journal = {Internet Things},
	volume = {21},
	pages = {100674},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100674},
	doi = {10.1016/J.IOT.2022.100674},
	timestamp = {Tue, 07 May 2024 20:25:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/IftikharGSXATDWGCGKACVRDU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource management in computing is a very challenging problem that involves making sequential decisions. Resource limitations, resource heterogeneity, dynamic and diverse nature of workload, and the unpredictability of fog/edge computing environments have made resource management even more challenging to be considered in the fog landscape. Recently Artificial Intelligence (AI) and Machine Learning (ML) based solutions are adopted to solve this problem. AI/ML methods with the capability to make sequential decisions like reinforcement learning seem most promising for these type of problems. But these algorithms come with their own challenges such as high variance, explainability, and online training. The continuously changing fog/edge environment dynamics require solutions that learn online, adopting changing computing environment. In this paper, we used standard review methodology to conduct this Systematic Literature Review (SLR) to analyze the role of AI/ML algorithms and the challenges in the applicability of these algorithms for resource management in fog/edge computing environments. Further, various machine learning, deep learning and reinforcement learning techniques for edge AI management have been discussed. Furthermore, we have presented the background and current status of AI/ML-based Fog/Edge Computing. Moreover, a taxonomy of AI/ML-based resource management techniques for fog/edge computing has been proposed and compared the existing techniques based on the proposed taxonomy. Finally, open challenges and promising future research directions have been identified and discussed in the area of AI/ML-based fog/edge computing.}
}


@article{DBLP:journals/iot/TzavarasMP23,
	author = {Aimilios Tzavaras and
                  Nikolaos Mainas and
                  Euripides G. M. Petrakis},
	title = {OpenAPI framework for the Web of Things},
	journal = {Internet Things},
	volume = {21},
	pages = {100675},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100675},
	doi = {10.1016/J.IOT.2022.100675},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/TzavarasMP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Web of Things (WoT) Architecture recommendation of W3C defines a framework for integrating Things (e.g. devices) into the Web. It establishes an information representation of Things based on JSON-LD and RESTful API interfaces to enable access to Things. Leveraging the latest results on hypermedia construction and documentation of RESTful APIs, we argue that OpenAPI can also be used to provide documentation of Things that adheres to the W3C Architecture recommendation. OpenAPI is a widely accepted industry standard endorsed by prominent industry vendors and user communities. Therefore, it is mature and can become acceptable. An essential part of the approach is a mechanism that facilitates the generation of OpenAPI Thing Descriptions and a Web proxy that implements access to Things on the Web. An implementation of the API is also proposed and evaluated. Last but not least, OpenAPI achieves uniformity of representation of both types of WoT entities (i.e. Web services and Things) that co-exist and interact with each other in an application. This has a positive impact on the way WoT applications are designed and implemented.}
}


@article{DBLP:journals/iot/AhakonyeNLK23,
	author = {Love Allen Chijioke Ahakonye and
                  Cosmas Ifeanyi Nwakanma and
                  Jae{-}Min Lee and
                  Dong{-}Seong Kim},
	title = {{SCADA} intrusion detection scheme exploiting the fusion of modified
                  decision tree and Chi-square feature selection},
	journal = {Internet Things},
	volume = {21},
	pages = {100676},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100676},
	doi = {10.1016/J.IOT.2022.100676},
	timestamp = {Mon, 05 Feb 2024 20:22:24 +0100},
	biburl = {https://dblp.org/rec/journals/iot/AhakonyeNLK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The industrial internet of things (IIoT) and supervisory control and data acquisition (SCADA) have experienced ubiquitous growth recently. This growth comes with the challenge of an increased number of unusual attacks constituting threats. The existence and effect of intruders and their innovative attack techniques are rising. Although the existing intrusion detection systems (IDS) safeguard the networks, they have been computationally expensive. In real-time domains, available methods lag, necessitating additional research into effective feature extraction schemes with time exigency. An IDS with a fused feature selection (FS) approach for detecting and classifying attacks in a real-time SCADA network is imperative. It is to enable the resolution of computationally complex vulnerability detection schemes. The proposed technique is in three (3) phases: (a) data preparation which involves data cleansing and normalization, and (b) a fused feature selection approach built to obtain an optimal subset of features using Chi-square. (c) deployment of the modified decision tree (MDT) for anomaly detection and classification. Lastly, the reliability of the proposed model was validated, demonstrating suitability in precisely detecting abnormalities while minimizing computational time. This improvement enables adaptability for the IDS deployment scheme in a real-time situation, which could be in the control center. The validation results reveal that when the proposed chi-square-based (fused) feature extraction is employed, it performs optimally to other FS techniques and ML classifiers, compared across four (4) publicly available datasets. Cohen’s kappa coefficient (CKC) further validates the proposed model’s reliability. Further demonstrating the experimental results with recourse to false positive rates (FPR), the Mathews correlation coefficient (MCC) was employed. It also shows the resilience of the proposed model performance on an imbalanced dataset validating its suitability in real scenarios.}
}


@article{DBLP:journals/iot/DwivediS23,
	author = {Ashutosh Dhar Dwivedi and
                  Gautam Srivastava},
	title = {Security analysis of lightweight IoT encryption algorithms: {SIMON}
                  and {SIMECK}},
	journal = {Internet Things},
	volume = {21},
	pages = {100677},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2022.100677},
	doi = {10.1016/J.IOT.2022.100677},
	timestamp = {Sat, 13 May 2023 01:06:59 +0200},
	biburl = {https://dblp.org/rec/journals/iot/DwivediS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the differential cryptanalysis of two lightweight ciphers, namely, SIMON and SIMECK. The first cipher SIMON was presented by the U.S. National Security Agency in 2013 and the second one SIMECK was presented by researchers from the University of Waterloo, Canada in 2015. Both block ciphers belong to the lightweight encryption family that relies on the Feistel structure. The primary goal of making a differential attack is finding a high probability differential characteristic (path) for the cipher. However, finding a differential characteristic in a limited amount of time is the most challenging and take several hours, which can possibly be reduced by a heuristic approach. This paper used nested tree search-based methods to find differential paths and presented a state of art results but in reduced time and simpler framework.}
}


@article{DBLP:journals/iot/YaoGWY23,
	author = {Mengting Yao and
                  Qingqing Gan and
                  Xiaoming Wang and
                  Yuhao Yang},
	title = {A key-insulated secure multi-server authenticated key agreement protocol
                  for edge computing-based VANETs},
	journal = {Internet Things},
	volume = {21},
	pages = {100679},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100679},
	doi = {10.1016/J.IOT.2023.100679},
	timestamp = {Tue, 18 Apr 2023 16:41:52 +0200},
	biburl = {https://dblp.org/rec/journals/iot/YaoGWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authenticated key agreement is an important mechanism for the protection of data transmission in vehicular ad hoc networks (VANETs). However, in the existing authenticated key agreement protocols for VANETs, the key exposure problem has hardly been considered. Generally, the security of these protocols is based on a tamper-proof device (TPD), which stores secret parameters for vehicles. But the adversary can destroy the TPD to acquire some important information via side-channel attack. Once the secret key is exposed, the security of the entire VANET system will collapse. To resolve the above problem, we elaborate a key-insulated secure multi-server authenticated key agreement protocol for edge computing-based VANETs. In the proposed protocol, the secret key of the vehicle is periodically updated with the assistance of a delegation server. In this way, even if the key exposure in the current time period, it does not destroy the key security in earlier or later time period. Moreover, it is worth noting that our protocol has a more simplified setup phase with smaller public parameters, especially without the need to produce the master public key. Finally, the security proof and the performance analysis display the proposed scheme is secure and suitable regarding computation and communication overhead.}
}


@article{DBLP:journals/iot/KumarS23,
	author = {Prabhat Kumar and
                  S. Suresh},
	title = {DeepTransHAR: a novel clustering-based transfer learning approach
                  for recognizing the cross-domain human activities using GRUs (Gated
                  Recurrent Units) Networks},
	journal = {Internet Things},
	volume = {21},
	pages = {100681},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100681},
	doi = {10.1016/J.IOT.2023.100681},
	timestamp = {Sat, 27 May 2023 15:23:45 +0200},
	biburl = {https://dblp.org/rec/journals/iot/KumarS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning models have gained widespread acceptance in recent decades for their ability to recognize human activities. In terms of automated feature extraction capability and memory of the time-series streaming sensor data, these models beat traditional machine-learning techniques. Retraining the model as changes are made in the Human Activity Recognition (HAR) community is frequently a costly and time-consuming exercise. The research challenge arises when the model is trained on the source domain and predicted target domain due to the occurrence of 3-S (subjects, sensors, and sampling rates) conflicts. To address these research challenges, we proposed the Cross-Domain Activities Analysis (CDAA) and clustering-based DeepTransHAR model for recognizing human activities using smartphone sensor data. The CDAA technique has contributed to building the activity clusters by including the source and its target activities. The distance between the source and target domain have computed using the Wasserstein distance equation. Further, the proposed DeepTransHAR model carried out the Gated Recurrent Units (GRUs) layers. The lightweight DeepTransHAR model automatically extracts the efficient features from source sensory activity data for recognizing the target activities. We experimented with two openly accessible benchmark datasets, WISDM and KU-HAR, to assess the performance of our model. While comparing with the base RNN, LSTM, and Bi-LSTM model, the DeepTransHAR model achieved the highest average accuracy of 86.89%, F1 score of 89.94%, precision of 83.20%, recall of 90.85%, and elapsed average total training time of 18.30 s, and 55.33% saved training time.}
}


@article{DBLP:journals/iot/Said23,
	author = {Omar Said},
	title = {A bandwidth control scheme for reducing the negative impact of bottlenecks
                  in IoT environments: Simulation and performance evaluation},
	journal = {Internet Things},
	volume = {21},
	pages = {100682},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100682},
	doi = {10.1016/J.IOT.2023.100682},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/Said23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) environment comprises heterogeneous transmission channels. The statuses of these channels may change rapidly due to dynamic variations in things, data, topologies, etc. Thus, many bottlenecks may suddenly occur and dynamically change. Therefore, the quality of services (QoS) may be affected due to the lack of the bandwidth. Hence, this paper proposes a bandwidth control scheme to face the challenge of bottlenecks in the IoT environment. In this scheme, a bottleneck detection methodology, bandwidth prediction approach, reduction of bandwidth usage mechanism, and bandwidth management model are proposed. This bandwidth management model comprises reassigned and reallocated bandwidth plans. To test the proposed bandwidth control scheme, a large-scale simulation environment was constructed using NS-3. The performance of the proposed scheme was measured using the effect of reassigned and reallocated bandwidth plans in the cases of normal and prioritized data. In addition, packet loss, energy consumption, delay and bandwidth prediction accuracy were measured. Moreover, to make sure that the proposed scheme was positively effectiveness, its simulation results were compared to those of the famous machine learning and deep learning techniques: long short-term memory (LSTM), gated recurrent unit (GRU), autoregressive integrated moving average (ARIMA), multi-layer perceptron (MLP), and deep reinforcement learning (DRL). Finally, the simulation results proved that the proposed bandwidth control scheme notably outperformed the IoT environment's efficiency and limited the negative impact of the bottlenecks.}
}


@article{DBLP:journals/iot/BozorgiGYA23,
	author = {Seyed Mostafa Bozorgi and
                  Mehdi Golsorkhtabaramiri and
                  Samaneh Yazdani and
                  Sahar Adabi},
	title = {A smart optimizer approach for clustering protocol in UAV-assisted
                  IoT wireless networks},
	journal = {Internet Things},
	volume = {21},
	pages = {100683},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100683},
	doi = {10.1016/J.IOT.2023.100683},
	timestamp = {Mon, 26 Jun 2023 20:57:55 +0200},
	biburl = {https://dblp.org/rec/journals/iot/BozorgiGYA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is a network in the socio-physical space that provides a platform for large-scale data collection. In this context, Wireless Sensor Nodes (WSN) and Unmanned Aerial Vehicles (UAVs) can play an essential role in reducing costs and ease of usage. One of the critical challenges of this network is energy consumption, the main part is related to the transmission unit. In this way, the Smart Optimizer Approach (SOA) to solving global optimization, engineering problems, and clustering in UAV-Assisted IoT Wireless Networks is presented. The SOA uses a Hybrid Vector-Based Operator (HVO), a Smart Selection Search Mode (S3M), and a Smart Re-Randomize Schema (SRS). Numerical comparison and discussions are done on CEC 2017 functions and benchmark functions and engineering problems. Comparison of performance with other algorithms shows the high generality of the SOA. Finally, this paper presented a new SOA-based Clustering protocol (SOAC) for large-scale UAV-Assisted IoT Wireless Networks. In this network, UAVs are used as Air Base Station (ABS). According to this large-scale network and the UAVs' movement, traditional protocols with high control message overhead aren't suitable. Clustering is presented with the aim of reducing overhead and increasing throughput by UAVs simultaneously. SOAC uses two new mechanisms, Assistant to the Cluster Head (ACH) and discretion license (DL), which have been introduced recently. The results of the simulations show the proposed clustering has a good improvement in stability, energy/load balancing, network lifetime, and throughput.}
}


@article{DBLP:journals/iot/ChenCLALSC23,
	author = {Zigang Chen and
                  Zhiquan Cheng and
                  Wenjun Luo and
                  Jin Ao and
                  Yuhong Liu and
                  Kai Sheng and
                  Long Chen},
	title = {{FSMFA:} Efficient firmware-secure multi-factor authentication protocol
                  for IoT devices},
	journal = {Internet Things},
	volume = {21},
	pages = {100685},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100685},
	doi = {10.1016/J.IOT.2023.100685},
	timestamp = {Mon, 26 Jun 2023 20:57:55 +0200},
	biburl = {https://dblp.org/rec/journals/iot/ChenCLALSC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of 5G mobile communication technology and the continuous expansion of the scale of the Internet of Things (IoT) industry, the number of IoT devices has grown exponentially. IoT devices have uneven security guarantees due to different application scenarios, which brings great security threats to the devices themselves and the IoT system. Especially with the physical threats of the device and firmware tampering, attackers can even make your home’s sweeping robot a spy by modifying the firmware. In view of these threats, this paper proposes a FSMFA: Firmware-Secure Multi-Factor Authentication protocol based on PUF (physical unclonable function) and device firmware integrity, which enhance the physical and software security of IoT devices while enhancing the security of IoT system. So as to realize mutual authentication and key negotiation between the device and the server. At the same time, in order to guarantee the safety of the whole life cycle of the device, we propose the challenge response pairs\n(\nC\nR\nP\ns\n)\nand firmware update scheme of the device. Finally, we use\nB\nA\nN\nlogic and\nP\nr\no\nV\ne\nr\ni\nf\nto prove the security of authentication and update protocols. Compared with other similar protocols, the proposed protocol achieves better security and higher efficiency.}
}


@article{DBLP:journals/iot/LukajMFCV23,
	author = {Valeria Lukaj and
                  Francesco Martella and
                  Maria Fazio and
                  Antonio Celesti and
                  Massimo Villari},
	title = {Establishment of a trusted environment for IoT service provisioning
                  based on X3DH-Based brokering and Federated Blockchain},
	journal = {Internet Things},
	volume = {21},
	pages = {100686},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100686},
	doi = {10.1016/J.IOT.2023.100686},
	timestamp = {Sat, 29 Apr 2023 19:27:16 +0200},
	biburl = {https://dblp.org/rec/journals/iot/LukajMFCV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The adoption of non-certified Internet of Things (IoT) devices can expose the system to cyber attacks that can disrupt IoT-based applications or generate fake data. At the same time, complex cryptographic approaches cannot be adopted due to the limited computational and power resources of IoT devices. In the literature, the certification of IoT devices is performed through a Certification Authority (CA) that generates and stores certificates for all the IoT nodes. Usually, CA is hosted in remote sites (e.g., in the Cloud or the IoT service administrator's private network) and this exposes the IoT ecosystem to attacks. This paper overcomes these challenges by proposing a new Broker based certification process which decouples at the Edge the communication between IoT devices and the CA. Acting as an “intermediary”, the Mobile Edge Computing (MEC) node shields the communication between untrusted IoT devices and the CA, taking the responsibility for the node certification. The establishment of a trusted ecosystem is further reinforced to guarantee integrity and non-repudiation of the data by using a Federated Blockchain, which is a distributed storage of non-falsifiable data in digital ledgers. Confidentiality and robustness against network issues or temporary disconnections is also achieved using the Extended Triple Diffie-Hellman (X3DH) protocol, which set up secure communication over the Internet among the involved. In the paper, we present the design of the whole proposed solution together with the exploited technologies and details on our implementation. We also present evaluation results to show the efficiency and performance of our solution.}
}


@article{DBLP:journals/iot/TianGZSX23,
	author = {Yingjie Tian and
                  Weizhi Gao and
                  Qin Zhang and
                  Pu Sun and
                  Dongkuan Xu},
	title = {Improving long-tailed classification by disentangled variance transfer},
	journal = {Internet Things},
	volume = {21},
	pages = {100687},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100687},
	doi = {10.1016/J.IOT.2023.100687},
	timestamp = {Mon, 08 May 2023 14:38:38 +0200},
	biburl = {https://dblp.org/rec/journals/iot/TianGZSX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Image classification is very important in the system of internet of things (IoT), and long-tailed distribution data are common in our daily life. Extremely imbalanced classes in long-tailed classification lead to a huge performance gap between training and testing. A number of methods have been proposed to transfer knowledge from head classes to tail classes, which expects to augment semantic information in tail. However, by projecting feature vectors onto classifier vectors, we find that the projection part and the orthogonal part behave differently in testing phase as the number of instances decreases. In order to properly transfer covariance information in long-tailed classification task, we propose a novel class-based covariance transfer method from the perspective of disentangling. Extensive experimental results on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT and iNaturalist 2018 illustrate the effectiveness of our method, which will further improve the validity of IoT system.}
}


@article{DBLP:journals/iot/DingZLMGL23,
	author = {Xuhui Ding and
                  Yue Zhang and
                  Jiaxuan Li and
                  Boyan Mao and
                  Yuting Guo and
                  Gaoyang Li},
	title = {A feasibility study of multi-mode intelligent fusion medical data
                  transmission technology of industrial Internet of Things combined
                  with medical Internet of Things},
	journal = {Internet Things},
	volume = {21},
	pages = {100689},
	year = {2023},
	url = {https://doi.org/10.1016/j.iot.2023.100689},
	doi = {10.1016/J.IOT.2023.100689},
	timestamp = {Sat, 30 Sep 2023 10:16:53 +0200},
	biburl = {https://dblp.org/rec/journals/iot/DingZLMGL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a branch of the Internet of Things (IoT) dedicated to medical care industry, the Internet of Medical Things (IoMT) includes networking devices and applications for medical and health information technology applications. The IoMT still has many shortcomings, such as unstable information transmission, low accuracy of fault diagnosis and classification, and lack of anomaly detection capability. Therefore, this paper first comprehensively describes some relatively new research and popular background technologies of the Industrial Internet of Things (IIoT) and IoMT, and then, according to the functional requirements analysis of the intelligent data transmission system based on IoMT, we discuss in detail the problems of fault diagnosis and resource allocation faced in the IoMT model. An intelligent data transmission model is proposed to apply the wireless communication transmission technology of the industrial Internet of Things to the Internet of Medical Things scene. This model has the ability of high-quality data transmission, high accuracy accident diagnosis classification and real-time anomaly monitoring, which makes up for the shortcomings of traditional IoMT models. In particular, for the accident diagnosis and classification functions in this model, we innovatively adopt the multi-mode data fusion CNN algorithm. The experimental results show that the classification accuracy of the accident diagnosis results is effectively improved. Compared with other algorithms, this model with multi-mode data fusion CNN algorithm improves the data transmission rate, reduces the average data delay, and improves the real-time anomaly monitoring capability of IoMT without increasing the signal leakage rate, thus further improving the overall stability of IoMT.}
}
